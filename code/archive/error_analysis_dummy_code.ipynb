{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`updates from the last main_model_code.ipynb in master branch.`\n",
    "\n",
    "`0.1 possibly some new libraries`\n",
    "\n",
    "`2.1 I moved this code chunk from the bottom to here.`\n",
    "\n",
    "`2.3 New code chunk -- this gets the Y label for the decoder`\n",
    "\n",
    "`2.6 Major code change here to allow encoder option`\n",
    "\n",
    "`3.2 New code chunk -- this calculates cosine similarity loss for decoder.`\n",
    "\n",
    "`3.4, 3.5 compile_model() function is now customized as two compile_caps_model() and compile_cnn_model() for code claritys sake. corresponding loss functions and loss weights are updated`\n",
    "\n",
    "`(you can delete the two existing code chunks which contains `def decoder_loss()` and compile_model())`\n",
    "\n",
    "`3.6 fit_model() arguments have changed to allow for multiple model outputs, replace this whole chunk`\n",
    "\n",
    "`4.0 'embed_dim' : embed_dim;  swtich 'use_decoder' to True to enable decoder`\n",
    "\n",
    "`4.1 'embed_dim' : embed_dim`\n",
    "\n",
    "`4.2 compile_model() now becomes compile_caps_model`\n",
    "\n",
    "`4.3 compile_model() now becomes compile_cnn_model`\n",
    "\n",
    "`4.4 New code chunk -- draw and compile CapsNet with decoder`\n",
    "\n",
    "`4.5, 4.6 minor argument format change according to change described in 3.6`\n",
    "\n",
    "`4.7 New code chunk to fit CapsNet with decoder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yeunghoman/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "from keras import callbacks, optimizers\n",
    "from keras import backend as KB\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Activation\n",
    "from keras.layers import LeakyReLU, Dense, Input, Embedding, Dropout, Reshape, Concatenate, MaxPooling1D, Flatten\n",
    "from keras.layers import Bidirectional, GRU, Flatten, SpatialDropout1D, Conv1D\n",
    "from keras.layers import Add\n",
    "# from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from common import vocabulary, utils\n",
    "\n",
    "# capsule layers from Xifeng Guo \n",
    "# https://github.com/XifengGuo/CapsNet-Keras\n",
    "from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask\n",
    "import glove_helper\n",
    "\n",
    "import time # !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.4.1\n",
      "Keras version: 2.1.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"Keras version:\", K.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"../data/conll2003/eng.train\"\n",
    "DEV_FILE = \"../data/conll2003/eng.testa\"\n",
    "TEST_FILE = \"../data/conll2003/eng.testb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Class !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# local untils\n",
    "\n",
    "# timeit decorator\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print ('%r  %2.2f ms' % \\\n",
    "                  (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_embedding_matrix(embed_dim):\n",
    "    \"\"\"\n",
    "    construct embedding matrix from GloVe 6Bn word data\n",
    "    \n",
    "    reuse glove_helper code from w266 \n",
    "    \n",
    "    Returns: an embedding matrix directly plugged into keras.layers.Embedding(weights=[embedding_matrix])\n",
    "    \"\"\"\n",
    "    reload(glove_helper)\n",
    "    hands = glove_helper.Hands(ndim=embed_dim)\n",
    "    embedding_matrix = np.zeros((vocabData.vocab.size, embed_dim))\n",
    "\n",
    "    for i in range(vocabData.vocab.size):\n",
    "        word = vocabData.vocab.ids_to_words([i])[0]\n",
    "        try:\n",
    "            embedding_vector = hands.get_vector(word)\n",
    "        except:\n",
    "            embedding_vector = hands.get_vector(\"<unk>\")\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class conll2003Data(object):\n",
    "    \"\"\"\n",
    "    Keep track of data and processing operations for a single CoNLL2003 data file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filePath_train):\n",
    "        \"\"\"\n",
    "        filePath(string): path to a CoNLL2003 raw data file for training the vocabulary\n",
    "        \"\"\"\n",
    "        self.vocab = []\n",
    "        self.posTags = []\n",
    "        self.nerTags = []\n",
    "        self.train_sentences = self.readFile(filePath_train)\n",
    "\n",
    "\n",
    "    @timeit\n",
    "    def readFile(self, filePath, canonicalize=True, verbose=False):\n",
    "        \"\"\"\n",
    "        Read the conll2003 raw data file\n",
    "\n",
    "        filename(string) - path to conll2003 file (train, test, etc.)\n",
    "        \n",
    "        Returns: a list of lists of lists corresponding to the words, pos tags, ner tags, capitalization\n",
    "                 in each sentence\n",
    "\n",
    "        \"\"\"\n",
    "        print (\"----------------------------------------------------\")\n",
    "        print (\"reading file from path\", str(filePath))\n",
    "        f = open(filePath)\n",
    "        sentences = []\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            if len(line) == 0 or line.startswith(\"-DOCSTART\") or line[0] == '\\n':\n",
    "                if len(sentence) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                continue\n",
    "            \n",
    "            # input format is [ word, pos tag, chunck tag, ner tag]\n",
    "            # we are ignoring the chunck tag\n",
    "            splits = line.strip().split(' ')\n",
    "            if canonicalize:\n",
    "                word = [utils.canonicalize_word(splits[0]), splits[1], splits[3], self.capitalizaion(splits[0])]\n",
    "            else:\n",
    "                word = [splits[0], splits[1], splits[3], capitalizaion(splits[0])]\n",
    "            sentence.append( word)\n",
    "        \n",
    "        # don't forget the last sentence\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "        \n",
    "        if verbose: \n",
    "            print (\"number of sentences on file =\",len(sentences))\n",
    "            print (\"first 5 sentences:\")\n",
    "            print (sentences[:5])\n",
    "\n",
    "        return sentences\n",
    "    \n",
    "    \n",
    "    def capitalizaion(self, word):\n",
    "        \"\"\"\n",
    "        check capitalization info for a word\n",
    "        return 'lowercase' for 'sfsd'\n",
    "        return 'allCaps' for 'SFSD'\n",
    "        return 'upperInitial' for 'Sfsd'\n",
    "        return 'mixedCaps' for 'SfSd'\n",
    "        return 'noinfo' for '$#%@#' or '12334'\n",
    "        \"\"\"\n",
    "        alphas = [c.isalpha() for c in word] \n",
    "        if sum(alphas) != len(word):\n",
    "            return 'noinfo'\n",
    "        caps = [char.lower()==char for char in word]\n",
    "        if sum(caps) == len(word):\n",
    "            return 'lowercase'\n",
    "        elif sum(caps) == 0:\n",
    "            return 'allCaps'\n",
    "        elif caps[0] == False and sum(caps) == len(word)-1:\n",
    "            return 'upperInitial'\n",
    "        elif 0 < sum(caps) < len(word):\n",
    "            return 'mixedCaps'\n",
    "        else:\n",
    "            return 'noinfo'    \n",
    "    \n",
    "    @timeit\n",
    "    def buildVocab(self, vocabSize=None, verbose=False, return_vocab_objects=False):\n",
    "        \"\"\"\n",
    "        Builds the vocabulary based on the initial data file\n",
    "        \n",
    "        vocabSize(int, default: None-all words) - max number of words to use for vocabulary\n",
    "                                                  (only used for training)\n",
    "        verbose(boolean, default: False)        - print extra info\n",
    "        \"\"\"    \t\n",
    "        print (\"----------------------------------------------------\")\n",
    "        print (\"building vocabulary from TRAINING data...\")\n",
    "\n",
    "        flatData = [w for w in zip(*utils.flatten(self.train_sentences))]\n",
    "\n",
    "        # remember these vocabs will have the <s>, </s>, and <unk> tags in there\n",
    "        # sizes need to be interpreted \"-3\" - consider replacing...\n",
    "        self.vocab = vocabulary.Vocabulary( flatData[0], size=vocabSize)\n",
    "        self.posTags = vocabulary.Vocabulary( flatData[1])\n",
    "        self.nerTags = vocabulary.Vocabulary( flatData[2])\n",
    "        self.capitalTags = vocabulary.Vocabulary(flatData[3])\n",
    "\n",
    "        if verbose:\n",
    "            print (\"vocabulary for words, posTags, nerTags built and stored in object\")\n",
    "            print (\"vocab size =\", vocabSize)\n",
    "            print (\"10 sampled words from vocabulary\\n\", list(self.vocab.wordset)[:10], \"\\n\")\n",
    "            print (\"number of unique pos Tags in training =\", self.posTags.size)\n",
    "            print (\"all posTags used\\n\", list(self.posTags.wordset), \"\\n\")\n",
    "            print (\"number of unique NER tags in training =\", self.nerTags.size)\n",
    "            print (\"all nerTags for prediction\", list(self.nerTags.wordset), \"\\n\")\n",
    "            print (\"number of unique capitalization tags in training =\", self.capitalTags.size)\n",
    "            print ('all capitalTags for prediction', list(self.capitalTags.wordset), \"\\n\")\n",
    "\n",
    "        if return_vocab_objects:\n",
    "            return self.vocab, self.posTags, self.nerTags, self.capitalTags\n",
    "\n",
    "\n",
    "    @timeit\n",
    "    def formatWindowedData(self, sentences, windowLength=9, verbose=False):\n",
    "        \"\"\"\n",
    "        Format the raw data by blocking it into context windows of a fixed length corresponding \n",
    "        to the single target NER tag of the central word.\n",
    "        Make sure to call buildVocab first.\n",
    "        \n",
    "        sentences(list of lists of lists) - raw data from the CoNLL2003 dataset\n",
    "        windowLength(int, default: 9)     - The length of the context window\n",
    "                    NOTE - windowLength must be odd to have a central word. If itsn't, 1 will be added.\n",
    "        verbose(boolean, default: False)  - print extra info\n",
    "        \n",
    "        Returns: 4 numpy arrays: vocabulary training data windowed and converted to IDs, \n",
    "                                 POS tags windowed and converted to IDs,\n",
    "                                 Capitalization info windowed and converted into IDs,\n",
    "                                 NER label tags converted to IDs\n",
    "        \"\"\"\n",
    "\n",
    "        print (\"----------------------------------------------------\")\n",
    "        print (\"formatting sentences into input windows...\")\n",
    "\n",
    "        if windowLength % 2 == 0 or windowLength == 1:\n",
    "            raise ValueError(\"window Length must be an odd number and greater than one.\")\n",
    "    \n",
    "        pads = windowLength // 2\n",
    "\n",
    "        # we have a list of lists (sentences) of lists ([word, posTag, nerTag])\n",
    "        # parse through, pad each sentence with pads open and close tags, then convert to IDs\n",
    "        vocabIDs = [ self.vocab.words_to_ids( [\"<s>\"] * pads + [word[0] for word in sent] + [\"</s>\"] * pads) \\\n",
    "                     for sent in sentences]\n",
    "        posIDs = [ self.posTags.words_to_ids( [\"<s>\"] * pads + [word[1] for word in sent] + [\"</s>\"] * pads) \\\n",
    "                   for sent in sentences]\n",
    "        capitalIDs = [self.capitalTags.words_to_ids([\"<s>\"]*pads + [word[3] for word in sent] + [\"</s>\"]*pads) \\\n",
    "                     for sent in sentences]\n",
    "        nerIDs = [ self.nerTags.words_to_ids( [\"<s>\"] * pads + [word[2] for word in sent] + [\"</s>\"] * pads) \\\n",
    "                   for sent in sentences]\n",
    "        \n",
    "        if verbose: \n",
    "            print (\"STEP 1/2 -- PADDING\")\n",
    "            print (\"all sentences padded with {} pads to either end\".format(pads))\n",
    "            print (\"vocab idx for first 5 sentences:\\n\", vocabIDs[:5], \"\\n\")\n",
    "            print (\"pos idx for first 5 sentences:\\n\", posIDs[:5], \"\\n\")\n",
    "            print (\"ner idx for first 5 sentences:\\n\", nerIDs[:5], \"\\n\")\n",
    "            print (\"capitalization idx for first 5 sentences: \\n\", capitalIDs[:5], \"\\n\")\n",
    "            print (\"number of sentences = {}\".format(len(vocabIDs)), \"\\n\")\n",
    "\n",
    "        assert(len(vocabIDs) == len(posIDs) and len(posIDs) == len(nerIDs) == len(capitalIDs))\n",
    "\n",
    "        if verbose: \n",
    "            print (\"STEP 2/2 -- WINDOWING\")\n",
    "\n",
    "        # build the data to train on by sliding the window across each sentence\n",
    "        # at this point, all 3 lists are the same size, so we can run through them all at once\n",
    "        featsVocab, featsPOS, featsNER, featsCAPITAL = [], [], [], []\n",
    "        for sentID in range( len(vocabIDs)):\n",
    "            sent = vocabIDs[sentID]\n",
    "            sentPOS = posIDs[sentID]\n",
    "            sentNER = nerIDs[sentID]\n",
    "            sentCAPITAL = capitalIDs[sentID]\n",
    "            \n",
    "            for ID in range( len(sent) - windowLength + 1):\n",
    "                featsVocab.append( sent[ID:ID + windowLength])\n",
    "                featsPOS.append( sentPOS[ID:ID + windowLength])\n",
    "                featsCAPITAL.append(sentCAPITAL[ID:ID + windowLength])\n",
    "                featsNER.append( sentNER[ID + windowLength // 2])\n",
    "                \n",
    "        if verbose:\n",
    "            print (\"sample windows:\")\n",
    "            for i in range(3):\n",
    "                print (\"Vocab for window {}\".format(i))\n",
    "                print (featsVocab[i])\n",
    "                print (self.vocab.ids_to_words(featsVocab[i]))\n",
    "                print (\"PoS tags for window {}\".format(i))\n",
    "                print (featsPOS[i])\n",
    "                print (self.posTags.ids_to_words(featsPOS[i]))\n",
    "                print (\"Capitalization tags for window {}\".format(i))\n",
    "                print (featsCAPITAL[i])\n",
    "                print (self.capitalTags.ids_to_words(featsCAPITAL[i]))\n",
    "                print (\"NER tags for center word\")\n",
    "                print (featsNER[i])\n",
    "                print (self.nerTags.ids_to_words([featsNER[i]]),\"\\n\")\n",
    "                \n",
    "            print (\"rows of vocab features = {}\".format(len(featsVocab)))\n",
    "            print (\"rows of PoS features = {}\".format(len(featsVocab)))\n",
    "            print (\"rows of Capitalization features = {}\".format(len(featsCAPITAL)))\n",
    "            print (\"rows of NER features = {}\".format(len(featsNER)))\n",
    "            \n",
    "            print (\"numpy feature arrays are returned\")\n",
    "\n",
    "        assert(len(featsVocab) == len(featsVocab) == len(featsNER) == len(featsCAPITAL))\n",
    "        return np.array(featsVocab), np.array(featsPOS), np.array(featsCAPITAL), np.array(featsNER)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "reading file from path ../data/conll2003/eng.train\n",
      "'readFile'  1336.43 ms\n",
      "----------------------------------------------------\n",
      "building vocabulary from TRAINING data...\n",
      "'buildVocab'  1104.56 ms\n",
      "----------------------------------------------------\n",
      "formatting sentences into input windows...\n",
      "'formatWindowedData'  2017.61 ms\n",
      "----------------------------------------------------\n",
      "reading file from path ../data/conll2003/eng.testa\n",
      "'readFile'  313.65 ms\n",
      "----------------------------------------------------\n",
      "formatting sentences into input windows...\n",
      "'formatWindowedData'  528.49 ms\n",
      "----------------------------------------------------\n",
      "reading file from path ../data/conll2003/eng.testb\n",
      "'readFile'  335.59 ms\n",
      "----------------------------------------------------\n",
      "formatting sentences into input windows...\n",
      "'formatWindowedData'  628.77 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "windowLength = 9\n",
    "testNumSents = 5000\n",
    "\n",
    "# Use training set to build vocab here\n",
    "vocabData = conll2003Data(TRAIN_FILE)\n",
    "vocabData.buildVocab( vocabSize=20000)\n",
    "\n",
    "# Format training data\n",
    "trainX, trainX_pos, trainX_capitals, trainY  = vocabData.formatWindowedData( vocabData.train_sentences, \n",
    "                                                  windowLength=windowLength,\n",
    "                                                  verbose=False)\n",
    "\n",
    "# read in dev data\n",
    "devSents = vocabData.readFile( DEV_FILE)\n",
    "devX, devX_pos, devX_capitals, devY = vocabData.formatWindowedData( devSents, \n",
    "                                              windowLength=windowLength,\n",
    "                                              verbose=False)\n",
    "\n",
    "# read in the test data\n",
    "testSents = vocabData.readFile( TEST_FILE)\n",
    "testX, testX_pos, testX_capitals, testY = vocabData.formatWindowedData( testSents, \n",
    "                                                windowLength=windowLength,\n",
    "                                                verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.50d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 50))\n"
     ]
    }
   ],
   "source": [
    "# !\n",
    "embed_dim = 50\n",
    "# Load GloVe embedding matrix\n",
    "embedding_matrix = construct_embedding_matrix(embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Y\n",
    "\n",
    "# encoding 1-hot for ner targets\n",
    "trainY_cat = to_categorical(trainY.astype('float32'))\n",
    "devY_cat = to_categorical(devY.astype('float32'), num_classes=trainY_cat.shape[1])\n",
    "testY_cat = to_categorical(testY.astype('float32'), num_classes=trainY_cat.shape[1])\n",
    "\n",
    "trainY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), trainY_cat)), dtype=np.float)\n",
    "devY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), devY_cat)), dtype=np.float)\n",
    "testY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), testY_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DUMMY NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 7, ..., 3, 5, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gold label\n",
    "trainY_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203621, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dummy prediction labels\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(trainY_cat.shape[0]))\n",
    "trainY_cat_pred = trainY_cat[shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY_cat_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203621, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY_cat_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 4, ..., 0, 2, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(trainY_cat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 7, ..., 3, 5, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(trainY_cat, axis=1) + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cross_entropy_per_row(y_true_row, y_pred_row):\n",
    "    \"\"\"\n",
    "    Returns: a real number. cross-entropy for one single prediction\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_worst_predictions(y_true, y_pred, k=5):\n",
    "    \"\"\"\n",
    "    print worst k predictions by cross_entropy loss\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    compute precision = correct entities / predicted entities\n",
    "    \n",
    "    Arguments:\n",
    "        y_true : trainY/devY/testY. an array of shape(?,). Each value correspond to the ner tag for a word\n",
    "        y_pred : model prediction. same shape and format as y_true\n",
    "    \n",
    "    Returns:\n",
    "        precision : real number\n",
    "    \"\"\"\n",
    "    model_entities_filter = (y_pred != 3).astype(\"int\") # of the words our model say has a NER class\n",
    "    precision_correct_entities = (y_pred[np.where(model_entities_filter)] == y_true[np.where(model_entities_filter)]).astype(\"int\")\n",
    "    precision = np.sum(precision_correct_entities)/np.sum(model_entities_filter)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_recall(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    compute recall = correct entities / all gold entities\n",
    "    \n",
    "    Arguments:\n",
    "        y_true : trainY/devY/testY. an array of shape(?,). Each value correspond to the ner tag for a word\n",
    "        y_pred : model prediction. same shape and format as y_true\n",
    "    \n",
    "    Returns:\n",
    "        recall : real number\n",
    "    \"\"\"\n",
    "    true_entities_filter = (y_true != 3).astype(\"int\") # of the words that truly has a NER class\n",
    "    recall_correct_entities = (y_pred[np.where(true_entities_filter)] == y_true[np.where(true_entities_filter)]).astype(\"int\")\n",
    "    recall = np.sum(recall_correct_entities)/np.sum(true_entities_filter)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_raw_y_pred(raw_y_pred):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        raw_y_pred : raw predictions generated from model.predict() method. \\\n",
    "        this is a 2D matrix of shape (?, number of NER classes). Each row correspond to one 1-hot NER vector\n",
    "    \n",
    "    Returns:\n",
    "        an array of shape (?,). Each value correspond to the predicted ner tag for a word.\\\n",
    "        You can convert this array back to NER tags by vocabData.ner_vocab.ids_to_words()\n",
    "    \"\"\"\n",
    "    return np.argmax(raw_y_pred, axis=1) + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_f1(y_true, y_pred):\n",
    "    precision = get_precision(y_true, y_pred)\n",
    "    recall = get_recall(y_true, y_pred)\n",
    "    f1 = 2*(precision*recall)/(precision + recall)\n",
    "    return (precision, recall, f1)\n",
    "\n",
    "# !!\n",
    "assert(get_f1(y_true=np.array([5,7,3,4]), y_pred=np.array([[1,0,0,0,0],[0,0,1,0,0],[1,0,0,0,0],[0,1,0,0,0]])) == (1/2,1/3,0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our model hallucinated that there's a NER class\n",
    "def get_ner_hallucination(y_true, raw_y_pred):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our model missed a NER class\n",
    "def get_missed_ner(y_true, raw_y_pred):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ner exist in both model pred and gold \n",
    "# model got right/wrong\n",
    "def get_missed_ner(y_true, raw_y_pred):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct dumb baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dissect by true ner tag\n",
    "# see mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !\n",
    "# Get decoder Y -- 50 dim embedding of center word\n",
    "\n",
    "train_decoderY = embedding_matrix[trainX[:,4]]\n",
    "dev_decoderY = embedding_matrix[devX[:,4]]\n",
    "test_decoderY = embedding_matrix[testX[:,4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get X pos tags\n",
    "\n",
    "# encoding 1-hot for pos tags\n",
    "trainX_pos_cat = to_categorical(trainX_pos.astype('float32'))\n",
    "devX_pos_cat = to_categorical(devX_pos.astype('float32'), num_classes=trainX_pos_cat.shape[2]) \n",
    "testX_pos_cat = to_categorical(testX_pos.astype('float32'), num_classes=trainX_pos_cat.shape[2])\n",
    "\n",
    "trainX_pos_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), trainX_pos_cat)), dtype=np.float)\n",
    "devX_pos_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), devX_pos_cat)), dtype=np.float)\n",
    "testX_pos_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), testX_pos_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get X capitlization \n",
    "\n",
    "# encoding 1-hot for capitalization info  (\"allCaps\", \"upperInitial\", \"lowercase\", \"mixedCaps\", \"noinfo\")\n",
    "trainX_capitals_cat = to_categorical(trainX_capitals.astype('float32'))\n",
    "devX_capitals_cat = to_categorical(devX_capitals.astype('float32'), num_classes=trainX_capitals_cat.shape[2]) \n",
    "testX_capitals_cat = to_categorical(testX_capitals.astype('float32'), num_classes=trainX_capitals_cat.shape[2])\n",
    "\n",
    "trainX_capitals_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), trainX_capitals_cat)), dtype=np.float)\n",
    "devX_capitals_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), devX_capitals_cat)), dtype=np.float)\n",
    "testX_capitals_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), testX_capitals_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_capsnet_model(hyper_param, embedding_matrix=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Input: hyper parameters dictionary\n",
    "    \n",
    "    Construct:\n",
    "        input layers : x , x_pos(o), x_captialization(o)\n",
    "        embedding matrix : use_glove or randomly initialize\n",
    "        conv1 : first convolution layer\n",
    "        primarycaps : conv2 and squash function applied\n",
    "        ner_caps : make 8 ner capsules of specified dim\n",
    "        out_pred : calc length of 8 ner capsules as 8 prob. predictions over 8 ner classes\n",
    "    \n",
    "    Returns: \n",
    "        if decoding/reconstruction disabled --> a single keras.models.Model object\n",
    "        if decoding/reconstruction enabled --> three keras.models.Model objects\n",
    "    \"\"\"\n",
    "\n",
    "    # input layer(s)\n",
    "    x = Input(shape=(hyper_param['maxlen'],), name='x')\n",
    "    if hyper_param['use_pos_tags'] : \n",
    "        x_pos = Input(shape=(hyper_param['maxlen'],hyper_param['poslen']), name='x_pos')\n",
    "    if hyper_param['use_capitalization_info'] : \n",
    "        x_capital = Input(shape=(hyper_param['maxlen'], hyper_param['capitallen']), name='x_capital') \n",
    "    \n",
    "    # embedding matrix\n",
    "    if hyper_param['use_glove']:\n",
    "        embed = Embedding(hyper_param['max_features'], hyper_param['embed_dim'], weights=[embedding_matrix],\\\n",
    "                          input_length=hyper_param['maxlen'], trainable=hyper_param['allow_glove_retrain'])(x)\n",
    "    else:\n",
    "        embed = Embedding(hyper_param['max_features'], hyper_param['embed_dim'], input_length=hyper_param['maxlen'],\\\n",
    "                          embeddings_initializer=\"random_uniform\" )(x)\n",
    "\n",
    "    # concat embeddings with additional features\n",
    "    if hyper_param['use_pos_tags'] and hyper_param['use_capitalization_info'] : \n",
    "        embed = Concatenate(axis=-1)([embed, x_pos, x_capital])\n",
    "    elif hyper_param['use_pos_tags'] and (not hyper_param['use_capitalization_info']) :\n",
    "        embed = Concatenate(axis=-1)([embed, x_pos])\n",
    "    elif (not hyper_param['use_pos_tags']) and hyper_param['use_capitalization_info'] :\n",
    "        embed = Concatenate(axis=-1)([embed, x_capital])    \n",
    "    else :\n",
    "        embed = embed\n",
    "    \n",
    "    # feed embeddings into conv1\n",
    "    conv1 = Conv1D( filters=hyper_param['conv1_filters'], \\\n",
    "                   kernel_size=hyper_param['conv1_kernel_size'],\\\n",
    "                   strides=hyper_param['conv1_strides'], \\\n",
    "                   padding=hyper_param['conv1_padding'],\\\n",
    "                   activation='relu', name='conv1')(embed)\n",
    "\n",
    "    # make primary capsules\n",
    "    primarycaps = PrimaryCap(embed, \\\n",
    "                             dim_capsule=hyper_param['primarycaps_dim_capsule'],\\\n",
    "                             n_channels=hyper_param['primarycaps_n_channels'],\\\n",
    "                             kernel_size=hyper_param['primarycaps_kernel_size'], \\\n",
    "                             strides=hyper_param['primarycaps_strides'], \\\n",
    "                             padding=hyper_param['primarycaps_padding'])\n",
    "\n",
    "    # make ner capsules\n",
    "    ner_caps = CapsuleLayer(num_capsule=hyper_param['ner_classes'], \\\n",
    "                            dim_capsule=hyper_param['ner_capsule_dim'], \\\n",
    "                            routings=hyper_param['num_dynamic_routing_passes'], \\\n",
    "                            name='nercaps')(primarycaps)\n",
    "\n",
    "    # replace each ner capsuel with its length\n",
    "    out_pred = Length(name='out_pred')(ner_caps)\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print (\"x\", x.get_shape())\n",
    "        if hyper_param['use_pos_tags'] : print (\"x_pos\", x_pos.get_shape())\n",
    "        if hyper_param['use_capitalization_info'] : print (\"x_capital\", x_capital.get_shape())\n",
    "        print (\"embed\", embed.get_shape())\n",
    "        print (\"conv1\", conv1.get_shape())\n",
    "        print (\"primarycaps\", primarycaps.get_shape())   \n",
    "        print (\"ner_caps\", ner_caps.get_shape())\n",
    "        print (\"out_pred\", out_pred.get_shape())\n",
    "\n",
    "\n",
    "    if hyper_param['use_decoder']:\n",
    "        decoder_y_cat = Input(shape=(hyper_param['ner_classes'],), name='decoder_y_cat')\n",
    "        masked_by_y = Mask(name='masked_by_y')([ner_caps, decoder_y_cat]) # true label is used to mask during training\n",
    "        masked = Mask()(ner_caps) # mask using capsule with maximal length for predicion\n",
    "\n",
    "        # decoder for training \n",
    "        train_decoder_dense1 = Dense(hyper_param['decoder_feed_forward_1'], activation='relu',\\\n",
    "                               input_dim=hyper_param['ner_capsule_dim']*hyper_param['ner_classes'],\\\n",
    "                               name='train_decoder_dense1')(masked_by_y)\n",
    "        train_decoder_dense1_dropout = Dropout(hyper_param['decoder_dropout'])(train_decoder_dense1)\n",
    "        train_decoder_dense2 = Dense(hyper_param['decoder_feed_forward_2'], activation='relu',\\\n",
    "                                     name='train_decoder_dense2')(train_decoder_dense1_dropout)\n",
    "        train_decoder_dense2_dropout = Dropout(hyper_param['decoder_dropout'])(train_decoder_dense2)\n",
    "        train_decoder_output = Dense(hyper_param['embed_dim'], activation='softmax',\\\n",
    "                                     name='train_decoder_output')(train_decoder_dense2_dropout)\n",
    "\n",
    "\n",
    "        # decoder for evaluation (prediction) \n",
    "        eval_decoder_dense1 = Dense(hyper_param['decoder_feed_forward_1'], activation='relu',\\\n",
    "                               input_dim=hyper_param['ner_capsule_dim']*hyper_param['ner_classes'],\\\n",
    "                               name='eval_decoder_dense1')(masked)\n",
    "        eval_decoder_dense2 = Dense(hyper_param['decoder_feed_forward_2'], activation='relu',\\\n",
    "                                     name='eval_decoder_dense2')(eval_decoder_dense1)\n",
    "        eval_decoder_output = Dense(hyper_param['embed_dim'], activation='softmax',\\\n",
    "                                     name='eval_decoder_output')(eval_decoder_dense2)\n",
    "        \n",
    "        \n",
    "        if verbose:\n",
    "            print (\"Decoder model enabled for GloVe vector deconstruction...\")\n",
    "            print (\"decoder_y_cat\", decoder_y_cat.get_shape())\n",
    "            print (\"masked_by_y\", masked_by_y.get_shape())\n",
    "            print (\"train_decoder_dense1\", train_decoder_dense1.get_shape())\n",
    "            print (\"train_decoder_dense1_dropout\", train_decoder_dense1_dropout.get_shape())\n",
    "            print (\"train_decoder_dense2\", train_decoder_dense2.get_shape())\n",
    "            print (\"train_decoder_dense2_dropout\", train_decoder_dense2_dropout.get_shape())\n",
    "            print (\"train_decoder_output\", train_decoder_output.get_shape())\n",
    "            print (\"masked\", masked.get_shape())\n",
    "            print (\"eval_decoder_dense1\", eval_decoder_dense1.get_shape())\n",
    "            print (\"eval_decoder_dense2\", eval_decoder_dense2.get_shape())\n",
    "            print (\"eval_decoder_output\", eval_decoder_output.get_shape())\n",
    "\n",
    "    # construct input list\n",
    "    if hyper_param['use_pos_tags'] and hyper_param['use_capitalization_info'] : \n",
    "        input_list = [x, x_pos, x_capital]\n",
    "    elif hyper_param['use_pos_tags'] and (not hyper_param['use_capitalization_info']) :\n",
    "        input_list = [x, x_pos]\n",
    "    elif (not hyper_param['use_pos_tags']) and hyper_param['use_capitalization_info'] :\n",
    "        input_list = [x, x_capital]\n",
    "    else:\n",
    "        input_list = [x]\n",
    "\n",
    "\n",
    "    if hyper_param['use_decoder']==False:\n",
    "        print (\"decoder/reconstruction DISabled\")\n",
    "        print (\"returning 1 model\")\n",
    "        return Model(inputs=input_list, outputs=[out_pred])\n",
    "    else :\n",
    "        train_model = Model(inputs=input_list+[decoder_y_cat], outputs=[out_pred, train_decoder_output])\n",
    "        eval_model = Model(inputs=input_list, outputs=[out_pred, eval_decoder_output])\n",
    "        print (\"decoder/reconstruction enabled\")\n",
    "        print (\"returning a list of 2 models: train_model, eval_model\")\n",
    "        return train_model, eval_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_cnn_model(hyper_param, embedding_matrix=None ,verbose=True):\n",
    "    \"\"\"\n",
    "    Input: hyper_parameters dictionary\n",
    "    \n",
    "    Construct:\n",
    "        input layers : x , x_pos(o), x_captialization(o)\n",
    "        embedding matrix : use_glove or randomly initialize\n",
    "        conv1 : first convolution layer\n",
    "        conv2 : second convolution layer\n",
    "        conv3 : third convolution layer\n",
    "        max pooling\n",
    "        flatten : concant maxpooled univariate vectors into one long vector\n",
    "        ff1, ff2: two feed forward layers\n",
    "        out_pred: softmax over all ner classes\n",
    "    \n",
    "    Returns: keras.models.Model object\n",
    "    \"\"\"\n",
    "\n",
    "    # input layer(s)\n",
    "    x = Input(shape=(hyper_param['maxlen'],), name='x')\n",
    "    if hyper_param['use_pos_tags'] : \n",
    "        x_pos = Input(shape=(hyper_param['maxlen'],hyper_param['poslen']), name='x_pos')\n",
    "    if hyper_param['use_capitalization_info'] : \n",
    "        x_capital = Input(shape=(hyper_param['maxlen'], hyper_param['capitallen']), name='x_capital') \n",
    "    \n",
    "    # embedding matrix\n",
    "    if hyper_param['use_glove']:\n",
    "        embed = Embedding(hyper_param['max_features'], hyper_param['embed_dim'], weights=[embedding_matrix],\\\n",
    "                          input_length=hyper_param['maxlen'], trainable=hyper_param['allow_glove_retrain'])(x)\n",
    "    else:\n",
    "        embed = Embedding(hyper_param['max_features'], hyper_param['embed_dim'], input_length=hyper_param['maxlen'],\\\n",
    "                          embeddings_initializer=\"random_uniform\" )(x)\n",
    "\n",
    "    # concat embeddings with additional features\n",
    "    if hyper_param['use_pos_tags'] and hyper_param['use_capitalization_info'] : \n",
    "        embed = Concatenate(axis=-1)([embed, x_pos, x_capital])\n",
    "    elif hyper_param['use_pos_tags'] and (not hyper_param['use_capitalization_info']) :\n",
    "        embed = Concatenate(axis=-1)([embed, x_pos])\n",
    "    elif (not hyper_param['use_pos_tags']) and hyper_param['use_capitalization_info'] :\n",
    "        embed = Concatenate(axis=-1)([embed, x_capital])    \n",
    "    else :\n",
    "        embed = embed\n",
    "    \n",
    "    # feed embeddings into conv1\n",
    "    conv1 = Conv1D( filters=hyper_param['conv1_filters'], \\\n",
    "                   kernel_size=hyper_param['conv1_kernel_size'],\\\n",
    "                   strides=hyper_param['conv1_strides'], \\\n",
    "                   padding=hyper_param['conv1_padding'],\\\n",
    "                   activation='relu', name='conv1')(embed)\n",
    "    \n",
    "    # update this\n",
    "    # make primary capsules\n",
    "    conv2 = Conv1D( filters=hyper_param['conv2_filters'], \\\n",
    "                   kernel_size=hyper_param['conv2_kernel_size'],\\\n",
    "                   strides=hyper_param['conv2_strides'], \\\n",
    "                   padding=hyper_param['conv2_padding'],\\\n",
    "                   activation='relu', name='conv2')(conv1)    \n",
    "    \n",
    "    # update this\n",
    "    # make primary capsules\n",
    "    conv3 = Conv1D( filters=hyper_param['conv3_filters'], \\\n",
    "                   kernel_size=hyper_param['conv3_kernel_size'],\\\n",
    "                   strides=hyper_param['conv3_strides'], \\\n",
    "                   padding=hyper_param['conv3_padding'],\\\n",
    "                   activation='relu', name='conv3')(conv2)    \n",
    "    \n",
    "    # max pooling layer\n",
    "    max_pooled = MaxPooling1D(pool_size=hyper_param['max_pooling_size'], \\\n",
    "                              strides=hyper_param['max_pooling_strides'], \\\n",
    "                              padding=hyper_param['max_pooling_padding'])(conv3)\n",
    "    # dropout\n",
    "    maxpooled_dropout = Dropout(hyper_param['maxpool_dropout'])(max_pooled)\n",
    "    \n",
    "    # flatten many univariate vectos into 1 long vector\n",
    "    flattened = Flatten()(maxpooled_dropout)\n",
    "    \n",
    "    # to feed-forward layers\n",
    "    ff1 = Dense(hyper_param['feed_forward_1'], activation='relu')(flattened)    \n",
    "    ff1_dropout = Dropout(hyper_param['ff1_dropout'])(ff1)\n",
    "    \n",
    "    ff2 = Dense(hyper_param['feed_forward_2'], activation='relu')(ff1_dropout)    \n",
    "    ff2_dropout = Dropout(hyper_param['ff2_dropout'])(ff2)    \n",
    "    \n",
    "    out_pred = Dense(hyper_param['ner_classes'], activation='softmax', name='out_pred')(ff2) #!\n",
    "    \n",
    "             \n",
    "    if verbose:\n",
    "        print (\"x\", x.get_shape())\n",
    "        if hyper_param['use_pos_tags'] : print (\"x_pos\", x_pos.get_shape())\n",
    "        if hyper_param['use_capitalization_info'] : print (\"x_capital\", x_capital.get_shape())\n",
    "        print (\"embed\", embed.get_shape())\n",
    "        print (\"embed\", embed.get_shape())\n",
    "        \n",
    "        print (\"conv1\", conv1.get_shape())\n",
    "        print (\"conv2\", conv2.get_shape())   \n",
    "        print (\"conv3\", conv3.get_shape())\n",
    "        print (\"max_pooled\", max_pooled.get_shape())\n",
    "        print (\"flattened\", flattened.get_shape())   \n",
    "        print (\"ff1\", ff1.get_shape())\n",
    "        print (\"ff2\", ff2.get_shape())\n",
    "        print (\"out_pred\", out_pred.get_shape())        \n",
    "  \n",
    "    # return final model\n",
    "    if hyper_param['use_pos_tags'] and hyper_param['use_capitalization_info'] : \n",
    "        cnnmodel = Model(inputs=[x, x_pos, x_capital], outputs=[out_pred])\n",
    "    elif hyper_param['use_pos_tags'] and (not hyper_param['use_capitalization_info']) :\n",
    "        cnnmodel = Model(inputs=[x, x_pos], outputs=[out_pred])\n",
    "    elif (not hyper_param['use_pos_tags']) and hyper_param['use_capitalization_info'] :\n",
    "        cnnmodel = Model(inputs=[x, x_capital], outputs=[out_pred])   \n",
    "    else :\n",
    "        cnnmodel = Model(inputs=[x], outputs=[out_pred])\n",
    "        \n",
    "    return cnnmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_cosine_proximity(y_true, y_pred):\n",
    "    y_true = tf.nn.l2_normalize(y_true, dim=-1)\n",
    "    y_pred = tf.nn.l2_normalize(y_pred, dim=-1)\n",
    "    return -KB.sum(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#margin_loss\n",
    "def margin_loss(y_true, y_pred):\n",
    "    L = y_true * KB.square(KB.maximum(0., 0.9 - y_pred)) + 0.5 * (1 - y_true) * KB.square(KB.maximum(0., y_pred - 0.1))\n",
    "    return KB.mean(KB.sum(L, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !\n",
    "def compile_caps_model(hyper_param, model): #!\n",
    "    \"\"\"\n",
    "    Input: keras.models.Model object, see draw_capsnet_model() output. This is a graph with all layers drawn and connected\n",
    "    \n",
    "    do:\n",
    "        compile with loss function and optimizer\n",
    "    \n",
    "    Returns: compiled model\n",
    "    \"\"\"\n",
    "    if hyper_param['optimizer'] == \"Adam\":\n",
    "        opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    elif hyper_param['optimizer'] == \"SGD\": \n",
    "        opt = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "    elif hyper_param['optimizer'] == None:\n",
    "        raise Exception(\"No optimizer specified\")\n",
    "\n",
    "    if hyper_param.get('use_decoder') == True:\n",
    "        model_loss = [margin_loss, custom_cosine_proximity] # work in progress\n",
    "        loss_wts = [1, hyper_param['lam_recon']]\n",
    "    else:\n",
    "        model_loss = margin_loss\n",
    "        loss_wts = None\n",
    "    \n",
    "    model.compile(optimizer=opt, #'adam',\n",
    "                  loss=model_loss,\n",
    "                  loss_weights=loss_wts,\n",
    "                  metrics={'out_pred':'accuracy'})\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compile_cnn_model(hyper_param, model): #!\n",
    "    \"\"\"\n",
    "    Input: keras.models.Model object, see draw_capsnet_model() output. This is a graph with all layers drawn and connected\n",
    "    \n",
    "    do:\n",
    "        compile with loss function and optimizer\n",
    "    \n",
    "    Returns: compiled model\n",
    "    \"\"\"\n",
    "    if hyper_param['optimizer'] == \"Adam\":\n",
    "        opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    elif hyper_param['optimizer'] == \"SGD\": \n",
    "        opt = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "    elif hyper_param['optimizer'] == None:\n",
    "        raise Exception(\"No optimizer specified\")\n",
    "\n",
    "    model.compile(optimizer=opt, #'adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model(hyper_param, model, trainX_dict, devX_list_arrayS, trainY_dict, devY_list_arrayS ): #!\n",
    "    #Saving weights and logging\n",
    "    log = callbacks.CSVLogger(hyper_param['save_dir'] + '/log.csv')\n",
    "    tb = callbacks.TensorBoard(log_dir=hyper_param['save_dir'] + '/tensorboard-logs', \\\n",
    "                               batch_size=hyper_param['batch_size'], histogram_freq=hyper_param['debug'])\n",
    "    checkpoint = callbacks.ModelCheckpoint(hyper_param['save_dir'] + '/weights-{epoch:02d}.h5', \\\n",
    "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
    "    #lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
    "\n",
    "    model.summary()\n",
    "    #Save a png of the model shapes and flow\n",
    "    #plot_model(capsmodel, to_file=save_dir + '/reuters-model.png', show_shapes=True)\n",
    "\n",
    "    #loss = margin_loss\n",
    "    \n",
    "    data = model.fit( x=trainX_dict, # {'x':trainX, 'x_pos':trainX_pos_cat, 'x_capital':trainX_capitals_cat, (o)'decoder_y':trainY_cat}\n",
    "                      y=trainY_dict, #!{'out_pred':trainY_cat, (o)'decoder_output':train_decoderY}\n",
    "                      batch_size=hyper_param['batch_size'], \n",
    "                      epochs=hyper_param['epochs'], \n",
    "                      validation_data=[devX_list_arrayS, devY_list_arrayS], #! [devX, devX_pos_cat, devX_capitals_cat, (o)devY_cat], [devY_cat, (o)dev_decoderY]\n",
    "                      callbacks=[log, tb, checkpoint], \n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!\n",
    "# define hyper parameters for model\n",
    "# CAPSNET\n",
    "hyper_param_caps = {\n",
    "    \n",
    "    'max_features' : vocabData.vocab.size,  # 20000\n",
    "    'maxlen' : trainX.shape[1],  # window size (9)\n",
    "    'poslen' : trainX_pos_cat.shape[2],  # pos classes (45)\n",
    "    'capitallen' : trainX_capitals_cat.shape[2],  # capitalization classes (5)\n",
    "    'ner_classes' : trainY_cat.shape[1],  # 8 \n",
    "    'embed_dim' : embed_dim,  # word embedding size\n",
    "    'num_routing' : 3, \n",
    "\n",
    "    'use_glove' : True,\n",
    "    'allow_glove_retrain' : False,\n",
    "    'use_pos_tags' : True,\n",
    "    'use_capitalization_info' : True,    \n",
    "    \n",
    "    'conv1_filters' : 256,\n",
    "    'conv1_kernel_size' : 3,\n",
    "    'conv1_strides' : 1,\n",
    "    'conv1_padding' : 'valid',\n",
    "    \n",
    "    'primarycaps_dim_capsule' : 8,\n",
    "    'primarycaps_n_channels' : 32,\n",
    "    'primarycaps_kernel_size' : 3,\n",
    "    'primarycaps_strides' : 1,\n",
    "    'primarycaps_padding' : 'valid',\n",
    "\n",
    "    'ner_capsule_dim' : 16,\n",
    "    \n",
    "    'num_dynamic_routing_passes' : 3,\n",
    "    \n",
    "    # decoder is still work in progress\n",
    "    'use_decoder' : False,\n",
    "    'decoder_feed_forward_1' : 100,\n",
    "    'decoder_feed_forward_2' : 100, \n",
    "    'decoder_dropout' : 0.3,\n",
    "    \n",
    "    'save_dir' : './result',\n",
    "    'batch_size' : 100,\n",
    "    'debug' : 2,\n",
    "    'epochs' : 5,\n",
    "    'dropout_p' : 0.25,\n",
    "    'embed_dropout' : 0.25,\n",
    "    'lam_recon' : 0.0005,\n",
    "    \n",
    "    'optimizer' : 'Adam', #or 'SGD'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!\n",
    "# define hyper parameters for model\n",
    "# CNN\n",
    "hyper_param_cnn = {\n",
    "    \n",
    "    'max_features' : vocabData.vocab.size,  # 20000\n",
    "    'maxlen' : trainX.shape[1],  # window size (9)\n",
    "    'poslen' : trainX_pos_cat.shape[2],  # pos classes (45)\n",
    "    'capitallen' : trainX_capitals_cat.shape[2],  # capitalization classes (5)\n",
    "    'ner_classes' : trainY_cat.shape[1],  # 8 \n",
    "    'embed_dim' : embed_dim,  # word embedding size\n",
    "    'num_routing' : 3, \n",
    "\n",
    "    'use_glove' : True,\n",
    "    'allow_glove_retrain' : False,\n",
    "    'use_pos_tags' : True,\n",
    "    'use_capitalization_info' : True,    \n",
    "    \n",
    "    'conv1_filters' : 256,\n",
    "    'conv1_kernel_size' : 3,\n",
    "    'conv1_strides' : 1,\n",
    "    'conv1_padding' : 'valid',\n",
    "    \n",
    "    'conv2_filters' : 256,\n",
    "    'conv2_kernel_size' : 3,\n",
    "    'conv2_strides' : 1,\n",
    "    'conv2_padding' : 'valid',\n",
    "    \n",
    "    'conv3_filters' : 128,\n",
    "    'conv3_kernel_size' : 3,\n",
    "    'conv3_strides' : 1,\n",
    "    'conv3_padding' : 'valid',\n",
    "    \n",
    "    'max_pooling_size' : 3,\n",
    "    'max_pooling_strides' : 1,\n",
    "    'max_pooling_padding' : 'valid',\n",
    "    'maxpool_dropout' : 0.3,\n",
    "    \n",
    "    'feed_forward_1' : 328,\n",
    "    'ff1_dropout' : 0.3,\n",
    "    'feed_forward_2' : 192,\n",
    "    'ff2_dropout' : 0.3,\n",
    "    \n",
    "    'save_dir' : './result',\n",
    "    'batch_size' : 100,\n",
    "    'debug' : 2,\n",
    "    'epochs' : 5,\n",
    "    'dropout_p' : 0.25,\n",
    "    'embed_dropout' : 0.25,\n",
    "    'lam_recon' : 0.0005,\n",
    "    \n",
    "    'optimizer' : 'Adam', #or 'SGD'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "decoder/reconstruction DISabled\n",
      "returning 1 model\n"
     ]
    }
   ],
   "source": [
    "# no decoder\n",
    "# draw and compile capsmodel GloVe embedding matrix\n",
    "capsmodel = draw_capsnet_model(hyper_param=hyper_param_caps, embedding_matrix=embedding_matrix, verbose=True)\n",
    "capsmodel = compile_caps_model(hyper_param_caps, capsmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "conv2 (?, 5, 256)\n",
      "conv3 (?, 3, 128)\n",
      "max_pooled (?, 1, 128)\n",
      "flattened (?, ?)\n",
      "ff1 (?, 328)\n",
      "ff2 (?, 192)\n",
      "out_pred (?, 8)\n"
     ]
    }
   ],
   "source": [
    "# draw and compile capsmodel GloVe embedding matrix\n",
    "cnnmodel = draw_cnn_model(hyper_param=hyper_param_cnn, embedding_matrix=embedding_matrix, verbose=True)\n",
    "cnnmodel = compile_cnn_model(hyper_param_cnn, cnnmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "Decoder model enabled for GloVe vector deconstruction...\n",
      "decoder_y_cat (?, 8)\n",
      "masked_by_y (?, ?)\n",
      "train_decoder_dense1 (?, 100)\n",
      "train_decoder_dense2 (?, 100)\n",
      "train_decoder_output (?, 50)\n",
      "masked (?, ?)\n",
      "eval_decoder_dense1 (?, 100)\n",
      "eval_decoder_dense2 (?, 100)\n",
      "eval_decoder_output (?, 50)\n",
      "decoder/reconstruction enabled\n",
      "returning a list of 2 models: train_model, eval_model\n"
     ]
    }
   ],
   "source": [
    "# with decoder\n",
    "capsmodel_decoder_train, capsmodel_decoder_eval = draw_capsnet_model(hyper_param=hyper_param_caps, embedding_matrix=embedding_matrix, verbose=True)\n",
    "capsmodel_decoder = compile_caps_model(hyper_param_caps, capsmodel_decoder_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 9, 100)       0           embedding_2[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       77056       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv1D)                  (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv1D)                  (None, 3, 128)       98432       conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 128)       0           conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 128)       0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 128)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 328)          42312       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 328)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 192)          63168       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 8)            1544        dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,366,776\n",
      "Trainable params: 479,376\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/5\n",
      "203621/203621 [==============================] - 360s 2ms/step - loss: 0.2034 - acc: 0.8326 - val_loss: 0.2035 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20351, saving model to ./result/weights-01.h5\n",
      "Epoch 2/5\n",
      "203621/203621 [==============================] - 375s 2ms/step - loss: 0.2031 - acc: 0.8328 - val_loss: 0.2035 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/5\n",
      "203621/203621 [==============================] - 367s 2ms/step - loss: 0.2031 - acc: 0.8328 - val_loss: 0.2035 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/5\n",
      "203621/203621 [==============================] - 350s 2ms/step - loss: 0.2031 - acc: 0.8328 - val_loss: 0.2035 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/5\n",
      "203621/203621 [==============================] - 358s 2ms/step - loss: 0.2031 - acc: 0.8328 - val_loss: 0.2035 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "fit_model(hyper_param_cnn, cnnmodel, \\\n",
    "          trainX_dict={'x':trainX, 'x_pos':trainX_pos_cat, 'x_capital':trainX_capitals_cat}, \\\n",
    "          devX_list_arrayS=[devX, devX_pos_cat, devX_capitals_cat], \\\n",
    "          trainY_dict={'out_pred':trainY_cat}, \\\n",
    "          devY_list_arrayS=[devY_cat] ) #!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 100)       0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv2d (Conv1D)      (None, 7, 256)       77056       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 224, 8)       0           primarycap_conv2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 224, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        229376      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,193,832\n",
      "Trainable params: 306,432\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/5\n",
      "  7500/203621 [>.............................] - ETA: 13:27 - loss: 0.2043 - acc: 0.8188"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-eb5b0c440081>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper_param_caps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapsmodel\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0mtrainX_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x_pos'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainX_pos_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x_capital'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainX_capitals_cat\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0mdevX_list_arrayS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevX_pos_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevX_capitals_cat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0mtrainY_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'out_pred'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainY_cat\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0mdevY_list_arrayS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevY_cat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-37156bb1072f>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(hyper_param, model, trainX_dict, devX_list_arrayS, trainY_dict, devY_list_arrayS)\u001b[0m\n\u001b[1;32m     18\u001b[0m                       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevX_list_arrayS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevY_list_arrayS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#! [devX, devX_pos_cat, devX_capitals_cat, (o)devY_cat], [devY_cat, (o)dev_decoderY]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                       \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                       verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit_model(hyper_param_caps, capsmodel, \\\n",
    "          trainX_dict={'x':trainX, 'x_pos':trainX_pos_cat, 'x_capital':trainX_capitals_cat}, \\\n",
    "          devX_list_arrayS=[devX, devX_pos_cat, devX_capitals_cat], \\\n",
    "          trainY_dict={'out_pred':trainY_cat}, \\\n",
    "          devY_list_arrayS=[devY_cat] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 9, 100)       0           embedding_13[0][0]               \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv2d (Conv1D)      (None, 7, 256)       77056       concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 224, 8)       0           primarycap_conv2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 224, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        229376      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_y_cat (InputLayer)      (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masked_by_y (Mask)              (None, 128)          0           nercaps[0][0]                    \n",
      "                                                                 decoder_y_cat[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "train_decoder_dense1 (Dense)    (None, 100)          12900       masked_by_y[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "train_decoder_dense2 (Dense)    (None, 100)          10100       train_decoder_dense1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "train_decoder_output (Dense)    (None, 50)           5050        train_decoder_dense2[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 1,221,882\n",
      "Trainable params: 334,482\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/5\n",
      "  9600/203621 [>.............................] - ETA: 13:06 - loss: 0.1694 - out_pred_loss: 0.1868 - train_decoder_output_loss: -34.8223 - out_pred_acc: 0.8264"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-d250f282444e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# with decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper_param_caps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapsmodel_decoder_train\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0mtrainX_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x_pos'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainX_pos_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x_capital'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainX_capitals_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'decoder_y_cat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainY_cat\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0mdevX_list_arrayS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevX_pos_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevX_capitals_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevY_cat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0mtrainY_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'out_pred'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainY_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_decoder_output'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_decoderY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0mdevY_list_arrayS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevY_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_decoderY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-37156bb1072f>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(hyper_param, model, trainX_dict, devX_list_arrayS, trainY_dict, devY_list_arrayS)\u001b[0m\n\u001b[1;32m     18\u001b[0m                       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevX_list_arrayS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevY_list_arrayS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#! [devX, devX_pos_cat, devX_capitals_cat, (o)devY_cat], [devY_cat, (o)dev_decoderY]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                       \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                       verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# with decoder\n",
    "fit_model(hyper_param_caps, capsmodel_decoder_train, \\\n",
    "          trainX_dict={'x':trainX, 'x_pos':trainX_pos_cat, 'x_capital':trainX_capitals_cat, 'decoder_y_cat':trainY_cat}, \\\n",
    "          devX_list_arrayS=[devX, devX_pos_cat, devX_capitals_cat, devY_cat], \\\n",
    "          trainY_dict={'out_pred':trainY_cat, 'train_decoder_output':train_decoderY}, \\\n",
    "          devY_list_arrayS=[devY_cat, dev_decoderY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
