{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yeunghoman/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# UPDATES!\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "from keras import callbacks, optimizers\n",
    "from keras import backend as KB\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Activation\n",
    "from keras.layers import LeakyReLU, Dense, Input, Embedding, Dropout, Reshape, Concatenate # !\n",
    "from keras.layers import Bidirectional, GRU, Flatten, SpatialDropout1D, Conv1D\n",
    "from keras.datasets import imdb # probably redundant\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from common import vocabulary, utils\n",
    "\n",
    "import time # !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.4.1\n",
      "Keras version: 2.1.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"Keras version:\", K.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"../data/conll2003/eng.train\"\n",
    "DEV_FILE = \"../data/conll2003/eng.testa\"\n",
    "TEST_FILE = \"../data/conll2003/eng.testb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-DOCSTART- -X- -X- O\r\n",
      "\r\n",
      "EU NNP I-NP I-ORG\r\n",
      "rejects VBZ I-VP O\r\n",
      "German JJ I-NP I-MISC\r\n",
      "call NN I-NP O\r\n",
      "to TO I-VP O\r\n",
      "boycott VB I-VP O\r\n",
      "British JJ I-NP I-MISC\r\n",
      "lamb NN I-NP O\r\n"
     ]
    }
   ],
   "source": [
    "# NOT PIPELINE\n",
    "!head -10 {TRAIN_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOT PIPELINE\n",
    "# this is odd that the B-PER tag doesn't appear... investigate this.\n",
    "!grep \"B-PER\" {TRAIN_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOT PIPELINE\n",
    "def readfile( filename, pos=False):\n",
    "    '''\n",
    "    read the conll2003 file\n",
    "    \n",
    "    filename(string) - path to conll2003 file (train, test, etc.)\n",
    "    pos(boolean) - flag if true will include pos tags in returned list\n",
    "    returns a list of lists of lists corresponding to the words in each sentence\n",
    "    \n",
    "    '''\n",
    "    f = open(filename)\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in f:\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.strip().split(' ')\n",
    "        word = [splits[0], splits[1], splits[-1]] if pos else [splits[0], splits[-1]]\n",
    "        sentence.append( word)\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        sentences.append(sentence)\n",
    "        sentence = []\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOT PIPELINE\n",
    "trainSentences = readfile(TRAIN_FILE, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['EU', 'NNP', 'I-ORG'],\n",
       "  ['rejects', 'VBZ', 'O'],\n",
       "  ['German', 'JJ', 'I-MISC'],\n",
       "  ['call', 'NN', 'O'],\n",
       "  ['to', 'TO', 'O'],\n",
       "  ['boycott', 'VB', 'O'],\n",
       "  ['British', 'JJ', 'I-MISC'],\n",
       "  ['lamb', 'NN', 'O'],\n",
       "  ['.', '.', 'O']],\n",
       " [['Peter', 'NNP', 'I-PER'], ['Blackburn', 'NNP', 'I-PER']],\n",
       " [['BRUSSELS', 'NNP', 'I-LOC'], ['1996-08-22', 'CD', 'O']]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOT PIPELINE\n",
    "trainSentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 2, ..., 4, 2, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOT PIPELINE\n",
    "trainLens = np.array([len(s) for s in trainSentences])\n",
    "trainLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzKyk/h/bYaQEEYcnkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8148f99748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences longer than 37.0 words: 5 %\n"
     ]
    }
   ],
   "source": [
    "# NOT PIPELINE\n",
    "from matplotlib import pyplot as plt\n",
    "clipPct = 5\n",
    "\n",
    "maxLen = np.percentile(trainLens, 100 - clipPct)\n",
    "histLens = plt.hist(trainLens, bins=30)\n",
    "plt.vlines( maxLen, 0, max(histLens[0]), linestyles=\"dashed\")\n",
    "plt.title(\"Sentence lengths in corpus\")\n",
    "plt.hist(trainLens, bins=30)\n",
    "plt.show()\n",
    "print( \"Sentences longer than\", maxLen, \"words:\", clipPct, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  2.66500000e+03,   3.33600000e+03,   2.12800000e+03,\n",
      "         9.26000000e+02,   8.61000000e+02,   8.54000000e+02,\n",
      "         8.58000000e+02,   6.27000000e+02,   7.23000000e+02,\n",
      "         4.59000000e+02,   3.25000000e+02,   1.37000000e+02,\n",
      "         7.90000000e+01,   3.90000000e+01,   1.00000000e+01,\n",
      "         9.00000000e+00,   2.00000000e+00,   1.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
      "         0.00000000e+00,   0.00000000e+00,   1.00000000e+00]), array([   1.        ,    4.73333333,    8.46666667,   12.2       ,\n",
      "         15.93333333,   19.66666667,   23.4       ,   27.13333333,\n",
      "         30.86666667,   34.6       ,   38.33333333,   42.06666667,\n",
      "         45.8       ,   49.53333333,   53.26666667,   57.        ,\n",
      "         60.73333333,   64.46666667,   68.2       ,   71.93333333,\n",
      "         75.66666667,   79.4       ,   83.13333333,   86.86666667,\n",
      "         90.6       ,   94.33333333,   98.06666667,  101.8       ,\n",
      "        105.53333333,  109.26666667,  113.        ]), <a list of 30 Patch objects>)\n",
      "Max sentence length:  113\n"
     ]
    }
   ],
   "source": [
    "# NOT PIPELINE\n",
    "print (histLens)\n",
    "print (\"Max sentence length: \", max(trainLens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LUNCH', 'conversion', '0:10', 'paramedics', 'unwound', '7944', 'billion', 'Carlton', 'socialism', 'Currently', 'Kamio', 'Scout', 'pressed', 'Rain', '32.40', 'forfeited', '11.00', 'consult', 'Pope', 'relegated']\n",
      "Vocab size: 23626\n",
      "['NNPS', 'VBD', 'WP', '$', 'JJS', 'NNS', 'POS', 'WP$', 'UH', 'CC', '.', 'NNP', 'EX', 'TO', 'MD', 'WRB', 'NN|SYM', ',', 'RBR', 'PRP', '<s>', 'RP', ':', '<unk>', 'NN', 'VB', 'JJR', 'VBG', 'FW', 'JJ', 'CD', 'VBP', 'RBS', \"''\", 'PDT', 'IN', ')', 'VBZ', '\"', 'RB', '</s>', 'WDT', 'DT', 'PRP$', 'LS', '(', 'VBN', 'SYM']\n",
      "['</s>', 'O', 'I-LOC', 'B-ORG', 'I-MISC', '<s>', 'B-MISC', 'I-ORG', 'B-LOC', '<unk>', 'I-PER']\n"
     ]
    }
   ],
   "source": [
    "# NOT PIPELINE\n",
    "\n",
    "# build vocabulary - thank you w266\n",
    "# -- first attempt, leave in all numbers and maintain case\n",
    "flatData = [w for w in zip(*utils.flatten(trainSentences))]\n",
    "\n",
    "# try with lower vocab sizes... 10k, 15k, 20k\n",
    "vocab = vocabulary.Vocabulary( flatData[0])\n",
    "posTags = vocabulary.Vocabulary( flatData[1])\n",
    "nerTags = vocabulary.Vocabulary( flatData[2])\n",
    "\n",
    "print( (list(vocab.wordset)[:20]))\n",
    "print( \"Vocab size:\", vocab.size)\n",
    "print( (list(posTags.wordset)))\n",
    "print( (list(nerTags.wordset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<common.vocabulary.Vocabulary object at 0x7f812c01fdd8>\n"
     ]
    }
   ],
   "source": [
    "# NOT PIPELINE\n",
    "print (vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NNP',\n",
       " 'VBZ',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " '.',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'CD',\n",
       " 'DT',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'PRP',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'NNS',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'VBP',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'VBN',\n",
       " 'TO',\n",
       " 'NN',\n",
       " '.',\n",
       " 'NNP',\n",
       " 'POS',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'DT',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'POS',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'NNS',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'JJ',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'JJR',\n",
       " '.',\n",
       " '\"',\n",
       " 'PRP',\n",
       " 'VBP',\n",
       " 'RB',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'PRP',\n",
       " 'VBP',\n",
       " 'RB',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'PRP',\n",
       " ',',\n",
       " '\"',\n",
       " 'DT',\n",
       " 'NNP',\n",
       " 'POS',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'FW',\n",
       " 'NNP',\n",
       " 'VBD',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " '.',\n",
       " 'PRP',\n",
       " 'VBD',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'VBN',\n",
       " 'CC',\n",
       " 'IN',\n",
       " 'PRP',\n",
       " 'VBD',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'VBN',\n",
       " 'PRP',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " '.',\n",
       " 'PRP',\n",
       " 'VBD',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " ',',\n",
       " 'NNS',\n",
       " 'CC',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'CC',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'VBD',\n",
       " 'DT',\n",
       " 'RB',\n",
       " 'JJ',\n",
       " 'CC',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " '.',\n",
       " 'JJR',\n",
       " 'VBN',\n",
       " 'NNP',\n",
       " 'VBZ',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'CC',\n",
       " 'NNP',\n",
       " 'WDT',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'NN',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " '(',\n",
       " 'NNP',\n",
       " ')',\n",
       " ':',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'NN',\n",
       " '.',\n",
       " 'CC',\n",
       " 'NNP',\n",
       " 'VBD',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'PRP$',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNP',\n",
       " 'POS',\n",
       " 'NN',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " ',',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " ',',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " 'RB',\n",
       " 'VBD',\n",
       " 'RB',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " '.',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'VBD',\n",
       " 'RBR',\n",
       " 'VBN',\n",
       " 'NNP',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'POS',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'VBG',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " '\"',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " '.',\n",
       " '\"',\n",
       " '.',\n",
       " 'RB',\n",
       " 'NNP',\n",
       " 'CC',\n",
       " 'NNP',\n",
       " 'VBD',\n",
       " 'NNP',\n",
       " 'POS',\n",
       " 'NN',\n",
       " '.',\n",
       " 'DT',\n",
       " 'NNP',\n",
       " 'POS',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'CC',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'VBP',\n",
       " 'JJ',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'RB',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'CC',\n",
       " 'VB',\n",
       " 'NNS',\n",
       " 'TO',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " '.',\n",
       " 'NNP',\n",
       " 'VBP',\n",
       " 'RB',\n",
       " 'VBN',\n",
       " 'VBN',\n",
       " 'TO',\n",
       " 'NN',\n",
       " 'NN',\n",
       " ',',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'JJ',\n",
       " 'TO',\n",
       " 'NNP',\n",
       " 'WDT',\n",
       " 'VBZ',\n",
       " 'VBN',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'VBN',\n",
       " 'VBN',\n",
       " 'TO',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'VBG',\n",
       " 'NN',\n",
       " 'NN',\n",
       " '.',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'EX',\n",
       " 'VBD',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'PRP$',\n",
       " 'NN',\n",
       " ',',\n",
       " 'CC',\n",
       " 'VBD',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'NNS',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " '.',\n",
       " '\"',\n",
       " 'WP',\n",
       " 'PRP',\n",
       " 'VBP',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'RB',\n",
       " 'JJ',\n",
       " 'IN',\n",
       " 'VBZ',\n",
       " 'WRB',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'VBP',\n",
       " 'VBG',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'NNP',\n",
       " 'POS',\n",
       " 'NN',\n",
       " ',',\n",
       " '\"',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'POS',\n",
       " 'NNP',\n",
       " '(',\n",
       " 'NNP',\n",
       " ')',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'NN',\n",
       " '.',\n",
       " 'NNP',\n",
       " 'VBZ',\n",
       " 'VBN',\n",
       " 'NNS',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'NNS',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'JJ',\n",
       " 'TO',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'VBG',\n",
       " 'VBN',\n",
       " 'NN',\n",
       " '.',\n",
       " 'NNP',\n",
       " 'VBD',\n",
       " 'CD',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " ',',\n",
       " 'RB',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " '.',\n",
       " 'PRP',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'CD',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " ',',\n",
       " 'DT',\n",
       " 'CD',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " '.',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'VBZ',\n",
       " 'IN',\n",
       " 'RB',\n",
       " '$',\n",
       " 'CD',\n",
       " '.',\n",
       " 'NNP',\n",
       " 'CD',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'VBD',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " 'RB',\n",
       " '$',\n",
       " 'CD',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'POS',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " '.',\n",
       " 'DT',\n",
       " 'NNP',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'CD',\n",
       " 'NNS',\n",
       " '(',\n",
       " '$',\n",
       " 'CD',\n",
       " ')',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " '\"',\n",
       " 'VBZ',\n",
       " 'RB',\n",
       " 'DT',\n",
       " 'VBG',\n",
       " '\"',\n",
       " ',',\n",
       " 'WDT',\n",
       " 'NNP',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'CD',\n",
       " '.',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNP',\n",
       " 'CD',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'PRP',\n",
       " 'VBD',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " ',',\n",
       " 'WRB',\n",
       " 'PRP',\n",
       " 'VBD',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " '.',\n",
       " 'NNS',\n",
       " 'RB',\n",
       " 'VBD',\n",
       " 'RB',\n",
       " 'CD',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'WDT',\n",
       " 'VBD',\n",
       " 'VBN',\n",
       " 'RP',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'POS',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " ',',\n",
       " 'WP',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'PRP',\n",
       " 'IN',\n",
       " 'CD',\n",
       " 'TO',\n",
       " 'CD',\n",
       " '.',\n",
       " 'PRP',\n",
       " 'VBD',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'CC',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'NN',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'PRP$',\n",
       " 'NNS',\n",
       " ',',\n",
       " 'WDT',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'CD',\n",
       " 'NNS',\n",
       " '(',\n",
       " '$',\n",
       " 'CD',\n",
       " ')',\n",
       " '.',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'CD',\n",
       " 'VBN',\n",
       " 'CD',\n",
       " '.',\n",
       " 'NNP',\n",
       " 'VBZ',\n",
       " 'NNP',\n",
       " 'VBZ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " '.',\n",
       " 'VBG',\n",
       " 'CD',\n",
       " 'NNP',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'VBD',\n",
       " 'NNP',\n",
       " 'IN',\n",
       " 'VBG',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'NNP',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'WDT',\n",
       " 'VBD',\n",
       " 'NNP',\n",
       " '.',\n",
       " 'VBG',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'VBD',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'RB',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " ',',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'VBD',\n",
       " 'NNP',\n",
       " ':',\n",
       " '\"',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " 'VBZ',\n",
       " 'VBN',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNP',\n",
       " 'NNS',\n",
       " '.',\n",
       " '\"',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'VBD',\n",
       " 'NNP',\n",
       " 'POS',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " ',',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " ',',\n",
       " 'IN',\n",
       " 'VBG',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'IN',\n",
       " 'PRP',\n",
       " 'VBD',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " '.',\n",
       " '\"',\n",
       " 'RB',\n",
       " 'VBZ',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'CD',\n",
       " 'NNS',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " ':',\n",
       " 'WDT',\n",
       " 'VBZ',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NN',\n",
       " ',',\n",
       " '\"',\n",
       " 'NNP',\n",
       " 'POS',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNPS',\n",
       " 'POS',\n",
       " 'JJ',\n",
       " 'VBN',\n",
       " 'NNP',\n",
       " 'IN',\n",
       " 'VBG',\n",
       " '.',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'POS',\n",
       " 'NNP',\n",
       " 'VBD',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'PRP',\n",
       " 'VBD',\n",
       " 'VBN',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'POS',\n",
       " 'NNS',\n",
       " 'CC',\n",
       " 'VBD',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'WRB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'VBD',\n",
       " 'VBN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'RB',\n",
       " 'IN',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " ',',\n",
       " 'WDT',\n",
       " 'NNP',\n",
       " 'VBZ',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " '.',\n",
       " 'NNP',\n",
       " ',',\n",
       " 'WDT',\n",
       " 'VBZ',\n",
       " 'RB',\n",
       " 'VBN',\n",
       " 'DT',\n",
       " 'NNP',\n",
       " 'NNS',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'JJR',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " ',',\n",
       " 'VBD',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'TO',\n",
       " 'NNP',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " '.',\n",
       " 'NNP',\n",
       " 'VBZ',\n",
       " 'NN',\n",
       " 'RB',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'NNS',\n",
       " '.',\n",
       " 'VBG',\n",
       " 'CD',\n",
       " 'NNP',\n",
       " 'VBZ',\n",
       " 'VBD',\n",
       " 'PRP',\n",
       " 'VBD',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'CC',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NN',\n",
       " '.',\n",
       " 'NNS',\n",
       " 'MD',\n",
       " 'VB',\n",
       " 'VBN',\n",
       " 'TO',\n",
       " 'VB',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'CC',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " ',',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'VBN',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " ',',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNP',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " ',',\n",
       " 'IN',\n",
       " 'VBG',\n",
       " 'RB',\n",
       " 'RB',\n",
       " 'NNP',\n",
       " '.',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'RB',\n",
       " 'CD',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'SYM',\n",
       " 'NN',\n",
       " '.',\n",
       " 'NNP',\n",
       " 'CD',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'NNS',\n",
       " 'IN',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'VBD',\n",
       " 'CD',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'IN',\n",
       " ...)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatData[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Class !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UPDATES!\n",
    "\n",
    "\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print ('%r  %2.2f ms' % \\\n",
    "                  (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed\n",
    "\n",
    "\n",
    "def capitalizaion(word):\n",
    "    \"\"\"\n",
    "    check capitalization info for a word\n",
    "    return 'lowercase' for 'sfsd'\n",
    "    return 'allCaps' for 'SFSD'\n",
    "    return 'upperInitial' for 'Sfsd'\n",
    "    return 'mixedCaps' for 'SfSd'\n",
    "    return 'noinfo' for '$#%@#' or '12334'\n",
    "    \"\"\"\n",
    "    alphas = [c.isalpha() for c in word] \n",
    "    if sum(alphas) != len(word):\n",
    "        return 'noinfo'\n",
    "    caps = [char.lower()==char for char in word]\n",
    "    if sum(caps) == len(word):\n",
    "        return 'lowercase'\n",
    "    elif sum(caps) == 0:\n",
    "        return 'allCaps'\n",
    "    elif caps[0] == False and sum(caps) == len(word)-1:\n",
    "        return 'upperInitial'\n",
    "    elif 0 < sum(caps) < len(word):\n",
    "        return 'mixedCaps'\n",
    "    else:\n",
    "        return 'noinfo'\n",
    "        \n",
    "\n",
    "\n",
    "class conll2003Data(object):\n",
    "    \"\"\"\n",
    "    Keep track of data and processing operations for a single CoNLL2003 data file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filePath_train):\n",
    "        \"\"\"\n",
    "        filePath(string): path to a CoNLL2003 raw data file for training the vocabulary\n",
    "        \"\"\"\n",
    "        self.vocab = []\n",
    "        self.posTags = []\n",
    "        self.nerTags = []\n",
    "        self.train_sentences = self.readFile(filePath_train)\n",
    "\n",
    "\n",
    "    @timeit\n",
    "    def readFile(self, filePath, verbose=True):\n",
    "        \"\"\"\n",
    "        Read the conll2003 raw data file\n",
    "\n",
    "        filename(string) - path to conll2003 file (train, test, etc.)\n",
    "        \n",
    "        Returns: a list of lists of lists corresponding to the words, pos tags, and ner tags\n",
    "                 in each sentence\n",
    "\n",
    "        \"\"\"\n",
    "        f = open(filePath)\n",
    "        sentences = []\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            if len(line) == 0 or line.startswith(\"-DOCSTART\") or line[0] == '\\n':\n",
    "                if len(sentence) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                continue\n",
    "            \n",
    "            # input format is [ word, pos tag, chunck tag, ner tag]\n",
    "            # we are ignoring the chunck tag\n",
    "            splits = line.strip().split(' ')\n",
    "            word = [utils.canonicalize_word(splits[0]), splits[1], splits[3], capitalizaion(splits[0])]\n",
    "            sentence.append( word)\n",
    "        \n",
    "        # don't forget the last sentence\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "        \n",
    "        if verbose: \n",
    "            print (\"----------------------------------------------------\")\n",
    "            print (\"reading file from path\", str(filePath))\n",
    "            print (\"number of sentences on file =\",len(sentences))\n",
    "            print (\"first 5 sentences:\")\n",
    "            print (sentences[:5])\n",
    "\n",
    "        return sentences\n",
    "\n",
    "\n",
    "    @timeit\n",
    "    def buildVocab(self, vocabSize=None, verbose=True, return_vocab_objects=False):\n",
    "        \"\"\"\n",
    "        Builds the vocabulary based on the initial data file\n",
    "        \n",
    "        vocabSize(int, default: None-all words) - max number of words to use for vocabulary\n",
    "                                                  (only used for training)\n",
    "        verbose(boolean, default: False)        - print extra info\n",
    "        \"\"\"    \t\n",
    "        if verbose:\n",
    "            print (\"----------------------------------------------------\")\n",
    "            print (\"building vocabulary from TRAINING data...\")\n",
    "\n",
    "        flatData = [w for w in zip(*utils.flatten(self.train_sentences))]\n",
    "\n",
    "        # remember these vocabs will have the <s>, </s>, and <unk> tags in there\n",
    "        # sizes need to be interpreted \"-3\" - consider replacing...\n",
    "        self.vocab = vocabulary.Vocabulary( flatData[0], size=vocabSize)\n",
    "        self.posTags = vocabulary.Vocabulary( flatData[1])\n",
    "        self.nerTags = vocabulary.Vocabulary( flatData[2])\n",
    "        self.capitalTags = vocabulary.Vocabulary(flatData[3])\n",
    "\n",
    "        if verbose:\n",
    "            print (\"vocabulary for words, posTags, nerTags built and stored in object\")\n",
    "            print (\"vocab size =\", vocabSize)\n",
    "            print (\"10 sampled words from vocabulary\\n\", list(self.vocab.wordset)[:10], \"\\n\")\n",
    "            print (\"number of unique pos Tags in training =\", self.posTags.size)\n",
    "            print (\"all posTags used\\n\", list(self.posTags.wordset), \"\\n\")\n",
    "            print (\"number of unique NER tags in training =\", self.nerTags.size)\n",
    "            print (\"all nerTags for prediction\", list(self.nerTags.wordset), \"\\n\")\n",
    "            print (\"number of unique capitalization tags in training =\", self.capitalTags.size)\n",
    "            print ('all capitalTags for prediction', list(self.capitalTags.wordset), \"\\n\")\n",
    "\n",
    "        if return_vocab_objects:\n",
    "            return self.vocab, self.posTags, self.nerTags, self.capitalTags\n",
    "\n",
    "\n",
    "    @timeit\n",
    "    def formatWindowedData(self, sentences, windowLength=9, verbose=False):\n",
    "        \"\"\"\n",
    "        Format the raw data by blocking it into context windows of a fixed length corresponding \n",
    "        to the single target NER tag of the central word.\n",
    "        Make sure to call buildVocab first.\n",
    "        \n",
    "        sentences(list of lists of lists) - raw data from the CoNLL2003 dataset\n",
    "        windowLength(int, default: 9)     - The length of the context window\n",
    "                    NOTE - windowLength must be odd to have a central word. If itsn't, 1 will be added.\n",
    "        verbose(boolean, default: False)  - print extra info\n",
    "        \n",
    "        Returns: 3 numpy arrays: vocabulary training data windowed and converted to IDs, \n",
    "                                 POS tags windowed and converted to IDs,\n",
    "                                 NER label tags converted to IDs\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print (\"----------------------------------------------------\")\n",
    "            print (\"formatting sentences into input windows...\")\n",
    "\n",
    "        if windowLength % 2 == 0 or windowLength == 1:\n",
    "            raise ValueError(\"window Length must be an odd number and greater than one.\")\n",
    "    \n",
    "        pads = windowLength // 2\n",
    "\n",
    "        # we have a list of lists (sentences) of lists ([word, posTag, nerTag])\n",
    "        # parse through, pad each sentence with pads open and close tags, then convert to IDs\n",
    "        vocabIDs = [ self.vocab.words_to_ids( [\"<s>\"] * pads + [word[0] for word in sent] + [\"</s>\"] * pads) \\\n",
    "                     for sent in sentences]\n",
    "        posIDs = [ self.posTags.words_to_ids( [\"<s>\"] * pads + [word[1] for word in sent] + [\"</s>\"] * pads) \\\n",
    "                   for sent in sentences]\n",
    "        capitalIDs = [self.capitalTags.words_to_ids([\"<s>\"]*pads + [word[3] for word in sent] + [\"</s>\"]*pads) \\\n",
    "                     for sent in sentences]\n",
    "        nerIDs = [ self.nerTags.words_to_ids( [\"<s>\"] * pads + [word[2] for word in sent] + [\"</s>\"] * pads) \\\n",
    "                   for sent in sentences]\n",
    "        \n",
    "        if verbose: \n",
    "            print (\"STEP 1/2 -- PADDING\")\n",
    "            print (\"all sentences padded with {} pads to either end\".format(pads))\n",
    "            print (\"vocab idx for first 5 sentences:\\n\", vocabIDs[:5], \"\\n\")\n",
    "            print (\"pos idx for first 5 sentences:\\n\", posIDs[:5], \"\\n\")\n",
    "            print (\"ner idx for first 5 sentences:\\n\", nerIDs[:5], \"\\n\")\n",
    "            print (\"capitalization idx for first 5 sentences: \\n\", capitalIDs[:5], \"\\n\")\n",
    "            print (\"number of sentences = {}\".format(len(vocabIDs)), \"\\n\")\n",
    "\n",
    "        assert(len(vocabIDs) == len(posIDs) and len(posIDs) == len(nerIDs) == len(capitalIDs))\n",
    "\n",
    "        if verbose: \n",
    "            print (\"STEP 2/2 -- WINDOWING\")\n",
    "\n",
    "        # build the data to train on by sliding the window across each sentence\n",
    "        # at this point, all 3 lists are the same size, so we can run through them all at once\n",
    "        featsVocab, featsPOS, featsNER, featsCAPITAL = [], [], [], []\n",
    "        for sentID in range( len(vocabIDs)):\n",
    "            sent = vocabIDs[sentID]\n",
    "            sentPOS = posIDs[sentID]\n",
    "            sentNER = nerIDs[sentID]\n",
    "            sentCAPITAL = capitalIDs[sentID]\n",
    "            \n",
    "            for ID in range( len(sent) - windowLength + 1):\n",
    "                featsVocab.append( sent[ID:ID + windowLength])\n",
    "                featsPOS.append( sentPOS[ID:ID + windowLength])\n",
    "                featsCAPITAL.append(sentCAPITAL[ID:ID + windowLength])\n",
    "                featsNER.append( sentNER[ID + windowLength // 2])\n",
    "                \n",
    "        if verbose:\n",
    "            print (\"sample windows:\")\n",
    "            for i in range(3):\n",
    "                print (\"Vocab for window {}\".format(i))\n",
    "                print (featsVocab[i])\n",
    "                print (self.vocab.ids_to_words(featsVocab[i]))\n",
    "                print (\"PoS tags for window {}\".format(i))\n",
    "                print (featsPOS[i])\n",
    "                print (self.posTags.ids_to_words(featsPOS[i]))\n",
    "                print (\"Capitalization tags for window {}\".format(i))\n",
    "                print (featsCAPITAL[i])\n",
    "                print (self.capitalTags.ids_to_words(featsCAPITAL[i]))\n",
    "                print (\"NER tags for center word\")\n",
    "                print (featsNER[i])\n",
    "                print (self.nerTags.ids_to_words([featsNER[i]]),\"\\n\")\n",
    "                \n",
    "            print (\"rows of vocab features = {}\".format(len(featsVocab)))\n",
    "            print (\"rows of PoS features = {}\".format(len(featsVocab)))\n",
    "            print (\"rows of Capitalization features = {}\".format(len(featsCAPITAL)))\n",
    "            print (\"rows of NER features = {}\".format(len(featsNER)))\n",
    "            \n",
    "            print (\"numpy feature arrays are returned\")\n",
    "\n",
    "        assert(len(featsVocab) == len(featsVocab) == len(featsNER) == len(featsCAPITAL))\n",
    "        return np.array(featsVocab), np.array(featsPOS), np.array(featsCAPITAL), np.array(featsNER) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "reading file from path ../data/conll2003/eng.train\n",
      "number of sentences on file = 14041\n",
      "first 5 sentences:\n",
      "[[['eu', 'NNP', 'I-ORG', 'allCaps'], ['rejects', 'VBZ', 'O', 'lowercase'], ['german', 'JJ', 'I-MISC', 'upperInitial'], ['call', 'NN', 'O', 'lowercase'], ['to', 'TO', 'O', 'lowercase'], ['boycott', 'VB', 'O', 'lowercase'], ['british', 'JJ', 'I-MISC', 'upperInitial'], ['lamb', 'NN', 'O', 'lowercase'], ['.', '.', 'O', 'noinfo']], [['peter', 'NNP', 'I-PER', 'upperInitial'], ['blackburn', 'NNP', 'I-PER', 'upperInitial']], [['brussels', 'NNP', 'I-LOC', 'allCaps'], ['DGDGDGDG-DGDG-DGDG', 'CD', 'O', 'noinfo']], [['the', 'DT', 'O', 'upperInitial'], ['european', 'NNP', 'I-ORG', 'upperInitial'], ['commission', 'NNP', 'I-ORG', 'upperInitial'], ['said', 'VBD', 'O', 'lowercase'], ['on', 'IN', 'O', 'lowercase'], ['thursday', 'NNP', 'O', 'upperInitial'], ['it', 'PRP', 'O', 'lowercase'], ['disagreed', 'VBD', 'O', 'lowercase'], ['with', 'IN', 'O', 'lowercase'], ['german', 'JJ', 'I-MISC', 'upperInitial'], ['advice', 'NN', 'O', 'lowercase'], ['to', 'TO', 'O', 'lowercase'], ['consumers', 'NNS', 'O', 'lowercase'], ['to', 'TO', 'O', 'lowercase'], ['shun', 'VB', 'O', 'lowercase'], ['british', 'JJ', 'I-MISC', 'upperInitial'], ['lamb', 'NN', 'O', 'lowercase'], ['until', 'IN', 'O', 'lowercase'], ['scientists', 'NNS', 'O', 'lowercase'], ['determine', 'VBP', 'O', 'lowercase'], ['whether', 'IN', 'O', 'lowercase'], ['mad', 'JJ', 'O', 'lowercase'], ['cow', 'NN', 'O', 'lowercase'], ['disease', 'NN', 'O', 'lowercase'], ['can', 'MD', 'O', 'lowercase'], ['be', 'VB', 'O', 'lowercase'], ['transmitted', 'VBN', 'O', 'lowercase'], ['to', 'TO', 'O', 'lowercase'], ['sheep', 'NN', 'O', 'lowercase'], ['.', '.', 'O', 'noinfo']], [['germany', 'NNP', 'I-LOC', 'upperInitial'], [\"'s\", 'POS', 'O', 'noinfo'], ['representative', 'NN', 'O', 'lowercase'], ['to', 'TO', 'O', 'lowercase'], ['the', 'DT', 'O', 'lowercase'], ['european', 'NNP', 'I-ORG', 'upperInitial'], ['union', 'NNP', 'I-ORG', 'upperInitial'], [\"'s\", 'POS', 'O', 'noinfo'], ['veterinary', 'JJ', 'O', 'lowercase'], ['committee', 'NN', 'O', 'lowercase'], ['werner', 'NNP', 'I-PER', 'upperInitial'], ['zwingmann', 'NNP', 'I-PER', 'upperInitial'], ['said', 'VBD', 'O', 'lowercase'], ['on', 'IN', 'O', 'lowercase'], ['wednesday', 'NNP', 'O', 'upperInitial'], ['consumers', 'NNS', 'O', 'lowercase'], ['should', 'MD', 'O', 'lowercase'], ['buy', 'VB', 'O', 'lowercase'], ['sheepmeat', 'NN', 'O', 'lowercase'], ['from', 'IN', 'O', 'lowercase'], ['countries', 'NNS', 'O', 'lowercase'], ['other', 'JJ', 'O', 'lowercase'], ['than', 'IN', 'O', 'lowercase'], ['britain', 'NNP', 'I-LOC', 'upperInitial'], ['until', 'IN', 'O', 'lowercase'], ['the', 'DT', 'O', 'lowercase'], ['scientific', 'JJ', 'O', 'lowercase'], ['advice', 'NN', 'O', 'lowercase'], ['was', 'VBD', 'O', 'lowercase'], ['clearer', 'JJR', 'O', 'lowercase'], ['.', '.', 'O', 'noinfo']]]\n",
      "'readFile'  1139.50 ms\n",
      "----------------------------------------------------\n",
      "building vocabulary from TRAINING data...\n",
      "vocabulary for words, posTags, nerTags built and stored in object\n",
      "vocab size = 20000\n",
      "10 sampled words from vocabulary\n",
      " ['spa', 'leoni', 'quell', 'troubles', 'maoist', 'receiving', 'admira', 'mandatory', 'auto', 'inderjit'] \n",
      "\n",
      "number of unique pos Tags in training = 48\n",
      "all posTags used\n",
      " [':', 'JJ', '</s>', ',', 'VBN', 'UH', 'TO', '.', 'CD', ')', 'PRP', 'VBG', 'EX', 'WP$', 'VBZ', 'RB', 'NNP', 'RP', 'DT', 'FW', 'PDT', '<unk>', 'SYM', 'NN|SYM', 'VBD', 'RBS', 'NNS', \"''\", 'RBR', 'IN', 'PRP$', '$', 'VB', '(', 'WDT', 'POS', '\"', 'JJR', 'NNPS', '<s>', 'LS', 'JJS', 'VBP', 'WP', 'MD', 'WRB', 'NN', 'CC'] \n",
      "\n",
      "number of unique NER tags in training = 11\n",
      "all nerTags for prediction ['</s>', 'I-PER', 'B-MISC', 'O', 'I-MISC', '<unk>', 'B-ORG', 'I-LOC', 'I-ORG', 'B-LOC', '<s>'] \n",
      "\n",
      "number of unique capitalization tags in training = 8\n",
      "all capitalTags for prediction ['allCaps', 'mixedCaps', '</s>', 'noinfo', '<unk>', 'lowercase', 'upperInitial', '<s>'] \n",
      "\n",
      "'buildVocab'  1003.20 ms\n",
      "----------------------------------------------------\n",
      "formatting sentences into input windows...\n",
      "STEP 1/2 -- PADDING\n",
      "all sentences padded with 4 pads to either end\n",
      "vocab idx for first 5 sentences:\n",
      " [[0, 0, 0, 0, 931, 9970, 198, 583, 10, 3751, 207, 5455, 4, 1, 1, 1, 1], [0, 0, 0, 0, 723, 1774, 1, 1, 1, 1], [0, 0, 0, 0, 675, 26, 1, 1, 1, 1], [0, 0, 0, 0, 3, 208, 314, 17, 16, 73, 31, 7031, 27, 198, 3752, 10, 2287, 10, 9971, 207, 5455, 375, 3220, 1906, 481, 1652, 1775, 601, 290, 44, 7032, 10, 1548, 4, 1, 1, 1, 1], [0, 0, 0, 0, 126, 18, 2849, 10, 3, 208, 283, 18, 2531, 749, 7033, 7034, 17, 16, 78, 2287, 261, 860, 7035, 30, 500, 129, 127, 140, 375, 3, 2288, 3752, 22, 9972, 4, 1, 1, 1, 1]] \n",
      "\n",
      "pos idx for first 5 sentences:\n",
      " [[0, 0, 0, 0, 3, 22, 8, 4, 17, 13, 8, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 5, 1, 1, 1, 1], [0, 0, 0, 0, 7, 3, 3, 10, 6, 3, 18, 10, 6, 8, 4, 17, 9, 17, 13, 8, 4, 6, 9, 27, 6, 8, 4, 4, 28, 13, 14, 17, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 3, 25, 4, 17, 7, 3, 3, 25, 8, 4, 3, 3, 10, 6, 3, 9, 28, 13, 4, 6, 9, 8, 6, 3, 6, 7, 8, 4, 10, 36, 11, 1, 1, 1, 1]] \n",
      "\n",
      "ner idx for first 5 sentences:\n",
      " [[0, 0, 0, 0, 5, 3, 7, 3, 3, 3, 7, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 4, 4, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 5, 5, 3, 3, 3, 3, 3, 3, 7, 3, 3, 3, 3, 3, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 3, 3, 3, 5, 5, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1]] \n",
      "\n",
      "capitalization idx for first 5 sentences: \n",
      " [[0, 0, 0, 0, 6, 3, 5, 3, 3, 3, 5, 3, 4, 1, 1, 1, 1], [0, 0, 0, 0, 5, 5, 1, 1, 1, 1], [0, 0, 0, 0, 6, 4, 1, 1, 1, 1], [0, 0, 0, 0, 5, 5, 5, 3, 3, 5, 3, 3, 3, 5, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 1, 1, 1, 1], [0, 0, 0, 0, 5, 4, 3, 3, 3, 5, 5, 4, 3, 3, 5, 5, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 4, 1, 1, 1, 1]] \n",
      "\n",
      "number of sentences = 14041 \n",
      "\n",
      "STEP 2/2 -- WINDOWING\n",
      "sample windows:\n",
      "Vocab for window 0\n",
      "[0, 0, 0, 0, 931, 9970, 198, 583, 10]\n",
      "['<s>', '<s>', '<s>', '<s>', 'eu', 'rejects', 'german', 'call', 'to']\n",
      "PoS tags for window 0\n",
      "[0, 0, 0, 0, 3, 22, 8, 4, 17]\n",
      "['<s>', '<s>', '<s>', '<s>', 'NNP', 'VBZ', 'JJ', 'NN', 'TO']\n",
      "Capitalization tags for window 0\n",
      "[0, 0, 0, 0, 6, 3, 5, 3, 3]\n",
      "['<s>', '<s>', '<s>', '<s>', 'allCaps', 'lowercase', 'upperInitial', 'lowercase', 'lowercase']\n",
      "NER tags for center word\n",
      "5\n",
      "['I-ORG'] \n",
      "\n",
      "Vocab for window 1\n",
      "[0, 0, 0, 931, 9970, 198, 583, 10, 3751]\n",
      "['<s>', '<s>', '<s>', 'eu', 'rejects', 'german', 'call', 'to', 'boycott']\n",
      "PoS tags for window 1\n",
      "[0, 0, 0, 3, 22, 8, 4, 17, 13]\n",
      "['<s>', '<s>', '<s>', 'NNP', 'VBZ', 'JJ', 'NN', 'TO', 'VB']\n",
      "Capitalization tags for window 1\n",
      "[0, 0, 0, 6, 3, 5, 3, 3, 3]\n",
      "['<s>', '<s>', '<s>', 'allCaps', 'lowercase', 'upperInitial', 'lowercase', 'lowercase', 'lowercase']\n",
      "NER tags for center word\n",
      "3\n",
      "['O'] \n",
      "\n",
      "Vocab for window 2\n",
      "[0, 0, 931, 9970, 198, 583, 10, 3751, 207]\n",
      "['<s>', '<s>', 'eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british']\n",
      "PoS tags for window 2\n",
      "[0, 0, 3, 22, 8, 4, 17, 13, 8]\n",
      "['<s>', '<s>', 'NNP', 'VBZ', 'JJ', 'NN', 'TO', 'VB', 'JJ']\n",
      "Capitalization tags for window 2\n",
      "[0, 0, 6, 3, 5, 3, 3, 3, 5]\n",
      "['<s>', '<s>', 'allCaps', 'lowercase', 'upperInitial', 'lowercase', 'lowercase', 'lowercase', 'upperInitial']\n",
      "NER tags for center word\n",
      "7\n",
      "['I-MISC'] \n",
      "\n",
      "rows of vocab features = 203621\n",
      "rows of PoS features = 203621\n",
      "rows of Capitalization features = 203621\n",
      "rows of NER features = 203621\n",
      "numpy feature arrays are returned\n",
      "'formatWindowedData'  1989.43 ms\n",
      "----------------------------------------------------\n",
      "reading file from path ../data/conll2003/eng.testa\n",
      "number of sentences on file = 3250\n",
      "first 5 sentences:\n",
      "[[['cricket', 'NNP', 'O', 'allCaps'], ['-', ':', 'O', 'noinfo'], ['leicestershire', 'NNP', 'I-ORG', 'allCaps'], ['take', 'NNP', 'O', 'allCaps'], ['over', 'IN', 'O', 'allCaps'], ['at', 'NNP', 'O', 'allCaps'], ['top', 'NNP', 'O', 'allCaps'], ['after', 'NNP', 'O', 'allCaps'], ['innings', 'NNP', 'O', 'allCaps'], ['victory', 'NN', 'O', 'allCaps'], ['.', '.', 'O', 'noinfo']], [['london', 'NNP', 'I-LOC', 'allCaps'], ['DGDGDGDG-DGDG-DGDG', 'CD', 'O', 'noinfo']], [['west', 'NNP', 'I-MISC', 'upperInitial'], ['indian', 'NNP', 'I-MISC', 'upperInitial'], ['all-rounder', 'NN', 'O', 'noinfo'], ['phil', 'NNP', 'I-PER', 'upperInitial'], ['simmons', 'NNP', 'I-PER', 'upperInitial'], ['took', 'VBD', 'O', 'lowercase'], ['four', 'CD', 'O', 'lowercase'], ['for', 'IN', 'O', 'lowercase'], ['DGDG', 'CD', 'O', 'noinfo'], ['on', 'IN', 'O', 'lowercase'], ['friday', 'NNP', 'O', 'upperInitial'], ['as', 'IN', 'O', 'lowercase'], ['leicestershire', 'NNP', 'I-ORG', 'upperInitial'], ['beat', 'VBD', 'O', 'lowercase'], ['somerset', 'NNP', 'I-ORG', 'upperInitial'], ['by', 'IN', 'O', 'lowercase'], ['an', 'DT', 'O', 'lowercase'], ['innings', 'NN', 'O', 'lowercase'], ['and', 'CC', 'O', 'lowercase'], ['DGDG', 'CD', 'O', 'noinfo'], ['runs', 'NNS', 'O', 'lowercase'], ['in', 'IN', 'O', 'lowercase'], ['two', 'CD', 'O', 'lowercase'], ['days', 'NNS', 'O', 'lowercase'], ['to', 'TO', 'O', 'lowercase'], ['take', 'VB', 'O', 'lowercase'], ['over', 'IN', 'O', 'lowercase'], ['at', 'IN', 'O', 'lowercase'], ['the', 'DT', 'O', 'lowercase'], ['head', 'NN', 'O', 'lowercase'], ['of', 'IN', 'O', 'lowercase'], ['the', 'DT', 'O', 'lowercase'], ['county', 'NN', 'O', 'lowercase'], ['championship', 'NN', 'O', 'lowercase'], ['.', '.', 'O', 'noinfo']], [['their', 'PRP$', 'O', 'upperInitial'], ['stay', 'NN', 'O', 'lowercase'], ['on', 'IN', 'O', 'lowercase'], ['top', 'NN', 'O', 'lowercase'], [',', ',', 'O', 'noinfo'], ['though', 'RB', 'O', 'lowercase'], [',', ',', 'O', 'noinfo'], ['may', 'MD', 'O', 'lowercase'], ['be', 'VB', 'O', 'lowercase'], ['short-lived', 'JJ', 'O', 'noinfo'], ['as', 'IN', 'O', 'lowercase'], ['title', 'NN', 'O', 'lowercase'], ['rivals', 'NNS', 'O', 'lowercase'], ['essex', 'NNP', 'I-ORG', 'upperInitial'], [',', ',', 'O', 'noinfo'], ['derbyshire', 'NNP', 'I-ORG', 'upperInitial'], ['and', 'CC', 'O', 'lowercase'], ['surrey', 'NNP', 'I-ORG', 'upperInitial'], ['all', 'DT', 'O', 'lowercase'], ['closed', 'VBD', 'O', 'lowercase'], ['in', 'RP', 'O', 'lowercase'], ['on', 'IN', 'O', 'lowercase'], ['victory', 'NN', 'O', 'lowercase'], ['while', 'IN', 'O', 'lowercase'], ['kent', 'NNP', 'I-ORG', 'upperInitial'], ['made', 'VBD', 'O', 'lowercase'], ['up', 'RP', 'O', 'lowercase'], ['for', 'IN', 'O', 'lowercase'], ['lost', 'VBN', 'O', 'lowercase'], ['time', 'NN', 'O', 'lowercase'], ['in', 'IN', 'O', 'lowercase'], ['their', 'PRP$', 'O', 'lowercase'], ['rain-affected', 'JJ', 'O', 'noinfo'], ['match', 'NN', 'O', 'lowercase'], ['against', 'IN', 'O', 'lowercase'], ['nottinghamshire', 'NNP', 'I-ORG', 'upperInitial'], ['.', '.', 'O', 'noinfo']], [['after', 'IN', 'O', 'upperInitial'], ['bowling', 'VBG', 'O', 'lowercase'], ['somerset', 'NNP', 'I-ORG', 'upperInitial'], ['out', 'RP', 'O', 'lowercase'], ['for', 'IN', 'O', 'lowercase'], ['DGDG', 'CD', 'O', 'noinfo'], ['on', 'IN', 'O', 'lowercase'], ['the', 'DT', 'O', 'lowercase'], ['opening', 'NN', 'O', 'lowercase'], ['morning', 'NN', 'O', 'lowercase'], ['at', 'IN', 'O', 'lowercase'], ['grace', 'NNP', 'I-LOC', 'upperInitial'], ['road', 'NNP', 'I-LOC', 'upperInitial'], [',', ',', 'O', 'noinfo'], ['leicestershire', 'NNP', 'I-ORG', 'upperInitial'], ['extended', 'VBD', 'O', 'lowercase'], ['their', 'PRP$', 'O', 'lowercase'], ['first', 'JJ', 'O', 'lowercase'], ['innings', 'NN', 'O', 'lowercase'], ['by', 'IN', 'O', 'lowercase'], ['DGDG', 'CD', 'O', 'noinfo'], ['runs', 'VBZ', 'O', 'lowercase'], ['before', 'IN', 'O', 'lowercase'], ['being', 'VBG', 'O', 'lowercase'], ['bowled', 'VBD', 'O', 'lowercase'], ['out', 'RP', 'O', 'lowercase'], ['for', 'IN', 'O', 'lowercase'], ['DGDGDG', 'CD', 'O', 'noinfo'], ['with', 'IN', 'O', 'lowercase'], ['england', 'NNP', 'I-LOC', 'upperInitial'], ['discard', 'VBP', 'O', 'lowercase'], ['andy', 'NNP', 'I-PER', 'upperInitial'], ['caddick', 'NNP', 'I-PER', 'upperInitial'], ['taking', 'VBG', 'O', 'lowercase'], ['three', 'CD', 'O', 'lowercase'], ['for', 'IN', 'O', 'lowercase'], ['DGDG', 'CD', 'O', 'noinfo'], ['.', '.', 'O', 'noinfo']]]\n",
      "'readFile'  249.38 ms\n",
      "----------------------------------------------------\n",
      "formatting sentences into input windows...\n",
      "STEP 1/2 -- PADDING\n",
      "all sentences padded with 4 pads to either end\n",
      "vocab idx for first 5 sentences:\n",
      " [[0, 0, 0, 0, 279, 20, 2331, 246, 76, 21, 358, 45, 274, 292, 4, 1, 1, 1, 1], [0, 0, 0, 0, 104, 26, 1, 1, 1, 1], [0, 0, 0, 0, 232, 892, 5610, 3281, 5588, 270, 155, 19, 9, 16, 101, 35, 2331, 60, 2900, 32, 41, 274, 12, 9, 356, 8, 52, 272, 10, 246, 76, 21, 3, 505, 7, 3, 642, 293, 4, 1, 1, 1, 1], [0, 0, 0, 0, 59, 1143, 16, 358, 5, 1625, 5, 187, 44, 14010, 35, 680, 2540, 1248, 5, 6256, 12, 1676, 123, 491, 8, 16, 292, 191, 2585, 189, 65, 19, 197, 125, 8, 59, 15311, 136, 82, 2332, 4, 1, 1, 1, 1], [0, 0, 0, 0, 45, 1126, 2900, 71, 19, 9, 16, 3, 728, 517, 21, 9131, 740, 5, 2331, 3259, 59, 47, 274, 32, 9, 356, 116, 230, 4015, 71, 19, 24, 27, 122, 2, 2604, 9110, 435, 91, 19, 9, 4, 1, 1, 1, 1]] \n",
      "\n",
      "pos idx for first 5 sentences:\n",
      " [[0, 0, 0, 0, 3, 23, 3, 3, 6, 3, 3, 3, 3, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 3, 5, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 4, 3, 3, 10, 5, 6, 5, 6, 3, 6, 3, 10, 3, 6, 7, 4, 16, 5, 9, 6, 5, 9, 17, 13, 6, 6, 7, 4, 6, 7, 4, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 26, 4, 6, 4, 12, 15, 12, 28, 13, 8, 6, 4, 9, 3, 12, 3, 16, 3, 7, 10, 31, 6, 4, 6, 3, 10, 31, 6, 14, 4, 6, 26, 8, 4, 6, 3, 11, 1, 1, 1, 1], [0, 0, 0, 0, 6, 21, 3, 31, 6, 5, 6, 7, 4, 4, 6, 3, 3, 12, 3, 10, 26, 8, 4, 6, 5, 22, 6, 21, 10, 31, 6, 5, 6, 3, 27, 3, 3, 21, 5, 6, 5, 11, 1, 1, 1, 1]] \n",
      "\n",
      "ner idx for first 5 sentences:\n",
      " [[0, 0, 0, 0, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 1, 1, 1, 1], [0, 0, 0, 0, 7, 7, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 5, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3, 5, 3, 5, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 6, 6, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 4, 4, 3, 3, 3, 3, 3, 1, 1, 1, 1]] \n",
      "\n",
      "capitalization idx for first 5 sentences: \n",
      " [[0, 0, 0, 0, 6, 4, 6, 6, 6, 6, 6, 6, 6, 6, 4, 1, 1, 1, 1], [0, 0, 0, 0, 6, 4, 1, 1, 1, 1], [0, 0, 0, 0, 5, 5, 4, 5, 5, 3, 3, 3, 4, 3, 5, 3, 5, 3, 5, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 1, 1, 1, 1], [0, 0, 0, 0, 5, 3, 3, 3, 4, 3, 4, 3, 3, 4, 3, 3, 3, 5, 4, 5, 3, 5, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 5, 4, 1, 1, 1, 1], [0, 0, 0, 0, 5, 3, 5, 3, 3, 4, 3, 3, 3, 3, 3, 5, 5, 4, 5, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 4, 3, 5, 3, 5, 5, 3, 3, 3, 4, 4, 1, 1, 1, 1]] \n",
      "\n",
      "number of sentences = 3250 \n",
      "\n",
      "STEP 2/2 -- WINDOWING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample windows:\n",
      "Vocab for window 0\n",
      "[0, 0, 0, 0, 279, 20, 2331, 246, 76]\n",
      "['<s>', '<s>', '<s>', '<s>', 'cricket', '-', 'leicestershire', 'take', 'over']\n",
      "PoS tags for window 0\n",
      "[0, 0, 0, 0, 3, 23, 3, 3, 6]\n",
      "['<s>', '<s>', '<s>', '<s>', 'NNP', ':', 'NNP', 'NNP', 'IN']\n",
      "Capitalization tags for window 0\n",
      "[0, 0, 0, 0, 6, 4, 6, 6, 6]\n",
      "['<s>', '<s>', '<s>', '<s>', 'allCaps', 'noinfo', 'allCaps', 'allCaps', 'allCaps']\n",
      "NER tags for center word\n",
      "3\n",
      "['O'] \n",
      "\n",
      "Vocab for window 1\n",
      "[0, 0, 0, 279, 20, 2331, 246, 76, 21]\n",
      "['<s>', '<s>', '<s>', 'cricket', '-', 'leicestershire', 'take', 'over', 'at']\n",
      "PoS tags for window 1\n",
      "[0, 0, 0, 3, 23, 3, 3, 6, 3]\n",
      "['<s>', '<s>', '<s>', 'NNP', ':', 'NNP', 'NNP', 'IN', 'NNP']\n",
      "Capitalization tags for window 1\n",
      "[0, 0, 0, 6, 4, 6, 6, 6, 6]\n",
      "['<s>', '<s>', '<s>', 'allCaps', 'noinfo', 'allCaps', 'allCaps', 'allCaps', 'allCaps']\n",
      "NER tags for center word\n",
      "3\n",
      "['O'] \n",
      "\n",
      "Vocab for window 2\n",
      "[0, 0, 279, 20, 2331, 246, 76, 21, 358]\n",
      "['<s>', '<s>', 'cricket', '-', 'leicestershire', 'take', 'over', 'at', 'top']\n",
      "PoS tags for window 2\n",
      "[0, 0, 3, 23, 3, 3, 6, 3, 3]\n",
      "['<s>', '<s>', 'NNP', ':', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']\n",
      "Capitalization tags for window 2\n",
      "[0, 0, 6, 4, 6, 6, 6, 6, 6]\n",
      "['<s>', '<s>', 'allCaps', 'noinfo', 'allCaps', 'allCaps', 'allCaps', 'allCaps', 'allCaps']\n",
      "NER tags for center word\n",
      "5\n",
      "['I-ORG'] \n",
      "\n",
      "rows of vocab features = 51362\n",
      "rows of PoS features = 51362\n",
      "rows of Capitalization features = 51362\n",
      "rows of NER features = 51362\n",
      "numpy feature arrays are returned\n",
      "'formatWindowedData'  481.53 ms\n",
      "----------------------------------------------------\n",
      "reading file from path ../data/conll2003/eng.testb\n",
      "number of sentences on file = 3453\n",
      "first 5 sentences:\n",
      "[[['soccer', 'NN', 'O', 'allCaps'], ['-', ':', 'O', 'noinfo'], ['japan', 'NNP', 'I-LOC', 'allCaps'], ['get', 'VB', 'O', 'allCaps'], ['lucky', 'NNP', 'O', 'allCaps'], ['win', 'NNP', 'O', 'allCaps'], [',', ',', 'O', 'noinfo'], ['china', 'NNP', 'I-PER', 'allCaps'], ['in', 'IN', 'O', 'allCaps'], ['surprise', 'DT', 'O', 'allCaps'], ['defeat', 'NN', 'O', 'allCaps'], ['.', '.', 'O', 'noinfo']], [['nadim', 'NNP', 'I-PER', 'upperInitial'], ['ladki', 'NNP', 'I-PER', 'upperInitial']], [['al-ain', 'NNP', 'I-LOC', 'noinfo'], [',', ',', 'O', 'noinfo'], ['united', 'NNP', 'I-LOC', 'upperInitial'], ['arab', 'NNP', 'I-LOC', 'upperInitial'], ['emirates', 'NNPS', 'I-LOC', 'upperInitial'], ['DGDGDGDG-DGDG-DGDG', 'CD', 'O', 'noinfo']], [['japan', 'NNP', 'I-LOC', 'upperInitial'], ['began', 'VBD', 'O', 'lowercase'], ['the', 'DT', 'O', 'lowercase'], ['defence', 'NN', 'O', 'lowercase'], ['of', 'IN', 'O', 'lowercase'], ['their', 'PRP$', 'O', 'lowercase'], ['asian', 'JJ', 'I-MISC', 'upperInitial'], ['cup', 'NNP', 'I-MISC', 'upperInitial'], ['title', 'NN', 'O', 'lowercase'], ['with', 'IN', 'O', 'lowercase'], ['a', 'DT', 'O', 'lowercase'], ['lucky', 'JJ', 'O', 'lowercase'], ['DG-DG', 'CD', 'O', 'noinfo'], ['win', 'VBP', 'O', 'lowercase'], ['against', 'IN', 'O', 'lowercase'], ['syria', 'NNP', 'I-LOC', 'upperInitial'], ['in', 'IN', 'O', 'lowercase'], ['a', 'DT', 'O', 'lowercase'], ['group', 'NNP', 'O', 'upperInitial'], ['c', 'NNP', 'O', 'allCaps'], ['championship', 'NN', 'O', 'lowercase'], ['match', 'NN', 'O', 'lowercase'], ['on', 'IN', 'O', 'lowercase'], ['friday', 'NNP', 'O', 'upperInitial'], ['.', '.', 'O', 'noinfo']], [['but', 'CC', 'O', 'upperInitial'], ['china', 'NNP', 'I-LOC', 'upperInitial'], ['saw', 'VBD', 'O', 'lowercase'], ['their', 'PRP$', 'O', 'lowercase'], ['luck', 'NN', 'O', 'lowercase'], ['desert', 'VB', 'O', 'lowercase'], ['them', 'PRP', 'O', 'lowercase'], ['in', 'IN', 'O', 'lowercase'], ['the', 'DT', 'O', 'lowercase'], ['second', 'NN', 'O', 'lowercase'], ['match', 'NN', 'O', 'lowercase'], ['of', 'IN', 'O', 'lowercase'], ['the', 'DT', 'O', 'lowercase'], ['group', 'NN', 'O', 'lowercase'], [',', ',', 'O', 'noinfo'], ['crashing', 'VBG', 'O', 'lowercase'], ['to', 'TO', 'O', 'lowercase'], ['a', 'DT', 'O', 'lowercase'], ['surprise', 'NN', 'O', 'lowercase'], ['DG-DG', 'CD', 'O', 'noinfo'], ['defeat', 'NN', 'O', 'lowercase'], ['to', 'TO', 'O', 'lowercase'], ['newcomers', 'NNS', 'O', 'lowercase'], ['uzbekistan', 'NNP', 'I-LOC', 'upperInitial'], ['.', '.', 'O', 'noinfo']]]\n",
      "'readFile'  227.72 ms\n",
      "----------------------------------------------------\n",
      "formatting sentences into input windows...\n",
      "STEP 1/2 -- PADDING\n",
      "all sentences padded with 4 pads to either end\n",
      "vocab idx for first 5 sentences:\n",
      " [[0, 0, 0, 0, 92, 20, 215, 410, 7671, 162, 5, 205, 8, 2628, 994, 4, 1, 1, 1, 1], [0, 0, 0, 0, 2, 2, 1, 1, 1, 1], [0, 0, 0, 0, 2, 5, 168, 1069, 10061, 26, 1, 1, 1, 1], [0, 0, 0, 0, 215, 511, 3, 824, 7, 59, 4709, 160, 680, 27, 11, 7671, 23, 162, 82, 1556, 8, 11, 119, 404, 293, 136, 16, 101, 4, 1, 1, 1, 1], [0, 0, 0, 0, 40, 205, 1201, 59, 5769, 10669, 196, 8, 3, 83, 136, 7, 3, 119, 5, 6084, 10, 11, 2628, 23, 994, 10, 15580, 2, 4, 1, 1, 1, 1]] \n",
      "\n",
      "pos idx for first 5 sentences:\n",
      " [[0, 0, 0, 0, 4, 23, 3, 13, 3, 3, 12, 3, 6, 7, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 12, 3, 3, 29, 5, 1, 1, 1, 1], [0, 0, 0, 0, 3, 10, 7, 4, 6, 26, 8, 3, 4, 6, 7, 8, 5, 27, 6, 3, 6, 7, 3, 3, 4, 4, 6, 3, 11, 1, 1, 1, 1], [0, 0, 0, 0, 16, 3, 10, 26, 4, 13, 18, 6, 7, 4, 4, 6, 7, 4, 12, 21, 17, 7, 4, 5, 4, 17, 9, 3, 11, 1, 1, 1, 1]] \n",
      "\n",
      "ner idx for first 5 sentences:\n",
      " [[0, 0, 0, 0, 3, 3, 6, 3, 3, 3, 3, 4, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 4, 4, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 6, 6, 6, 3, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 3, 3, 3, 3, 7, 7, 3, 3, 3, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 1, 1, 1, 1]] \n",
      "\n",
      "capitalization idx for first 5 sentences: \n",
      " [[0, 0, 0, 0, 6, 4, 6, 6, 6, 6, 4, 6, 6, 6, 6, 4, 1, 1, 1, 1], [0, 0, 0, 0, 5, 5, 1, 1, 1, 1], [0, 0, 0, 0, 4, 4, 5, 5, 5, 4, 1, 1, 1, 1], [0, 0, 0, 0, 5, 3, 3, 3, 3, 3, 5, 5, 3, 3, 3, 3, 4, 3, 3, 5, 3, 3, 5, 6, 3, 3, 3, 5, 4, 1, 1, 1, 1], [0, 0, 0, 0, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 4, 3, 3, 3, 5, 4, 1, 1, 1, 1]] \n",
      "\n",
      "number of sentences = 3453 \n",
      "\n",
      "STEP 2/2 -- WINDOWING\n",
      "sample windows:\n",
      "Vocab for window 0\n",
      "[0, 0, 0, 0, 92, 20, 215, 410, 7671]\n",
      "['<s>', '<s>', '<s>', '<s>', 'soccer', '-', 'japan', 'get', 'lucky']\n",
      "PoS tags for window 0\n",
      "[0, 0, 0, 0, 4, 23, 3, 13, 3]\n",
      "['<s>', '<s>', '<s>', '<s>', 'NN', ':', 'NNP', 'VB', 'NNP']\n",
      "Capitalization tags for window 0\n",
      "[0, 0, 0, 0, 6, 4, 6, 6, 6]\n",
      "['<s>', '<s>', '<s>', '<s>', 'allCaps', 'noinfo', 'allCaps', 'allCaps', 'allCaps']\n",
      "NER tags for center word\n",
      "3\n",
      "['O'] \n",
      "\n",
      "Vocab for window 1\n",
      "[0, 0, 0, 92, 20, 215, 410, 7671, 162]\n",
      "['<s>', '<s>', '<s>', 'soccer', '-', 'japan', 'get', 'lucky', 'win']\n",
      "PoS tags for window 1\n",
      "[0, 0, 0, 4, 23, 3, 13, 3, 3]\n",
      "['<s>', '<s>', '<s>', 'NN', ':', 'NNP', 'VB', 'NNP', 'NNP']\n",
      "Capitalization tags for window 1\n",
      "[0, 0, 0, 6, 4, 6, 6, 6, 6]\n",
      "['<s>', '<s>', '<s>', 'allCaps', 'noinfo', 'allCaps', 'allCaps', 'allCaps', 'allCaps']\n",
      "NER tags for center word\n",
      "3\n",
      "['O'] \n",
      "\n",
      "Vocab for window 2\n",
      "[0, 0, 92, 20, 215, 410, 7671, 162, 5]\n",
      "['<s>', '<s>', 'soccer', '-', 'japan', 'get', 'lucky', 'win', ',']\n",
      "PoS tags for window 2\n",
      "[0, 0, 4, 23, 3, 13, 3, 3, 12]\n",
      "['<s>', '<s>', 'NN', ':', 'NNP', 'VB', 'NNP', 'NNP', ',']\n",
      "Capitalization tags for window 2\n",
      "[0, 0, 6, 4, 6, 6, 6, 6, 4]\n",
      "['<s>', '<s>', 'allCaps', 'noinfo', 'allCaps', 'allCaps', 'allCaps', 'allCaps', 'noinfo']\n",
      "NER tags for center word\n",
      "6\n",
      "['I-LOC'] \n",
      "\n",
      "rows of vocab features = 46435\n",
      "rows of PoS features = 46435\n",
      "rows of Capitalization features = 46435\n",
      "rows of NER features = 46435\n",
      "numpy feature arrays are returned\n",
      "'formatWindowedData'  454.10 ms\n"
     ]
    }
   ],
   "source": [
    "# UPDATES!\n",
    "\n",
    "windowLength = 9\n",
    "testNumSents = 5000\n",
    "\n",
    "# Use training set to build vocab here\n",
    "vocabData = conll2003Data(TRAIN_FILE)\n",
    "vocabData.buildVocab( vocabSize=20000)\n",
    "\n",
    "# Format training data\n",
    "trainX, trainX_pos, trainX_capitals, trainY  = vocabData.formatWindowedData( vocabData.train_sentences, \n",
    "                                                  windowLength=windowLength,\n",
    "                                                  verbose=True)\n",
    "\n",
    "# read in dev data\n",
    "devSents = vocabData.readFile( DEV_FILE)\n",
    "# Format dev data\n",
    "devX, devX_pos, devX_capitals, devY = vocabData.formatWindowedData( devSents, \n",
    "                                              windowLength=windowLength,\n",
    "                                              verbose=True)\n",
    "\n",
    "# read in the test data\n",
    "testSents = vocabData.readFile( TEST_FILE)\n",
    "# Format test data\n",
    "testX, testX_pos, testX_capitals, testY = vocabData.formatWindowedData( testSents, \n",
    "                                                windowLength=windowLength,\n",
    "                                                verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate NER tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 3 7 3 3]\n",
      "[[ 0  0  0  0  3 22  8  4 17]\n",
      " [ 0  0  0  3 22  8  4 17 13]\n",
      " [ 0  0  3 22  8  4 17 13  8]\n",
      " [ 0  3 22  8  4 17 13  8  4]\n",
      " [ 3 22  8  4 17 13  8  4 11]]\n",
      "[[    0     0     0     0   959 11985   235   764     8]\n",
      " [    0     0     0   959 11985   235   764     8  4149]\n",
      " [    0     0   959 11985   235   764     8  4149   211]\n",
      " [    0   959 11985   235   764     8  4149   211  6184]\n",
      " [  959 11985   235   764     8  4149   211  6184     3]]\n"
     ]
    }
   ],
   "source": [
    "# NOT PIPELINE\n",
    "print (trainY[:5])\n",
    "print (trainX_pos[:5])\n",
    "print (trainX[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203621,)\n",
      "(203621, 9)\n",
      "(203621, 9)\n"
     ]
    }
   ],
   "source": [
    "# NOT PIPELINE\n",
    "print (trainY.shape)\n",
    "print (trainX_pos.shape)\n",
    "print (trainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training NER labels\n",
      "{3, 4, 5, 6, 7, 8, 9, 10}\n",
      "['O', 'I-PER', 'I-ORG', 'I-LOC', 'I-MISC', 'B-MISC', 'B-ORG', 'B-LOC']\n",
      "\n",
      "dev NER labels\n",
      "{3, 4, 5, 6, 7, 8}\n",
      "['O', 'I-PER', 'I-ORG', 'I-LOC', 'I-MISC', 'B-MISC']\n",
      "\n",
      "test NER labels\n",
      "{3, 4, 5, 6, 7, 8, 9, 10}\n",
      "['O', 'I-PER', 'I-ORG', 'I-LOC', 'I-MISC', 'B-MISC', 'B-ORG', 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "# NOT PIPELINE\n",
    "print (\"training NER labels\")\n",
    "print (set(trainY))\n",
    "print (vocabData.nerTags.ids_to_words(set(trainY)))\n",
    "print (\"\\ndev NER labels\")\n",
    "print (set(devY))\n",
    "print (vocabData.nerTags.ids_to_words(set(devY)))\n",
    "print (\"\\ntest NER labels\")\n",
    "print (set(testY))\n",
    "print (vocabData.nerTags.ids_to_words(set(testY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'B-LOC': 11,\n",
       "         'B-MISC': 37,\n",
       "         'B-ORG': 24,\n",
       "         'I-LOC': 8286,\n",
       "         'I-MISC': 4556,\n",
       "         'I-ORG': 10001,\n",
       "         'I-PER': 11128,\n",
       "         'O': 169578})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOT PIPELINE\n",
    "vocabData.nerTags.unigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Israel ['retain', 'the', 'Golan', 'Heights', 'Israel', 'captured', 'from', 'Syria', 'in']\n",
      "B-LOC ['O', 'O', 'I-LOC', 'I-LOC', 'B-LOC', 'O', 'O', 'I-LOC', 'O']\n",
      "MT ['<s>', '<s>', '<s>', 'Billings', 'MT', '4.62', 'up', '.01', '4.50']\n",
      "B-LOC ['O', 'O', 'O', 'I-LOC', 'B-LOC', 'O', 'O', 'O', 'O']\n",
      "MT ['<s>', '<s>', '<s>', 'Havre', 'MT', '4.54', 'dn', '.10', '---']\n",
      "B-LOC ['O', 'O', 'O', 'I-LOC', 'B-LOC', 'O', 'O', 'O', 'O']\n",
      "MT ['<s>', '<s>', '<s>', 'Rudyard', 'MT', '4.54', 'dn', '.10', '---']\n",
      "B-LOC ['O', 'O', 'O', 'I-LOC', 'B-LOC', 'O', 'O', 'O', 'O']\n",
      "MT ['<s>', '<s>', 'Wolf', 'Point', 'MT', '4.41', 'dn', '.10', '---']\n",
      "B-LOC ['O', 'O', 'I-LOC', 'I-LOC', 'B-LOC', 'O', 'O', 'O', 'O']\n",
      "OR ['<s>', '<s>', '<s>', 'Portland', 'OR', '5.60', 'up', '.02', '5.1700']\n",
      "B-LOC ['O', 'O', 'O', 'I-LOC', 'B-LOC', 'O', 'O', 'O', 'O']\n",
      "OR ['<s>', '<s>', '<s>', 'Pendleton', 'OR', '---', '---', '---', '---']\n",
      "B-LOC ['O', 'O', 'O', 'I-LOC', 'B-LOC', 'O', 'O', 'O', 'O']\n",
      "WA ['<s>', '<s>', 'Coolee', 'City', 'WA', '5.13', 'up', '.02', '---']\n",
      "B-LOC ['O', 'O', 'I-LOC', 'I-LOC', 'B-LOC', 'O', 'O', 'O', 'O']\n",
      "WA ['<s>', '<s>', '<s>', 'Waterville', 'WA', '5.05', 'up', '.02', '---']\n",
      "B-LOC ['O', 'O', 'O', 'I-LOC', 'B-LOC', 'O', 'O', 'O', 'O']\n",
      "WA ['<s>', '<s>', '<s>', 'Wenatchee', 'WA', '5.15', 'up', '.02', '---']\n",
      "B-LOC ['O', 'O', 'O', 'I-LOC', 'B-LOC', 'O', 'O', 'O', 'O']\n",
      "High ['rainfall', 'in', 'the', 'U.S.', 'High', 'Plains', 'has', 'produced', '<unk>']\n",
      "B-LOC ['O', 'O', 'O', 'I-LOC', 'B-LOC', 'I-LOC', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# NOT PIPELINE\n",
    "from collections import Counter\n",
    "bc = Counter()\n",
    "a = []\n",
    "flatX = trainX.flatten()\n",
    "flatY = trainY.flatten()\n",
    "for j, i in enumerate(flatY):\n",
    "    if i == 10:\n",
    "        a += [j]\n",
    "        #print(vocabData.vocab.id_to_word[flatX[j]])\n",
    "    bc[int(i)] += 1\n",
    "    \n",
    "for j in a:\n",
    "    print( vocabData.vocab.id_to_word[trainX[j][4]], vocabData.vocab.ids_to_words(trainX[j]))\n",
    "    print( vocabData.nerTags.id_to_word[flatY[j]], vocabData.nerTags.ids_to_words(flatY[j-4:j+5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# capsule layers from Xifeng Guo \n",
    "# https://github.com/XifengGuo/CapsNet-Keras\n",
    "from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UPDATES!\n",
    "\n",
    "# encoding 1-hot for ner targets\n",
    "trainY_cat = to_categorical(trainY.astype('float32'))\n",
    "devY_cat = to_categorical(devY.astype('float32'), num_classes=trainY_cat.shape[1])\n",
    "testY_cat = to_categorical(testY.astype('float32'), num_classes=trainY_cat.shape[1])\n",
    "\n",
    "trainY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), trainY_cat)), dtype=np.float)\n",
    "devY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), devY_cat)), dtype=np.float)\n",
    "testY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), testY_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEW!\n",
    "\n",
    "# encoding 1-hot for pos tags\n",
    "trainX_pos_cat = to_categorical(trainX_pos.astype('float32'))\n",
    "devX_pos_cat = to_categorical(devX_pos.astype('float32'), num_classes=trainX_pos_cat.shape[2]) \n",
    "testX_pos_cat = to_categorical(testX_pos.astype('float32'), num_classes=trainX_pos_cat.shape[2])\n",
    "\n",
    "trainX_pos_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), trainX_pos_cat)), dtype=np.float)\n",
    "devX_pos_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), devX_pos_cat)), dtype=np.float)\n",
    "testX_pos_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), testX_pos_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEW!\n",
    "\n",
    "# encoding 1-hot for capitalization info  (\"allCaps\", \"upperInitial\", \"lowercase\", \"mixedCaps\", \"noinfo\")\n",
    "trainX_capitals_cat = to_categorical(trainX_capitals.astype('float32'))\n",
    "devX_capitals_cat = to_categorical(devX_capitals.astype('float32'), num_classes=trainX_capitals_cat.shape[2]) \n",
    "testX_capitals_cat = to_categorical(testX_capitals.astype('float32'), num_classes=trainX_capitals_cat.shape[2])\n",
    "\n",
    "trainX_capitals_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), trainX_capitals_cat)), dtype=np.float)\n",
    "devX_capitals_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), devX_capitals_cat)), dtype=np.float)\n",
    "testX_capitals_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), testX_capitals_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UPDATES!\n",
    "\n",
    "# Define hyperparameters\n",
    "max_features = vocabData.vocab.size # 20000\n",
    "maxlen = trainX.shape[1] # 9  --> window size\n",
    "poslen = trainX_pos_cat.shape[2] # 45 pos classes   #! NEW\n",
    "capitallen = trainX_capitals_cat.shape[2] # 5 capitalization classes #! NEW\n",
    "ner_classes = trainY_cat.shape[1] # 8 \n",
    "embed_dim = 50 # word embedding size\n",
    "num_routing = 3 \n",
    "\n",
    "save_dir = './result'\n",
    "batch_size = 100\n",
    "debug = 2\n",
    "epochs = 5\n",
    "dropout_p = 0.25\n",
    "embed_dropout = 0.25\n",
    "lam_recon = 0.0005\n",
    "\n",
    "#Load train and test data\n",
    "#(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_features)\n",
    "#x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "#x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17748"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#construct embedding matrix\n",
    "\n",
    "vocabData.vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 50)\n",
      "embed_concat_pos (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_caps (?, 8)\n"
     ]
    }
   ],
   "source": [
    "# UPDATES!\n",
    "\n",
    "# input\n",
    "x = Input(shape=(maxlen,))\n",
    "x_pos = Input(shape=(maxlen,poslen))\n",
    "x_capital = Input(shape=(maxlen, capitallen))\n",
    "print (\"x\", x.get_shape())\n",
    "print (\"x_pos\", x_pos.get_shape())\n",
    "print (\"x_capital\", x_capital.get_shape())\n",
    "\n",
    "# feed input into embedding layer\n",
    "embed = Embedding(max_features, embed_dim, input_length=maxlen, embeddings_initializer=\"random_uniform\" )(x)\n",
    "# embed = Embedding(max_features, embed_dim, weights=[embedding_matrix],input_length=maxlen, trainable=False)(x)\n",
    "print (\"embed\", embed.get_shape())\n",
    "\n",
    "# concat embeddings with pos Tags here\n",
    "embed_concat_pos = Concatenate(axis=-1)([embed, x_pos, x_capital])\n",
    "print (\"embed_concat_pos\", embed_concat_pos.get_shape())\n",
    "\n",
    "# feed embeddings into conv1\n",
    "conv1 = Conv1D( filters=256, kernel_size=3, strides=1, padding='valid', activation='relu', name='conv1')(embed_concat_pos)\n",
    "print (\"conv1\", conv1.get_shape())\n",
    "\n",
    "# make primary capsules\n",
    "primarycaps = PrimaryCap(embed, dim_capsule=8, n_channels=32, kernel_size=3, strides=1, padding='valid')\n",
    "print (\"primarycaps\", primarycaps.get_shape())\n",
    "\n",
    "# make ner capsules\n",
    "ner_caps = CapsuleLayer(num_capsule=ner_classes, dim_capsule=16, routings=num_routing, name='nercaps')(primarycaps)\n",
    "print (\"ner_caps\", ner_caps.get_shape())\n",
    "\n",
    "# replace each ner capsuel with its length\n",
    "out_caps = Length(name='out_caps')(ner_caps)\n",
    "print (\"out_caps\", out_caps.get_shape())\n",
    "\n",
    "capsmodel = Model(inputs=[x, x_pos], outputs=[out_caps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving weights and logging\n",
    "log = callbacks.CSVLogger(save_dir + '/log.csv')\n",
    "tb = callbacks.TensorBoard(log_dir=save_dir + '/tensorboard-logs', \n",
    "                           batch_size=batch_size, histogram_freq=debug)\n",
    "checkpoint = callbacks.ModelCheckpoint(save_dir + '/weights-{epoch:02d}.h5', \n",
    "                                       save_best_only=True, \n",
    "                                       save_weights_only=True, \n",
    "                                       verbose=1)\n",
    "\n",
    "def margin_loss(y_true, y_pred):\n",
    "    L = y_true*KB.square(KB.maximum(0., 0.9-y_pred)) + 0.5*(1-y_tue)*KB.square(KB.maximum(0., y_pred-0.1))\n",
    "    return KB.mean(KB.sum(L,1))\n",
    "\n",
    "opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "capsmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
