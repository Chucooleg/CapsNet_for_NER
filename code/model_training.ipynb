{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "> To facilitate a more automated training procedure, the model training is moved to a standalone file.  \n",
    "This keeps Keras much happier in terms of required restarts and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import time # !\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import glove_helper\n",
    "from loadutils import conll2003Data, saveProcessedData\n",
    "from common import vocabulary, utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(\"Tensorflow version:\", tf.__version__)\n",
    "#print(\"Keras version:\", K.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"../data/conll2003/eng.train\"\n",
    "DEV_FILE = \"../data/conll2003/eng.testa\"\n",
    "TEST_FILE = \"../data/conll2003/eng.testb\"\n",
    "\n",
    "# out files for IPC\n",
    "HYPER_PARAM_FILE = \"hyper_params.json\"\n",
    "\n",
    "VOCAB_SIZE = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local helper utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local untils\n",
    "\n",
    "# timeit decorator\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print ('%r  %2.2f ms' % \\\n",
    "                  (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_embedding_matrix(embed_dim, vocab_size):\n",
    "    \"\"\"\n",
    "    construct embedding matrix from GloVe 6Bn word data\n",
    "    \n",
    "    reuse glove_helper code from w266 \n",
    "    \n",
    "    Returns: an embedding matrix directly plugged into keras.layers.Embedding(weights=[embedding_matrix])\n",
    "    \"\"\"\n",
    "    reload(glove_helper)\n",
    "    hands = glove_helper.Hands(ndim=embed_dim)\n",
    "    embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "    \n",
    "    for i in range(vocabData.vocab.size):\n",
    "        word = vocabData.vocab.ids_to_words([i])[0]\n",
    "        try:\n",
    "            embedding_vector = hands.get_vector(word)\n",
    "        except:\n",
    "            embedding_vector = hands.get_vector(\"<unk>\")\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history( history):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "reading file from path ../data/conll2003/eng.train\n",
      "'readFile'  1161.07 ms\n",
      "----------------------------------------------------\n",
      "building vocabulary from TRAINING data...\n",
      "'buildVocab'  1066.03 ms\n",
      "----------------------------------------------------\n",
      "formatting sentences into input windows...\n",
      "'formatWindowedData'  1892.75 ms\n",
      "----------------------------------------------------\n",
      "reading file from path ../data/conll2003/eng.testa\n",
      "'readFile'  255.94 ms\n",
      "----------------------------------------------------\n",
      "formatting sentences into input windows...\n",
      "'formatWindowedData'  427.06 ms\n",
      "----------------------------------------------------\n",
      "reading file from path ../data/conll2003/eng.testb\n",
      "'readFile'  235.91 ms\n",
      "----------------------------------------------------\n",
      "formatting sentences into input windows...\n",
      "'formatWindowedData'  528.62 ms\n"
     ]
    }
   ],
   "source": [
    "# UPDATES!\n",
    "\n",
    "windowLength = 9\n",
    "#testNumSents = 20000\n",
    "\n",
    "# Use training set to build vocab here\n",
    "vocabData = conll2003Data(TRAIN_FILE)\n",
    "vocabData.buildVocab( vocabSize=VOCAB_SIZE)\n",
    "\n",
    "# Format training data\n",
    "trainX, trainX_pos, trainX_capitals, trainY  = vocabData.formatWindowedData( \n",
    "                                                  vocabData.train_sentences, \n",
    "                                                  windowLength=windowLength,\n",
    "                                                  verbose=False)\n",
    "\n",
    "# read in dev data\n",
    "devSents = vocabData.readFile( DEV_FILE)\n",
    "devX, devX_pos, devX_capitals, devY = vocabData.formatWindowedData( \n",
    "                                              devSents, \n",
    "                                              windowLength=windowLength,\n",
    "                                              verbose=False)\n",
    "\n",
    "# read in the test data\n",
    "testSents = vocabData.readFile( TEST_FILE)\n",
    "testX, testX_pos, testX_capitals, testY = vocabData.formatWindowedData( \n",
    "                                                testSents, \n",
    "                                                windowLength=windowLength,\n",
    "                                                verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Y\n",
    "\n",
    "# encoding 1-hot for ner targets\n",
    "trainY_cat = to_categorical(trainY.astype('float32'))\n",
    "devY_cat = to_categorical(devY.astype('float32'), num_classes=trainY_cat.shape[1])\n",
    "testY_cat = to_categorical(testY.astype('float32'), num_classes=trainY_cat.shape[1])\n",
    "\n",
    "trainY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), trainY_cat)), dtype=np.float)\n",
    "devY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), devY_cat)), dtype=np.float)\n",
    "testY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), testY_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X pos tags\n",
    "\n",
    "# encoding 1-hot for pos tags\n",
    "trainX_pos_cat = to_categorical(trainX_pos.astype('float32'))\n",
    "devX_pos_cat = to_categorical(devX_pos.astype('float32'), num_classes=trainX_pos_cat.shape[2]) \n",
    "testX_pos_cat = to_categorical(testX_pos.astype('float32'), num_classes=trainX_pos_cat.shape[2])\n",
    "\n",
    "trainX_pos_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), trainX_pos_cat)), dtype=np.float)\n",
    "devX_pos_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), devX_pos_cat)), dtype=np.float)\n",
    "testX_pos_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), testX_pos_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X capitlization \n",
    "\n",
    "# encoding 1-hot for capitalization info  (\"allCaps\", \"upperInitial\", \"lowercase\", \"mixedCaps\", \"noinfo\")\n",
    "trainX_capitals_cat = to_categorical(trainX_capitals.astype('float32'))\n",
    "devX_capitals_cat = to_categorical(devX_capitals.astype('float32'), num_classes=trainX_capitals_cat.shape[2]) \n",
    "testX_capitals_cat = to_categorical(testX_capitals.astype('float32'), num_classes=trainX_capitals_cat.shape[2])\n",
    "\n",
    "trainX_capitals_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), trainX_capitals_cat)), dtype=np.float)\n",
    "devX_capitals_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), devX_capitals_cat)), dtype=np.float)\n",
    "testX_capitals_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), testX_capitals_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper parameters for model\n",
    "# CAPSNET\n",
    "hyper_param_caps = {\n",
    "    \n",
    "    'max_features' : vocabData.vocab.size,  # 20000\n",
    "    'maxlen' : trainX.shape[1],  # window size (9)\n",
    "    'poslen' : trainX_pos_cat.shape[2],  # pos classes (45)\n",
    "    'capitallen' : trainX_capitals_cat.shape[2],  # capitalization classes (5)\n",
    "    'ner_classes' : trainY_cat.shape[1],  # 8 \n",
    "    'embed_dim' : 50,  # word embedding size\n",
    "    'num_routing' : 3, \n",
    "\n",
    "    'use_glove' : True,\n",
    "    'allow_glove_retrain' : False,\n",
    "    'use_pos_tags' : True,\n",
    "    'use_capitalization_info' : True,    \n",
    "    \n",
    "    'conv1_filters' : 256,\n",
    "    'conv1_kernel_size' : 3,\n",
    "    'conv1_strides' : 1,\n",
    "    'conv1_padding' : 'valid',\n",
    "    \n",
    "    'use_2D_primarycaps' : False,\n",
    "    'primarycaps_dim_capsule' : 8,\n",
    "    'primarycaps_n_channels' : 32,\n",
    "    'primarycaps_kernel_size' : 3,\n",
    "    'primarycaps_strides' : 1,\n",
    "    'primarycaps_padding' : 'valid',\n",
    "\n",
    "    'ner_capsule_dim' : 16,\n",
    "    \n",
    "    'num_dynamic_routing_passes' : 3,\n",
    "    \n",
    "    # decoder is still work in progress\n",
    "    'use_decoder' : False,\n",
    "    'decoder_feed_forward_1' : 100,\n",
    "    'decoder_feed_forward_2' : 100, \n",
    "    \n",
    "    'save_dir' : './result',\n",
    "    'batch_size' : 100,\n",
    "    'debug' : 2,\n",
    "    'epochs' : 5,\n",
    "    'stopping_patience' : 5, # default to same as epochs, ie don't use\n",
    "    'dropout_p' : 0.25,\n",
    "    'embed_dropout' : 0.25,\n",
    "    'lam_recon' : 0.0005,\n",
    "    \n",
    "    'optimizer' : 'Adam', #or 'SGD'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper parameters for model\n",
    "# CNN\n",
    "hyper_param_cnn = {\n",
    "    \n",
    "    'max_features' : vocabData.vocab.size,  # 20000\n",
    "    'maxlen' : trainX.shape[1],  # window size (9)\n",
    "    'poslen' : trainX_pos_cat.shape[2],  # pos classes (45)\n",
    "    'capitallen' : trainX_capitals_cat.shape[2],  # capitalization classes (5)\n",
    "    'ner_classes' : trainY_cat.shape[1],  # 8 \n",
    "    'embed_dim' : 50,  # word embedding size\n",
    "    'num_routing' : 3, \n",
    "\n",
    "    'use_glove' : True,\n",
    "    'allow_glove_retrain' : False,\n",
    "    'use_pos_tags' : True,\n",
    "    'use_capitalization_info' : True,    \n",
    "    \n",
    "    'conv1_filters' : 256,\n",
    "    'conv1_kernel_size' : 3,\n",
    "    'conv1_strides' : 1,\n",
    "    'conv1_padding' : 'valid',\n",
    "    \n",
    "    'conv2_filters' : 256,\n",
    "    'conv2_kernel_size' : 3,\n",
    "    'conv2_strides' : 1,\n",
    "    'conv2_padding' : 'valid',\n",
    "    \n",
    "    'conv3_filters' : 128,\n",
    "    'conv3_kernel_size' : 3,\n",
    "    'conv3_strides' : 1,\n",
    "    'conv3_padding' : 'valid',\n",
    "    \n",
    "    'max_pooling_size' : 3,\n",
    "    'max_pooling_strides' : 1,\n",
    "    'max_pooling_padding' : 'valid',\n",
    "    'maxpool_dropout' : 0.3,\n",
    "    \n",
    "    'feed_forward_1' : 328,\n",
    "    'ff1_dropout' : 0.3,\n",
    "    'feed_forward_2' : 192,\n",
    "    'ff2_dropout' : 0.3,\n",
    "    \n",
    "    'save_dir' : './result',\n",
    "    'batch_size' : 100,\n",
    "    'debug' : 2,\n",
    "    'epochs' : 5,\n",
    "    'stopping_patience' : 5, # default to same as epochs, ie don't use\n",
    "    'dropout_p' : 0.25,\n",
    "    'embed_dropout' : 0.25,  # set to 0 to disable dropout\n",
    "    'lam_recon' : 0.0005,\n",
    "    \n",
    "    'optimizer' : 'Adam', #or 'SGD'\n",
    "    #'loss_function' : margin_loss, # constructed loss function see margin_loss() in this notebook\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Glove Embeddings Matrix and Save All Data to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.50d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 50))\n",
      "17748\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embedding matrix\n",
    "# embedding_matrix = construct_embedding_matrix(hyper_param_caps['embed_dim'])\n",
    "embedding_matrix = construct_embedding_matrix( hyper_param_caps['embed_dim'], \n",
    "                                               hyper_param_caps['max_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all loaded data for use by training process\n",
    "saveProcessedData( trainX, trainX_capitals_cat, trainX_pos_cat, devX, devX_capitals_cat,\n",
    "                   devX_pos_cat, trainY_cat, devY_cat, embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit \n",
    "def trainModelSP( testFunc, modelName, hyper_params, embed_matrix=None, verbose=False):\n",
    "    \"\"\"\n",
    "    testFunc - the name of the python file to run\n",
    "    modelName - the internal name (ID) of the model to train\n",
    "    hyper_params - a dict of hyper parameters\n",
    "    \"\"\"\n",
    "    # save the hyperparams\n",
    "    with open(HYPER_PARAM_FILE, mode='w') as fp:\n",
    "        json.dump( hyper_params, fp)\n",
    "    \n",
    "    # call the train function\n",
    "    # consider replacing with a call to subprocess!!\n",
    "    !python {testFunc} {modelName} {HYPER_PARAM_FILE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit \n",
    "def testFeatures( testFunc, modelName, hyper_params):\n",
    "    \"\"\"\n",
    "    builds and trains 4 models for the configuration in hyper_params,\n",
    "    1 for each input feature configuration: base, pos, caps, pos + caps\n",
    "    \n",
    "    testFunc - the name of the python file to run\n",
    "    modelName - the model name to use for labeling\n",
    "    \"\"\"\n",
    "    hypers = hyper_params.copy()\n",
    "    \n",
    "    # try the embeddings with different features\n",
    "    \n",
    "    # base\n",
    "    curModel = modelName + \"_base\"\n",
    "    trainModelSP( testFunc, curModel, hypers )\n",
    "    \n",
    "    # pos tags\n",
    "    curModel = modelName + \"_pos\"\n",
    "    hypers['use_pos_tags'] = True\n",
    "    hypers['use_capitalization_info'] = False\n",
    "    trainModelSP( testFunc, curModel, hypers )\n",
    "    \n",
    "    # capitalization info\n",
    "    curModel = modelName + \"_caps\"\n",
    "    hypers['use_pos_tags'] = False\n",
    "    hypers['use_capitalization_info'] = True\n",
    "    trainModelSP( testFunc, curModel, hypers )\n",
    "    \n",
    "    # both\n",
    "    curModel = modelName + \"_pos_caps\"\n",
    "    hypers['use_pos_tags'] = True\n",
    "    hypers['use_capitalization_info'] = True\n",
    "    trainModelSP( testFunc, curModel, hypers )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still need:\n",
    "> a function to read in each historylog.csv file, optionally plot, then collect the best scoring epoch (lowest loss? - discuss, add F1?) to determine the best model and how long it trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Primary Caps Layer Training\n",
    "> I know the output isn't pretty, but we don't really need it since everything is stored in the history log... It is really just to show a sign of life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test changing vocab size?...will have to rerun all tests for each size. same for word embedding size. is ok.\n",
    "# change embedding sizes\n",
    "# primary caps conv1D kernel size - play with it!!!\n",
    "# - changing dropout rate will be perhaps in hyperparam tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Learn Embeddings\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "embed (?, 9, 50)\n",
      "embed (?, 9, 50)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_base\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 5, 256)            196864    \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             163840    \n",
      "_________________________________________________________________\n",
      "out_pred (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,286,760\n",
      "Trainable params: 1,286,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-11 23:45:25.683312: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-11 23:45:25.774409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-11 23:45:25.774750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-11 23:45:25.774783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.1052 - acc: 0.9039 - val_loss: 0.0323 - val_acc: 0.9584\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03226, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.0120 - acc: 0.9851 - val_loss: 0.0298 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03226 to 0.02975, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.0062 - acc: 0.9924 - val_loss: 0.0306 - val_acc: 0.9627\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.0042 - acc: 0.9949 - val_loss: 0.0305 - val_acc: 0.9643\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.0030 - acc: 0.9964 - val_loss: 0.0334 - val_acc: 0.9598\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 103s 507us/step - loss: 0.0026 - acc: 0.9969 - val_loss: 0.0321 - val_acc: 0.9633\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.0019 - acc: 0.9976 - val_loss: 0.0316 - val_acc: 0.9634\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 00007: early stopping\n",
      "'trainModelSP'  829949.42 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "embed (?, 9, 95)\n",
      "embed (?, 9, 95)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_pos\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 95)        0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       73216       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,321,320\n",
      "Trainable params: 1,321,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-11 23:59:15.779225: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-11 23:59:15.854737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-11 23:59:15.855065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-11 23:59:15.855096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 116s 569us/step - loss: 0.0452 - acc: 0.9499 - val_loss: 0.0200 - val_acc: 0.9745\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01995, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 116s 572us/step - loss: 0.0071 - acc: 0.9914 - val_loss: 0.0208 - val_acc: 0.9741\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 116s 572us/step - loss: 0.0037 - acc: 0.9957 - val_loss: 0.0201 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 116s 568us/step - loss: 0.0024 - acc: 0.9971 - val_loss: 0.0210 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 116s 570us/step - loss: 0.0017 - acc: 0.9979 - val_loss: 0.0210 - val_acc: 0.9768\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 117s 576us/step - loss: 0.0015 - acc: 0.9983 - val_loss: 0.0222 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 00006: early stopping\n",
      "'trainModelSP'  775470.15 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 55)\n",
      "embed (?, 9, 55)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 55)        0           embedding_1[0][0]                \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       42496       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,290,600\n",
      "Trainable params: 1,290,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 00:12:11.088450: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 00:12:11.160890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 00:12:11.161205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 00:12:11.161234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0409 - acc: 0.9533 - val_loss: 0.0175 - val_acc: 0.9782\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01746, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 107s 526us/step - loss: 0.0060 - acc: 0.9926 - val_loss: 0.0171 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01746 to 0.01708, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0030 - acc: 0.9965 - val_loss: 0.0178 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0021 - acc: 0.9977 - val_loss: 0.0177 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 108s 529us/step - loss: 0.0016 - acc: 0.9981 - val_loss: 0.0203 - val_acc: 0.9770\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 108s 528us/step - loss: 0.0013 - acc: 0.9985 - val_loss: 0.0197 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 107s 528us/step - loss: 0.0012 - acc: 0.9986 - val_loss: 0.0191 - val_acc: 0.9791\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 00007: early stopping\n",
      "'trainModelSP'  848465.60 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_pos_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 100)       0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       77056       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,325,160\n",
      "Trainable params: 1,325,160\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 00:26:19.632773: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 00:26:19.705135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 00:26:19.705541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 00:26:19.705585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0376 - acc: 0.9569 - val_loss: 0.0169 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01691, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 108s 529us/step - loss: 0.0056 - acc: 0.9933 - val_loss: 0.0157 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01691 to 0.01567, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0028 - acc: 0.9967 - val_loss: 0.0173 - val_acc: 0.9797\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 108s 530us/step - loss: 0.0019 - acc: 0.9978 - val_loss: 0.0207 - val_acc: 0.9764\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0015 - acc: 0.9983 - val_loss: 0.0174 - val_acc: 0.9797\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0012 - acc: 0.9986 - val_loss: 0.0176 - val_acc: 0.9809\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 8.7063e-04 - acc: 0.9990 - val_loss: 0.0188 - val_acc: 0.9789\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 00007: early stopping\n",
      "'trainModelSP'  850209.21 ms\n",
      "'testFeatures'  3304094.77 ms\n",
      "\n",
      "\n",
      "Learn Embeddings and Dropout\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "embed (?, 9, 50)\n",
      "embed (?, 9, 50)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_dropout_base\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 9, 50)             0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 5, 256)            196864    \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             163840    \n",
      "_________________________________________________________________\n",
      "out_pred (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,286,760\n",
      "Trainable params: 1,286,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 00:40:29.826558: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 00:40:29.899934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 00:40:29.900251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 00:40:29.900280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.1005 - acc: 0.9035 - val_loss: 0.0319 - val_acc: 0.9585\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03194, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0150 - acc: 0.9810 - val_loss: 0.0322 - val_acc: 0.9564\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.0084 - acc: 0.9896 - val_loss: 0.0303 - val_acc: 0.9601\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03194 to 0.03033, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0060 - acc: 0.9927 - val_loss: 0.0279 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03033 to 0.02788, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0047 - acc: 0.9943 - val_loss: 0.0289 - val_acc: 0.9656\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0039 - acc: 0.9953 - val_loss: 0.0302 - val_acc: 0.9630\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0033 - acc: 0.9959 - val_loss: 0.0316 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.0031 - acc: 0.9962 - val_loss: 0.0304 - val_acc: 0.9642\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0026 - acc: 0.9968 - val_loss: 0.0317 - val_acc: 0.9628\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 00009: early stopping\n",
      "'trainModelSP'  1079163.60 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "embed (?, 9, 95)\n",
      "embed (?, 9, 95)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_dropout_pos\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 95)        0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 95)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       73216       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,321,320\n",
      "Trainable params: 1,321,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 00:58:28.996657: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 00:58:29.068441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 00:58:29.068843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 00:58:29.068873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.0536 - acc: 0.9376 - val_loss: 0.0227 - val_acc: 0.9713\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02274, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0118 - acc: 0.9849 - val_loss: 0.0239 - val_acc: 0.9696\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0072 - acc: 0.9909 - val_loss: 0.0224 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02274 to 0.02235, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 108s 532us/step - loss: 0.0053 - acc: 0.9934 - val_loss: 0.0207 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02235 to 0.02068, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0042 - acc: 0.9948 - val_loss: 0.0208 - val_acc: 0.9756\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 107s 523us/step - loss: 0.0034 - acc: 0.9957 - val_loss: 0.0220 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.0030 - acc: 0.9963 - val_loss: 0.0221 - val_acc: 0.9741\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0026 - acc: 0.9967 - val_loss: 0.0220 - val_acc: 0.9752\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0021 - acc: 0.9973 - val_loss: 0.0217 - val_acc: 0.9747\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 00009: early stopping\n",
      "'trainModelSP'  1082428.43 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 55)\n",
      "embed (?, 9, 55)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_dropout_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 55)        0           embedding_1[0][0]                \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 55)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       42496       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,290,600\n",
      "Trainable params: 1,290,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 01:16:31.400023: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 01:16:31.473649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 01:16:31.473978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 01:16:31.474035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 117s 577us/step - loss: 0.0516 - acc: 0.9384 - val_loss: 0.0186 - val_acc: 0.9766\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01857, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 117s 574us/step - loss: 0.0105 - acc: 0.9868 - val_loss: 0.0174 - val_acc: 0.9785\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01857 to 0.01738, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 118s 581us/step - loss: 0.0062 - acc: 0.9923 - val_loss: 0.0187 - val_acc: 0.9771\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 119s 583us/step - loss: 0.0048 - acc: 0.9940 - val_loss: 0.0188 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621/203621 [==============================] - 118s 580us/step - loss: 0.0040 - acc: 0.9950 - val_loss: 0.0178 - val_acc: 0.9791\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 119s 583us/step - loss: 0.0032 - acc: 0.9961 - val_loss: 0.0184 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 118s 582us/step - loss: 0.0027 - acc: 0.9966 - val_loss: 0.0181 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 00007: early stopping\n",
      "'trainModelSP'  927465.92 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_dropout_pos_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 100)       0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 100)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       77056       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,325,160\n",
      "Trainable params: 1,325,160\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 01:31:58.913577: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 01:31:58.988748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 01:31:58.989100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 01:31:58.989157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 115s 567us/step - loss: 0.0457 - acc: 0.9456 - val_loss: 0.0219 - val_acc: 0.9710\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02191, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 116s 568us/step - loss: 0.0094 - acc: 0.9883 - val_loss: 0.0220 - val_acc: 0.9728\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 114s 560us/step - loss: 0.0057 - acc: 0.9931 - val_loss: 0.0224 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 114s 560us/step - loss: 0.0042 - acc: 0.9949 - val_loss: 0.0234 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 114s 561us/step - loss: 0.0033 - acc: 0.9959 - val_loss: 0.0228 - val_acc: 0.9729\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 115s 563us/step - loss: 0.0030 - acc: 0.9963 - val_loss: 0.0220 - val_acc: 0.9731\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 00006: early stopping\n",
      "'trainModelSP'  765590.40 ms\n",
      "'testFeatures'  3854648.80 ms\n",
      "\n",
      "\n",
      "Glove Embeddings\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 01:44:43.256055: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 01:44:43.338023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 01:44:43.338445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 01:44:43.338480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "embed (?, 9, 50)\n",
      "embed (?, 9, 50)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_nolearn_base\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 5, 256)            196864    \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             163840    \n",
      "_________________________________________________________________\n",
      "out_pred (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,286,760\n",
      "Trainable params: 399,360\n",
      "Non-trainable params: 887,400\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 114s 558us/step - loss: 0.0467 - acc: 0.9431 - val_loss: 0.0390 - val_acc: 0.9516\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03897, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 114s 562us/step - loss: 0.0237 - acc: 0.9696 - val_loss: 0.0342 - val_acc: 0.9558\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03897 to 0.03421, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 115s 563us/step - loss: 0.0185 - acc: 0.9765 - val_loss: 0.0352 - val_acc: 0.9557\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 114s 558us/step - loss: 0.0152 - acc: 0.9808 - val_loss: 0.0335 - val_acc: 0.9572\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03421 to 0.03348, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 113s 557us/step - loss: 0.0130 - acc: 0.9836 - val_loss: 0.0342 - val_acc: 0.9571\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 113s 555us/step - loss: 0.0114 - acc: 0.9855 - val_loss: 0.0328 - val_acc: 0.9595\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03348 to 0.03279, saving model to ./result/weights-06.h5\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 114s 561us/step - loss: 0.0099 - acc: 0.9872 - val_loss: 0.0338 - val_acc: 0.9581\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 114s 559us/step - loss: 0.0088 - acc: 0.9886 - val_loss: 0.0350 - val_acc: 0.9582\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 113s 557us/step - loss: 0.0079 - acc: 0.9898 - val_loss: 0.0338 - val_acc: 0.9593\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 114s 561us/step - loss: 0.0072 - acc: 0.9907 - val_loss: 0.0330 - val_acc: 0.9604\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 114s 562us/step - loss: 0.0065 - acc: 0.9916 - val_loss: 0.0334 - val_acc: 0.9608\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 00011: early stopping\n",
      "'trainModelSP'  1395999.14 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 02:07:59.129715: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 02:07:59.205000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 02:07:59.205336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 02:07:59.205383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "embed (?, 9, 95)\n",
      "embed (?, 9, 95)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_nolearn_pos\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 95)        0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       73216       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,321,320\n",
      "Trainable params: 433,920\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 107s 523us/step - loss: 0.0354 - acc: 0.9582 - val_loss: 0.0224 - val_acc: 0.9709\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02241, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0147 - acc: 0.9817 - val_loss: 0.0206 - val_acc: 0.9739\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02241 to 0.02061, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.0109 - acc: 0.9866 - val_loss: 0.0203 - val_acc: 0.9745\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02061 to 0.02028, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.0085 - acc: 0.9898 - val_loss: 0.0211 - val_acc: 0.9741\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.0070 - acc: 0.9914 - val_loss: 0.0205 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.0059 - acc: 0.9928 - val_loss: 0.0204 - val_acc: 0.9752\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0050 - acc: 0.9938 - val_loss: 0.0201 - val_acc: 0.9761\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02028 to 0.02009, saving model to ./result/weights-07.h5\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.0043 - acc: 0.9947 - val_loss: 0.0209 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0040 - acc: 0.9949 - val_loss: 0.0213 - val_acc: 0.9740\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.0033 - acc: 0.9958 - val_loss: 0.0214 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.0031 - acc: 0.9958 - val_loss: 0.0203 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0027 - acc: 0.9965 - val_loss: 0.0222 - val_acc: 0.9737\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 00012: early stopping\n",
      "'trainModelSP'  1407255.18 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 02:31:26.386825: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 02:31:26.463062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 02:31:26.463410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 02:31:26.463441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 55)\n",
      "embed (?, 9, 55)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_nolearn_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 55)        0           embedding_1[0][0]                \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       42496       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,290,600\n",
      "Trainable params: 403,200\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.0336 - acc: 0.9610 - val_loss: 0.0196 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01957, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.0128 - acc: 0.9845 - val_loss: 0.0167 - val_acc: 0.9786\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01957 to 0.01670, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0093 - acc: 0.9887 - val_loss: 0.0172 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.0073 - acc: 0.9915 - val_loss: 0.0159 - val_acc: 0.9805\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01670 to 0.01588, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.0060 - acc: 0.9927 - val_loss: 0.0163 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0050 - acc: 0.9940 - val_loss: 0.0158 - val_acc: 0.9806\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01588 to 0.01584, saving model to ./result/weights-06.h5\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0043 - acc: 0.9948 - val_loss: 0.0169 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0036 - acc: 0.9955 - val_loss: 0.0166 - val_acc: 0.9806\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0033 - acc: 0.9959 - val_loss: 0.0176 - val_acc: 0.9786\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.0030 - acc: 0.9962 - val_loss: 0.0165 - val_acc: 0.9803\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 105s 513us/step - loss: 0.0027 - acc: 0.9965 - val_loss: 0.0163 - val_acc: 0.9806\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 00011: early stopping\n",
      "'trainModelSP'  1303142.99 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 02:53:09.436715: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 02:53:09.509305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 02:53:09.509617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 02:53:09.509644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model: glove_nolearn_pos_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 100)       0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       77056       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,325,160\n",
      "Trainable params: 437,760\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 118s 577us/step - loss: 0.0311 - acc: 0.9634 - val_loss: 0.0194 - val_acc: 0.9769\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01939, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 117s 574us/step - loss: 0.0116 - acc: 0.9856 - val_loss: 0.0178 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01939 to 0.01779, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 116s 572us/step - loss: 0.0082 - acc: 0.9901 - val_loss: 0.0152 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01779 to 0.01523, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 117s 573us/step - loss: 0.0063 - acc: 0.9925 - val_loss: 0.0154 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 117s 576us/step - loss: 0.0051 - acc: 0.9938 - val_loss: 0.0167 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 116s 571us/step - loss: 0.0041 - acc: 0.9949 - val_loss: 0.0164 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 117s 576us/step - loss: 0.0035 - acc: 0.9958 - val_loss: 0.0161 - val_acc: 0.9806\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 117s 576us/step - loss: 0.0030 - acc: 0.9962 - val_loss: 0.0162 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 00008: early stopping\n",
      "'trainModelSP'  1034228.23 ms\n",
      "'testFeatures'  5140625.96 ms\n",
      "\n",
      "\n",
      "Glove Embeddings and Dropout\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 03:10:23.724395: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 03:10:23.799678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 03:10:23.800030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 03:10:23.800060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "embed (?, 9, 50)\n",
      "embed (?, 9, 50)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_nolearn_dropout_base\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 9, 50)             0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 5, 256)            196864    \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             163840    \n",
      "_________________________________________________________________\n",
      "out_pred (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,286,760\n",
      "Trainable params: 399,360\n",
      "Non-trainable params: 887,400\n",
      "_________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 112s 548us/step - loss: 0.0590 - acc: 0.9261 - val_loss: 0.0399 - val_acc: 0.9484\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03987, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 111s 544us/step - loss: 0.0359 - acc: 0.9530 - val_loss: 0.0385 - val_acc: 0.9506\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03987 to 0.03854, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621/203621 [==============================] - 111s 544us/step - loss: 0.0312 - acc: 0.9591 - val_loss: 0.0372 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03854 to 0.03720, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 110s 541us/step - loss: 0.0285 - acc: 0.9621 - val_loss: 0.0332 - val_acc: 0.9567\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03720 to 0.03318, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 110s 541us/step - loss: 0.0269 - acc: 0.9640 - val_loss: 0.0321 - val_acc: 0.9584\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03318 to 0.03211, saving model to ./result/weights-05.h5\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 111s 546us/step - loss: 0.0249 - acc: 0.9674 - val_loss: 0.0320 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03211 to 0.03204, saving model to ./result/weights-06.h5\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 110s 541us/step - loss: 0.0236 - acc: 0.9688 - val_loss: 0.0320 - val_acc: 0.9592\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03204 to 0.03197, saving model to ./result/weights-07.h5\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 111s 545us/step - loss: 0.0227 - acc: 0.9698 - val_loss: 0.0324 - val_acc: 0.9591\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 110s 538us/step - loss: 0.0215 - acc: 0.9711 - val_loss: 0.0332 - val_acc: 0.9594\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 111s 544us/step - loss: 0.0207 - acc: 0.9722 - val_loss: 0.0299 - val_acc: 0.9617\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03197 to 0.02995, saving model to ./result/weights-10.h5\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 112s 551us/step - loss: 0.0199 - acc: 0.9733 - val_loss: 0.0304 - val_acc: 0.9609\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "203621/203621 [==============================] - 112s 548us/step - loss: 0.0191 - acc: 0.9750 - val_loss: 0.0310 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/50\n",
      "203621/203621 [==============================] - 110s 540us/step - loss: 0.0187 - acc: 0.9753 - val_loss: 0.0310 - val_acc: 0.9612\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/50\n",
      "203621/203621 [==============================] - 111s 544us/step - loss: 0.0179 - acc: 0.9763 - val_loss: 0.0299 - val_acc: 0.9624\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02995 to 0.02989, saving model to ./result/weights-14.h5\n",
      "Epoch 15/50\n",
      "203621/203621 [==============================] - 113s 553us/step - loss: 0.0173 - acc: 0.9767 - val_loss: 0.0298 - val_acc: 0.9631\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02989 to 0.02976, saving model to ./result/weights-15.h5\n",
      "Epoch 16/50\n",
      "203621/203621 [==============================] - 112s 549us/step - loss: 0.0169 - acc: 0.9776 - val_loss: 0.0290 - val_acc: 0.9636\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02976 to 0.02903, saving model to ./result/weights-16.h5\n",
      "Epoch 17/50\n",
      "203621/203621 [==============================] - 110s 541us/step - loss: 0.0164 - acc: 0.9784 - val_loss: 0.0292 - val_acc: 0.9631\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/50\n",
      "203621/203621 [==============================] - 112s 551us/step - loss: 0.0158 - acc: 0.9790 - val_loss: 0.0292 - val_acc: 0.9635\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/50\n",
      "203621/203621 [==============================] - 112s 551us/step - loss: 0.0155 - acc: 0.9799 - val_loss: 0.0314 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/50\n",
      "203621/203621 [==============================] - 112s 550us/step - loss: 0.0152 - acc: 0.9801 - val_loss: 0.0302 - val_acc: 0.9624\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/50\n",
      "203621/203621 [==============================] - 112s 552us/step - loss: 0.0149 - acc: 0.9805 - val_loss: 0.0300 - val_acc: 0.9626\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 00021: early stopping\n",
      "'trainModelSP'  2592073.14 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 03:53:35.803079: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 03:53:35.877084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 03:53:35.877409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 03:53:35.877436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "embed (?, 9, 95)\n",
      "embed (?, 9, 95)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_nolearn_dropout_pos\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 95)        0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 95)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       73216       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,321,320\n",
      "Trainable params: 433,920\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 108s 529us/step - loss: 0.0474 - acc: 0.9416 - val_loss: 0.0248 - val_acc: 0.9674\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02483, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0259 - acc: 0.9663 - val_loss: 0.0213 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02483 to 0.02129, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0223 - acc: 0.9705 - val_loss: 0.0208 - val_acc: 0.9731\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02129 to 0.02084, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0201 - acc: 0.9735 - val_loss: 0.0186 - val_acc: 0.9755\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02084 to 0.01861, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 107s 523us/step - loss: 0.0185 - acc: 0.9756 - val_loss: 0.0191 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0174 - acc: 0.9770 - val_loss: 0.0183 - val_acc: 0.9769\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01861 to 0.01835, saving model to ./result/weights-06.h5\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 107s 526us/step - loss: 0.0163 - acc: 0.9784 - val_loss: 0.0187 - val_acc: 0.9764\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0153 - acc: 0.9798 - val_loss: 0.0177 - val_acc: 0.9769\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01835 to 0.01766, saving model to ./result/weights-08.h5\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0146 - acc: 0.9807 - val_loss: 0.0181 - val_acc: 0.9774\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0139 - acc: 0.9816 - val_loss: 0.0178 - val_acc: 0.9764\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0132 - acc: 0.9825 - val_loss: 0.0185 - val_acc: 0.9764\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0127 - acc: 0.9830 - val_loss: 0.0169 - val_acc: 0.9783\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01766 to 0.01694, saving model to ./result/weights-12.h5\n",
      "Epoch 13/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0123 - acc: 0.9836 - val_loss: 0.0189 - val_acc: 0.9756\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0119 - acc: 0.9842 - val_loss: 0.0171 - val_acc: 0.9773\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.0116 - acc: 0.9848 - val_loss: 0.0170 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0113 - acc: 0.9851 - val_loss: 0.0176 - val_acc: 0.9772\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/50\n",
      "203621/203621 [==============================] - 106s 523us/step - loss: 0.0108 - acc: 0.9854 - val_loss: 0.0185 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 00017: early stopping\n",
      "'trainModelSP'  2031063.21 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 04:27:26.918547: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 04:27:26.998573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 04:27:26.998945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 04:27:26.998985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 55)\n",
      "embed (?, 9, 55)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_nolearn_dropout_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 55)        0           embedding_1[0][0]                \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 55)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       42496       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,290,600\n",
      "Trainable params: 403,200\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0453 - acc: 0.9444 - val_loss: 0.0222 - val_acc: 0.9715\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02225, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0239 - acc: 0.9688 - val_loss: 0.0185 - val_acc: 0.9765\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02225 to 0.01854, saving model to ./result/weights-02.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.0206 - acc: 0.9727 - val_loss: 0.0180 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01854 to 0.01802, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0184 - acc: 0.9760 - val_loss: 0.0168 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01802 to 0.01684, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 107s 523us/step - loss: 0.0168 - acc: 0.9778 - val_loss: 0.0162 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01684 to 0.01616, saving model to ./result/weights-05.h5\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0157 - acc: 0.9793 - val_loss: 0.0156 - val_acc: 0.9798\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01616 to 0.01559, saving model to ./result/weights-06.h5\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 106s 523us/step - loss: 0.0147 - acc: 0.9805 - val_loss: 0.0162 - val_acc: 0.9793\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0142 - acc: 0.9813 - val_loss: 0.0154 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01559 to 0.01537, saving model to ./result/weights-08.h5\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 106s 523us/step - loss: 0.0132 - acc: 0.9826 - val_loss: 0.0147 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01537 to 0.01465, saving model to ./result/weights-09.h5\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0126 - acc: 0.9834 - val_loss: 0.0154 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 106s 523us/step - loss: 0.0123 - acc: 0.9838 - val_loss: 0.0151 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0117 - acc: 0.9844 - val_loss: 0.0149 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/50\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.0111 - acc: 0.9854 - val_loss: 0.0151 - val_acc: 0.9802\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/50\n",
      "203621/203621 [==============================] - 106s 523us/step - loss: 0.0109 - acc: 0.9857 - val_loss: 0.0139 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01465 to 0.01394, saving model to ./result/weights-14.h5\n",
      "Epoch 15/50\n",
      "203621/203621 [==============================] - 107s 523us/step - loss: 0.0104 - acc: 0.9864 - val_loss: 0.0146 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0102 - acc: 0.9866 - val_loss: 0.0146 - val_acc: 0.9807\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/50\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0100 - acc: 0.9869 - val_loss: 0.0148 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0095 - acc: 0.9874 - val_loss: 0.0145 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.0094 - acc: 0.9875 - val_loss: 0.0154 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 00019: early stopping\n",
      "'trainModelSP'  2256886.35 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 05:05:03.751829: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 05:05:03.825367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 05:05:03.825709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 05:05:03.825739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_nolearn_dropout_pos_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 100)       0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 100)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       77056       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,325,160\n",
      "Trainable params: 437,760\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0407 - acc: 0.9504 - val_loss: 0.0203 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02030, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 107s 528us/step - loss: 0.0208 - acc: 0.9729 - val_loss: 0.0166 - val_acc: 0.9786\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02030 to 0.01659, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0173 - acc: 0.9776 - val_loss: 0.0165 - val_acc: 0.9789\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01659 to 0.01645, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0155 - acc: 0.9801 - val_loss: 0.0153 - val_acc: 0.9802\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01645 to 0.01533, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 107s 523us/step - loss: 0.0140 - acc: 0.9816 - val_loss: 0.0159 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0126 - acc: 0.9836 - val_loss: 0.0157 - val_acc: 0.9801\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0118 - acc: 0.9848 - val_loss: 0.0139 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01533 to 0.01392, saving model to ./result/weights-07.h5\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 108s 529us/step - loss: 0.0112 - acc: 0.9854 - val_loss: 0.0144 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0105 - acc: 0.9863 - val_loss: 0.0139 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01392 to 0.01389, saving model to ./result/weights-09.h5\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0099 - acc: 0.9870 - val_loss: 0.0140 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.0093 - acc: 0.9880 - val_loss: 0.0141 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "203621/203621 [==============================] - 108s 529us/step - loss: 0.0091 - acc: 0.9881 - val_loss: 0.0137 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01389 to 0.01368, saving model to ./result/weights-12.h5\n",
      "Epoch 13/50\n",
      "203621/203621 [==============================] - 108s 529us/step - loss: 0.0085 - acc: 0.9888 - val_loss: 0.0135 - val_acc: 0.9822\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01368 to 0.01354, saving model to ./result/weights-13.h5\n",
      "Epoch 14/50\n",
      "203621/203621 [==============================] - 107s 528us/step - loss: 0.0082 - acc: 0.9895 - val_loss: 0.0143 - val_acc: 0.9822\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/50\n",
      "203621/203621 [==============================] - 107s 528us/step - loss: 0.0079 - acc: 0.9898 - val_loss: 0.0137 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0079 - acc: 0.9897 - val_loss: 0.0136 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/50\n",
      "203621/203621 [==============================] - 106s 523us/step - loss: 0.0074 - acc: 0.9904 - val_loss: 0.0134 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01354 to 0.01341, saving model to ./result/weights-17.h5\n",
      "Epoch 18/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0071 - acc: 0.9908 - val_loss: 0.0141 - val_acc: 0.9822\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/50\n",
      "203621/203621 [==============================] - 108s 531us/step - loss: 0.0070 - acc: 0.9910 - val_loss: 0.0137 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/50\n",
      "203621/203621 [==============================] - 108s 529us/step - loss: 0.0067 - acc: 0.9913 - val_loss: 0.0133 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01341 to 0.01333, saving model to ./result/weights-20.h5\n",
      "Epoch 21/50\n",
      "203621/203621 [==============================] - 108s 530us/step - loss: 0.0065 - acc: 0.9917 - val_loss: 0.0138 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.0064 - acc: 0.9915 - val_loss: 0.0135 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0063 - acc: 0.9919 - val_loss: 0.0134 - val_acc: 0.9831\n"
     ]
    }
   ],
   "source": [
    "# CONSIDER WRAPPING IN A FUNCTION... PROS AND CONS...\n",
    "\n",
    "# capsnet training function\n",
    "testFunc = \"trainCapsModel.py\"\n",
    "\n",
    "hypers = hyper_param_caps.copy()\n",
    "hypers['epochs'] = 50\n",
    "hypers['stopping_patience'] = 5\n",
    "hypers['use_pos_tags'] = False\n",
    "hypers['use_capitalization_info'] = False\n",
    "\n",
    "# try different embeddings\n",
    "# learn embeddings\n",
    "print(\"\\n\\nLearn Embeddings\")\n",
    "hypers['use_glove'] = False\n",
    "hypers['embed_dropout'] = 0.0\n",
    "testFeatures( testFunc, \"learn\", hypers)\n",
    "\n",
    "# learn embeddings + Dropout\n",
    "print(\"\\n\\nLearn Embeddings and Dropout\")\n",
    "hypers['use_glove'] = False\n",
    "hypers['embed_dropout'] = 0.25\n",
    "testFeatures( testFunc, \"learn_dropout\", hypers)\n",
    "\n",
    "# use glove, no learn\n",
    "print(\"\\n\\nGlove Embeddings\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = False\n",
    "hypers['embed_dropout'] = 0.0\n",
    "testFeatures( testFunc, \"glove_nolearn\", hypers)\n",
    "\n",
    "# use glove, no learn + Dropout\n",
    "print(\"\\n\\nGlove Embeddings and Dropout\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = False\n",
    "hypers['embed_dropout'] = 0.25\n",
    "testFeatures( testFunc, \"glove_nolearn_dropout\", hypers)\n",
    "\n",
    "# use glove, learn\n",
    "print(\"\\n\\nGlove Embeddings with Learning\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = True\n",
    "hypers['embed_dropout'] = 0.0\n",
    "testFeatures( testFunc, \"glove_learn\", hypers)\n",
    "\n",
    "# use glove, learn + Dropout\n",
    "print(\"\\n\\nGlove Embeddings with Learning and Dropout\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = True\n",
    "hypers['embed_dropout'] = 0.25\n",
    "testFeatures( testFunc, \"glove_learn_dropout\", hypers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder if pos tags could be more useful with another representation (besides 1-hot encodings). I expected them to rock. Are they too sparse? do they make the input too large for the network? Check the paper for clues..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it all again with SGD instead of Adam. Maybe Adam is too agressive?\n",
    "# train a caps model with 2D Primary caps and repeat tests\n",
    "\n",
    "# capsnet training function\n",
    "testFunc = \"trainCapsModel.py\"\n",
    "\n",
    "hypers = hyper_param_caps.copy()\n",
    "hypers['optimizer'] = \"SGD\"\n",
    "print(\"Training with SGD - Nesterov Momentum Optimizer\")\n",
    "\n",
    "hypers['epochs'] = 50\n",
    "hypers['stopping_patience'] = 5\n",
    "hypers['use_pos_tags'] = False\n",
    "hypers['use_capitalization_info'] = False\n",
    "\n",
    "# try different embeddings\n",
    "# learn embeddings\n",
    "print(\"\\n\\nLearn Embeddings\")\n",
    "hypers['use_glove'] = False\n",
    "hypers['embed_dropout'] = 0.0\n",
    "testFeatures( testFunc, \"SGD_primcaps_learn\", hypers)\n",
    "\n",
    "# learn embeddings + Dropout\n",
    "print(\"\\n\\nLearn Embeddings and Dropout\")\n",
    "hypers['use_glove'] = False\n",
    "hypers['embed_dropout'] = 0.25\n",
    "testFeatures( testFunc, \"SGD_primcaps_learn_dropout\", hypers)\n",
    "\n",
    "# use glove, no learn\n",
    "print(\"\\n\\nGlove Embeddings\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = False\n",
    "hypers['embed_dropout'] = 0.0\n",
    "testFeatures( testFunc, \"SGD_primcaps_glove_nolearn\", hypers)\n",
    "\n",
    "# use glove, no learn + Dropout\n",
    "print(\"\\n\\nGlove Embeddings and Dropout\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = False\n",
    "hypers['embed_dropout'] = 0.25\n",
    "testFeatures( testFunc, \"SGD_primcaps_glove_nolearn_dropout\", hypers)\n",
    "\n",
    "# use glove, learn\n",
    "print(\"\\n\\nGlove Embeddings with Learning\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = True\n",
    "hypers['embed_dropout'] = 0.0\n",
    "testFeatures( testFunc, \"SGD_primcaps_glove_learn\", hypers)\n",
    "\n",
    "# use glove, learn + Dropout\n",
    "print(\"\\n\\nGlove Embeddings with Learning and Dropout\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = True\n",
    "hypers['embed_dropout'] = 0.25\n",
    "testFeatures( testFunc, \"SGD_primcaps_glove_learn_dropout\", hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = hyper_param_caps.copy()\n",
    "hypers['optimizer'] = \"SGD\"\n",
    "print(\"Training with SGD - Nesterov Momentum Optimizer\")\n",
    "\n",
    "hypers['epochs'] = 60\n",
    "hypers['stopping_patience'] = 10 # more dropout... let it go longer\n",
    "hypers['use_pos_tags'] = False\n",
    "hypers['use_capitalization_info'] = False\n",
    "\n",
    "# More SGD\n",
    "# use glove, no learn + Dropout\n",
    "print(\"\\n\\nGlove Embeddings and Dropout at 50%\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = False\n",
    "hypers['embed_dropout'] = 0.5\n",
    "testFeatures( testFunc, \"SGD_primcaps_glove_nolearn_dropout_05\", hypers)\n",
    "\n",
    "# use glove, learn + Dropout\n",
    "print(\"\\n\\nGlove Embeddings with Learning and Dropout at 50%\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = True\n",
    "hypers['embed_dropout'] = 0.5\n",
    "testFeatures( testFunc, \"SGD_primcaps_glove_learn_dropout_05\", hypers)\n",
    "\n",
    "# learn embeddings + Dropout\n",
    "print(\"\\n\\nLearn Embeddings and Dropout at 50%\")\n",
    "hypers['use_glove'] = False\n",
    "hypers['embed_dropout'] = 0.5\n",
    "testFeatures( testFunc, \"SGD_primcaps_learn_dropout_05\", hypers)\n",
    "\n",
    "# Back to Adam\n",
    "hypers['optimizer'] = \"Adam\"\n",
    "print(\"Training with Adam\")\n",
    "\n",
    "# learn embeddings + Dropout\n",
    "print(\"\\n\\nLearn Embeddings and Dropout at 50%\")\n",
    "hypers['use_glove'] = False\n",
    "hypers['embed_dropout'] = 0.5\n",
    "testFeatures( testFunc, \"learn_dropout_05\", hypers)\n",
    "\n",
    "# use glove, no learn + Dropout\n",
    "print(\"\\n\\nGlove Embeddings and Dropout at 50%\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = False\n",
    "hypers['embed_dropout'] = 0.5\n",
    "testFeatures( testFunc, \"glove_nolearn_dropout_05\", hypers)\n",
    "\n",
    "# use glove, learn + Dropout\n",
    "print(\"\\n\\nGlove Embeddings with Learning and Dropout at 50%\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = True\n",
    "hypers['embed_dropout'] = 0.5\n",
    "testFeatures( testFunc, \"glove_learn_dropout_05\", hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testModel( draw_capsnet_model, hyper_param_caps)\n",
    "#testModel( capsmodel, hyper_param_caps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Primary Caps Layer\n",
    ">**NOT YET ATTEMPTED!**  \n",
    "* try one first, see if it even trains...  \n",
    "* may need a new set of hypers to get it working/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256, 1)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n"
     ]
    }
   ],
   "source": [
    "# train a caps model with 2D Primary caps and repeat tests\n",
    "\n",
    "# capsnet training function\n",
    "testFunc = \"trainCapsModel.py\"\n",
    "\n",
    "hypers = hyper_param_caps.copy()\n",
    "hypers['use_2D_primarycaps'] = True\n",
    "\n",
    "hypers['epochs'] = 1\n",
    "hypers['stopping_patience'] = 1\n",
    "hypers['use_pos_tags'] = False\n",
    "hypers['use_capitalization_info'] = False\n",
    "\n",
    "# try different embeddings\n",
    "# learn embeddings\n",
    "print(\"\\n\\nLearn Embeddings\")\n",
    "hypers['use_glove'] = False\n",
    "hypers['embed_dropout'] = 0.0\n",
    "testFeatures( testFunc, \"2D_primcaps_learn\", hypers)\n",
    "\n",
    "# learn embeddings + Dropout\n",
    "print(\"\\n\\nLearn Embeddings and Dropout\")\n",
    "hypers['use_glove'] = False\n",
    "hypers['embed_dropout'] = 0.25\n",
    "testFeatures( testFunc, \"2D_primcaps_learn_dropout\", hypers)\n",
    "\n",
    "# use glove, no learn\n",
    "print(\"\\n\\nGlove Embeddings\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = False\n",
    "hypers['embed_dropout'] = 0.0\n",
    "testFeatures( testFunc, \"2D_primcaps_glove_nolearn\", hypers)\n",
    "\n",
    "# use glove, no learn + Dropout\n",
    "print(\"\\n\\nGlove Embeddings and Dropout\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = False\n",
    "hypers['embed_dropout'] = 0.25\n",
    "testFeatures( testFunc, \"2D_primcaps_glove_nolearn_dropout\", hypers)\n",
    "\n",
    "# use glove, learn\n",
    "print(\"\\n\\nGlove Embeddings with Learning\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = True\n",
    "hypers['embed_dropout'] = 0.0\n",
    "testFeatures( testFunc, \"2D_primcaps_glove_learn\", hypers)\n",
    "\n",
    "# use glove, learn + Dropout\n",
    "print(\"\\n\\nGlove Embeddings with Learning and Dropout\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = True\n",
    "hypers['embed_dropout'] = 0.25\n",
    "testFeatures( testFunc, \"2D_primcaps_glove_learn_dropout\", hypers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline thoughts... we want the cnn to be the BEST. we want to compare our results to state of the art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
