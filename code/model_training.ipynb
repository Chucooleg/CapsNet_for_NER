{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "> To facilitate a more automated training procedure, the model training is moved to a standalone file.  \n",
    "This keeps Keras much happier in terms of required restarts and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import time # !\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import glove_helper\n",
    "from loadutils import conll2003Data, saveProcessedData\n",
    "from common import vocabulary, utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(\"Tensorflow version:\", tf.__version__)\n",
    "#print(\"Keras version:\", K.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"../data/conll2003/eng.train\"\n",
    "DEV_FILE = \"../data/conll2003/eng.testa\"\n",
    "TEST_FILE = \"../data/conll2003/eng.testb\"\n",
    "\n",
    "# out files for IPC\n",
    "HYPER_PARAM_FILE = \"hyper_params.json\"\n",
    "\n",
    "VOCAB_SIZE = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local helper utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local untils\n",
    "\n",
    "# timeit decorator\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print ('%r  %2.2f ms' % \\\n",
    "                  (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_embedding_matrix(embed_dim, vocab_size):\n",
    "    \"\"\"\n",
    "    construct embedding matrix from GloVe 6Bn word data\n",
    "    \n",
    "    reuse glove_helper code from w266 \n",
    "    \n",
    "    Returns: an embedding matrix directly plugged into keras.layers.Embedding(weights=[embedding_matrix])\n",
    "    \"\"\"\n",
    "    reload(glove_helper)\n",
    "    hands = glove_helper.Hands(ndim=embed_dim)\n",
    "    embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "    \n",
    "    for i in range(vocabData.vocab.size):\n",
    "        word = vocabData.vocab.ids_to_words([i])[0]\n",
    "        try:\n",
    "            embedding_vector = hands.get_vector(word)\n",
    "        except:\n",
    "            embedding_vector = hands.get_vector(\"<unk>\")\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history( history):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "reading file from path ../data/conll2003/eng.train\n",
      "'readFile'  1157.66 ms\n",
      "----------------------------------------------------\n",
      "building vocabulary from TRAINING data...\n",
      "'buildVocab'  1045.19 ms\n",
      "----------------------------------------------------\n",
      "formatting sentences into input windows...\n",
      "'formatWindowedData'  1837.57 ms\n",
      "----------------------------------------------------\n",
      "reading file from path ../data/conll2003/eng.testa\n",
      "'readFile'  252.86 ms\n",
      "----------------------------------------------------\n",
      "formatting sentences into input windows...\n",
      "'formatWindowedData'  412.34 ms\n",
      "----------------------------------------------------\n",
      "reading file from path ../data/conll2003/eng.testb\n",
      "'readFile'  226.20 ms\n",
      "----------------------------------------------------\n",
      "formatting sentences into input windows...\n",
      "'formatWindowedData'  513.65 ms\n"
     ]
    }
   ],
   "source": [
    "# UPDATES!\n",
    "\n",
    "windowLength = 9\n",
    "#testNumSents = 20000\n",
    "\n",
    "# Use training set to build vocab here\n",
    "vocabData = conll2003Data(TRAIN_FILE)\n",
    "vocabData.buildVocab( vocabSize=VOCAB_SIZE)\n",
    "\n",
    "# Format training data\n",
    "trainX, trainX_pos, trainX_capitals, trainY  = vocabData.formatWindowedData( \n",
    "                                                  vocabData.train_sentences, \n",
    "                                                  windowLength=windowLength,\n",
    "                                                  verbose=False)\n",
    "\n",
    "# read in dev data\n",
    "devSents = vocabData.readFile( DEV_FILE)\n",
    "devX, devX_pos, devX_capitals, devY = vocabData.formatWindowedData( \n",
    "                                              devSents, \n",
    "                                              windowLength=windowLength,\n",
    "                                              verbose=False)\n",
    "\n",
    "# read in the test data\n",
    "testSents = vocabData.readFile( TEST_FILE)\n",
    "testX, testX_pos, testX_capitals, testY = vocabData.formatWindowedData( \n",
    "                                                testSents, \n",
    "                                                windowLength=windowLength,\n",
    "                                                verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Y\n",
    "\n",
    "# encoding 1-hot for ner targets\n",
    "trainY_cat = to_categorical(trainY.astype('float32'))\n",
    "devY_cat = to_categorical(devY.astype('float32'), num_classes=trainY_cat.shape[1])\n",
    "testY_cat = to_categorical(testY.astype('float32'), num_classes=trainY_cat.shape[1])\n",
    "\n",
    "trainY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), trainY_cat)), dtype=np.float)\n",
    "devY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), devY_cat)), dtype=np.float)\n",
    "testY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), testY_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X pos tags\n",
    "\n",
    "# encoding 1-hot for pos tags\n",
    "trainX_pos_cat = to_categorical(trainX_pos.astype('float32'))\n",
    "devX_pos_cat = to_categorical(devX_pos.astype('float32'), num_classes=trainX_pos_cat.shape[2]) \n",
    "testX_pos_cat = to_categorical(testX_pos.astype('float32'), num_classes=trainX_pos_cat.shape[2])\n",
    "\n",
    "trainX_pos_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), trainX_pos_cat)), dtype=np.float)\n",
    "devX_pos_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), devX_pos_cat)), dtype=np.float)\n",
    "testX_pos_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), testX_pos_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X capitlization \n",
    "\n",
    "# encoding 1-hot for capitalization info  (\"allCaps\", \"upperInitial\", \"lowercase\", \"mixedCaps\", \"noinfo\")\n",
    "trainX_capitals_cat = to_categorical(trainX_capitals.astype('float32'))\n",
    "devX_capitals_cat = to_categorical(devX_capitals.astype('float32'), num_classes=trainX_capitals_cat.shape[2]) \n",
    "testX_capitals_cat = to_categorical(testX_capitals.astype('float32'), num_classes=trainX_capitals_cat.shape[2])\n",
    "\n",
    "trainX_capitals_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), trainX_capitals_cat)), dtype=np.float)\n",
    "devX_capitals_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), devX_capitals_cat)), dtype=np.float)\n",
    "testX_capitals_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), testX_capitals_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper parameters for model\n",
    "# CAPSNET\n",
    "hyper_param_caps = {\n",
    "    \n",
    "    'max_features' : vocabData.vocab.size,  # 20000\n",
    "    'maxlen' : trainX.shape[1],  # window size (9)\n",
    "    'poslen' : trainX_pos_cat.shape[2],  # pos classes (45)\n",
    "    'capitallen' : trainX_capitals_cat.shape[2],  # capitalization classes (5)\n",
    "    'ner_classes' : trainY_cat.shape[1],  # 8 \n",
    "    'embed_dim' : 50,  # word embedding size\n",
    "    'num_routing' : 3, \n",
    "\n",
    "    'use_glove' : True,\n",
    "    'allow_glove_retrain' : False,\n",
    "    'use_pos_tags' : True,\n",
    "    'use_capitalization_info' : True,    \n",
    "    \n",
    "    'conv1_filters' : 256,\n",
    "    'conv1_kernel_size' : 3,\n",
    "    'conv1_strides' : 1,\n",
    "    'conv1_padding' : 'valid',\n",
    "    \n",
    "    'use_2D_primarycaps' : False,\n",
    "    'primarycaps_dim_capsule' : 8,\n",
    "    'primarycaps_n_channels' : 32,\n",
    "    'primarycaps_kernel_size' : 3,\n",
    "    'primarycaps_strides' : 1,\n",
    "    'primarycaps_padding' : 'valid',\n",
    "\n",
    "    'ner_capsule_dim' : 16,\n",
    "    \n",
    "    'num_dynamic_routing_passes' : 3,\n",
    "    \n",
    "    # decoder is still work in progress\n",
    "    'use_decoder' : False,\n",
    "    'decoder_feed_forward_1' : 100,\n",
    "    'decoder_feed_forward_2' : 100, \n",
    "    \n",
    "    'save_dir' : './result',\n",
    "    'batch_size' : 100,\n",
    "    'debug' : 2,\n",
    "    'epochs' : 5,\n",
    "    'stopping_patience' : 5, # default to same as epochs, ie don't use\n",
    "    'dropout_p' : 0.25,\n",
    "    'embed_dropout' : 0.25,\n",
    "    'lam_recon' : 0.0005,\n",
    "    \n",
    "    'optimizer' : 'Adam', #or 'SGD'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper parameters for model\n",
    "# CNN\n",
    "hyper_param_cnn = {\n",
    "    \n",
    "    'max_features' : vocabData.vocab.size,  # 20000\n",
    "    'maxlen' : trainX.shape[1],  # window size (9)\n",
    "    'poslen' : trainX_pos_cat.shape[2],  # pos classes (45)\n",
    "    'capitallen' : trainX_capitals_cat.shape[2],  # capitalization classes (5)\n",
    "    'ner_classes' : trainY_cat.shape[1],  # 8 \n",
    "    'embed_dim' : 50,  # word embedding size\n",
    "    'num_routing' : 3, \n",
    "\n",
    "    'use_glove' : True,\n",
    "    'allow_glove_retrain' : False,\n",
    "    'use_pos_tags' : True,\n",
    "    'use_capitalization_info' : True,    \n",
    "    \n",
    "    'conv1_filters' : 256,\n",
    "    'conv1_kernel_size' : 3,\n",
    "    'conv1_strides' : 1,\n",
    "    'conv1_padding' : 'valid',\n",
    "    \n",
    "    'conv2_filters' : 256,\n",
    "    'conv2_kernel_size' : 3,\n",
    "    'conv2_strides' : 1,\n",
    "    'conv2_padding' : 'valid',\n",
    "    \n",
    "    'conv3_filters' : 128,\n",
    "    'conv3_kernel_size' : 3,\n",
    "    'conv3_strides' : 1,\n",
    "    'conv3_padding' : 'valid',\n",
    "    \n",
    "    'max_pooling_size' : 3,\n",
    "    'max_pooling_strides' : 1,\n",
    "    'max_pooling_padding' : 'valid',\n",
    "    'maxpool_dropout' : 0.3,\n",
    "    \n",
    "    'feed_forward_1' : 328,\n",
    "    'ff1_dropout' : 0.3,\n",
    "    'feed_forward_2' : 192,\n",
    "    'ff2_dropout' : 0.3,\n",
    "    \n",
    "    'save_dir' : './result',\n",
    "    'batch_size' : 100,\n",
    "    'debug' : 2,\n",
    "    'epochs' : 5,\n",
    "    'stopping_patience' : 5, # default to same as epochs, ie don't use\n",
    "    'dropout_p' : 0.25,\n",
    "    'embed_dropout' : 0.25,  # set to 0 to disable dropout\n",
    "    'lam_recon' : 0.0005,\n",
    "    \n",
    "    'optimizer' : 'Adam', #or 'SGD'\n",
    "    #'loss_function' : margin_loss, # constructed loss function see margin_loss() in this notebook\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Glove Embeddings Matrix and Save All Data to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.50d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 50))\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embedding matrix\n",
    "# embedding_matrix = construct_embedding_matrix(hyper_param_caps['embed_dim'])\n",
    "embedding_matrix = construct_embedding_matrix( hyper_param_caps['embed_dim'], \n",
    "                                               hyper_param_caps['max_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all loaded data for use by training process\n",
    "saveProcessedData( trainX, trainX_capitals_cat, trainX_pos_cat, devX, devX_capitals_cat,\n",
    "                   devX_pos_cat, trainY_cat, devY_cat, embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit \n",
    "def trainModelSP( testFunc, modelName, hyper_params, embed_matrix=None, verbose=False):\n",
    "    \"\"\"\n",
    "    testFunc - the name of the python file to run\n",
    "    modelName - the internal name (ID) of the model to train\n",
    "    hyper_params - a dict of hyper parameters\n",
    "    \"\"\"\n",
    "    # save the hyperparams\n",
    "    with open(HYPER_PARAM_FILE, mode='w') as fp:\n",
    "        json.dump( hyper_params, fp)\n",
    "    \n",
    "    # call the train function\n",
    "    # consider replacing with a call to subprocess!!\n",
    "    !python {testFunc} {modelName} {HYPER_PARAM_FILE}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit \n",
    "def testFeatures( testFunc, modelName, hyper_params):\n",
    "    \"\"\"\n",
    "    builds and trains 4 models for the configuration in hyper_params,\n",
    "    1 for each input feature configuration: base, pos, caps, pos + caps\n",
    "    \n",
    "    testFunc - the name of the python file to run\n",
    "    modelName - the model name to use for labeling\n",
    "    \"\"\"\n",
    "    hypers = hyper_params.copy()\n",
    "    \n",
    "    # try the embeddings with different features\n",
    "    \n",
    "    # base\n",
    "    curModel = modelName + \"_base\"\n",
    "    trainModelSP( testFunc, curModel, hypers )\n",
    "    \n",
    "    # pos tags\n",
    "    curModel = modelName + \"_pos\"\n",
    "    hypers['use_pos_tags'] = True\n",
    "    hypers['use_capitalization_info'] = False\n",
    "    trainModelSP( testFunc, curModel, hypers )\n",
    "    \n",
    "    # capitalization info\n",
    "    curModel = modelName + \"_caps\"\n",
    "    hypers['use_pos_tags'] = False\n",
    "    hypers['use_capitalization_info'] = True\n",
    "    trainModelSP( testFunc, curModel, hypers )\n",
    "    \n",
    "    # both\n",
    "    curModel = modelName + \"_pos_caps\"\n",
    "    hypers['use_pos_tags'] = True\n",
    "    hypers['use_capitalization_info'] = True\n",
    "    trainModelSP( testFunc, curModel, hypers )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still need:\n",
    "> a function to read in each historylog.csv file, optionally plot, then collect the best scoring epoch (lowest loss? - discuss, add F1?) to determine the best model and how long it trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Primary Caps Layer Training\n",
    "> I know the output isn't pretty, but we don't really need it since everything is stored in the history log... It is really just to show a sign of life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test changing vocab size?...will have to rerun all tests for each size. same for word embedding size. is ok.\n",
    "# change embedding sizes!!!\n",
    "# primary caps conv1D kernel size - play with it!!!\n",
    "# - changing dropout rate will be perhaps in hyperparam tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Learn Embeddings\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "embed (?, 9, 50)\n",
      "embed (?, 9, 50)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_base\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 5, 256)            196864    \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             163840    \n",
      "_________________________________________________________________\n",
      "out_pred (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,286,760\n",
      "Trainable params: 1,286,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-11 23:45:25.683312: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-11 23:45:25.774409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-11 23:45:25.774750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-11 23:45:25.774783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.1052 - acc: 0.9039 - val_loss: 0.0323 - val_acc: 0.9584\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03226, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.0120 - acc: 0.9851 - val_loss: 0.0298 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03226 to 0.02975, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.0062 - acc: 0.9924 - val_loss: 0.0306 - val_acc: 0.9627\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.0042 - acc: 0.9949 - val_loss: 0.0305 - val_acc: 0.9643\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.0030 - acc: 0.9964 - val_loss: 0.0334 - val_acc: 0.9598\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 103s 507us/step - loss: 0.0026 - acc: 0.9969 - val_loss: 0.0321 - val_acc: 0.9633\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.0019 - acc: 0.9976 - val_loss: 0.0316 - val_acc: 0.9634\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 00007: early stopping\n",
      "'trainModelSP'  829949.42 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "embed (?, 9, 95)\n",
      "embed (?, 9, 95)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_pos\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 95)        0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       73216       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,321,320\n",
      "Trainable params: 1,321,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-11 23:59:15.779225: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-11 23:59:15.854737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-11 23:59:15.855065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-11 23:59:15.855096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 116s 569us/step - loss: 0.0452 - acc: 0.9499 - val_loss: 0.0200 - val_acc: 0.9745\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01995, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 116s 572us/step - loss: 0.0071 - acc: 0.9914 - val_loss: 0.0208 - val_acc: 0.9741\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 116s 572us/step - loss: 0.0037 - acc: 0.9957 - val_loss: 0.0201 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 116s 568us/step - loss: 0.0024 - acc: 0.9971 - val_loss: 0.0210 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 116s 570us/step - loss: 0.0017 - acc: 0.9979 - val_loss: 0.0210 - val_acc: 0.9768\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 117s 576us/step - loss: 0.0015 - acc: 0.9983 - val_loss: 0.0222 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 00006: early stopping\n",
      "'trainModelSP'  775470.15 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 55)\n",
      "embed (?, 9, 55)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 55)        0           embedding_1[0][0]                \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       42496       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,290,600\n",
      "Trainable params: 1,290,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 00:12:11.088450: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 00:12:11.160890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 00:12:11.161205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 00:12:11.161234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0409 - acc: 0.9533 - val_loss: 0.0175 - val_acc: 0.9782\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01746, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 107s 526us/step - loss: 0.0060 - acc: 0.9926 - val_loss: 0.0171 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01746 to 0.01708, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0030 - acc: 0.9965 - val_loss: 0.0178 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0021 - acc: 0.9977 - val_loss: 0.0177 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 108s 529us/step - loss: 0.0016 - acc: 0.9981 - val_loss: 0.0203 - val_acc: 0.9770\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 108s 528us/step - loss: 0.0013 - acc: 0.9985 - val_loss: 0.0197 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 107s 528us/step - loss: 0.0012 - acc: 0.9986 - val_loss: 0.0191 - val_acc: 0.9791\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 00007: early stopping\n",
      "'trainModelSP'  848465.60 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_pos_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 100)       0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       77056       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,325,160\n",
      "Trainable params: 1,325,160\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 00:26:19.632773: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 00:26:19.705135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 00:26:19.705541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 00:26:19.705585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0376 - acc: 0.9569 - val_loss: 0.0169 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01691, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 108s 529us/step - loss: 0.0056 - acc: 0.9933 - val_loss: 0.0157 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01691 to 0.01567, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0028 - acc: 0.9967 - val_loss: 0.0173 - val_acc: 0.9797\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 108s 530us/step - loss: 0.0019 - acc: 0.9978 - val_loss: 0.0207 - val_acc: 0.9764\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0015 - acc: 0.9983 - val_loss: 0.0174 - val_acc: 0.9797\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0012 - acc: 0.9986 - val_loss: 0.0176 - val_acc: 0.9809\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 8.7063e-04 - acc: 0.9990 - val_loss: 0.0188 - val_acc: 0.9789\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 00007: early stopping\n",
      "'trainModelSP'  850209.21 ms\n",
      "'testFeatures'  3304094.77 ms\n",
      "\n",
      "\n",
      "Learn Embeddings and Dropout\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "embed (?, 9, 50)\n",
      "embed (?, 9, 50)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_dropout_base\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 9, 50)             0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 5, 256)            196864    \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             163840    \n",
      "_________________________________________________________________\n",
      "out_pred (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,286,760\n",
      "Trainable params: 1,286,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 00:40:29.826558: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 00:40:29.899934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 00:40:29.900251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 00:40:29.900280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.1005 - acc: 0.9035 - val_loss: 0.0319 - val_acc: 0.9585\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03194, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0150 - acc: 0.9810 - val_loss: 0.0322 - val_acc: 0.9564\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.0084 - acc: 0.9896 - val_loss: 0.0303 - val_acc: 0.9601\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03194 to 0.03033, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0060 - acc: 0.9927 - val_loss: 0.0279 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03033 to 0.02788, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0047 - acc: 0.9943 - val_loss: 0.0289 - val_acc: 0.9656\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0039 - acc: 0.9953 - val_loss: 0.0302 - val_acc: 0.9630\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0033 - acc: 0.9959 - val_loss: 0.0316 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.0031 - acc: 0.9962 - val_loss: 0.0304 - val_acc: 0.9642\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0026 - acc: 0.9968 - val_loss: 0.0317 - val_acc: 0.9628\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 00009: early stopping\n",
      "'trainModelSP'  1079163.60 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "embed (?, 9, 95)\n",
      "embed (?, 9, 95)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_dropout_pos\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 95)        0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 95)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       73216       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,321,320\n",
      "Trainable params: 1,321,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 00:58:28.996657: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 00:58:29.068441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 00:58:29.068843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 00:58:29.068873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.0536 - acc: 0.9376 - val_loss: 0.0227 - val_acc: 0.9713\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02274, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0118 - acc: 0.9849 - val_loss: 0.0239 - val_acc: 0.9696\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0072 - acc: 0.9909 - val_loss: 0.0224 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02274 to 0.02235, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 108s 532us/step - loss: 0.0053 - acc: 0.9934 - val_loss: 0.0207 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02235 to 0.02068, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0042 - acc: 0.9948 - val_loss: 0.0208 - val_acc: 0.9756\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 107s 523us/step - loss: 0.0034 - acc: 0.9957 - val_loss: 0.0220 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.0030 - acc: 0.9963 - val_loss: 0.0221 - val_acc: 0.9741\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0026 - acc: 0.9967 - val_loss: 0.0220 - val_acc: 0.9752\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0021 - acc: 0.9973 - val_loss: 0.0217 - val_acc: 0.9747\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 00009: early stopping\n",
      "'trainModelSP'  1082428.43 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 55)\n",
      "embed (?, 9, 55)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_dropout_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 55)        0           embedding_1[0][0]                \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 55)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       42496       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,290,600\n",
      "Trainable params: 1,290,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 01:16:31.400023: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 01:16:31.473649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 01:16:31.473978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 01:16:31.474035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 117s 577us/step - loss: 0.0516 - acc: 0.9384 - val_loss: 0.0186 - val_acc: 0.9766\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01857, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 117s 574us/step - loss: 0.0105 - acc: 0.9868 - val_loss: 0.0174 - val_acc: 0.9785\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01857 to 0.01738, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 118s 581us/step - loss: 0.0062 - acc: 0.9923 - val_loss: 0.0187 - val_acc: 0.9771\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 119s 583us/step - loss: 0.0048 - acc: 0.9940 - val_loss: 0.0188 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621/203621 [==============================] - 118s 580us/step - loss: 0.0040 - acc: 0.9950 - val_loss: 0.0178 - val_acc: 0.9791\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 119s 583us/step - loss: 0.0032 - acc: 0.9961 - val_loss: 0.0184 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 118s 582us/step - loss: 0.0027 - acc: 0.9966 - val_loss: 0.0181 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 00007: early stopping\n",
      "'trainModelSP'  927465.92 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_dropout_pos_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 100)       0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 100)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       77056       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,325,160\n",
      "Trainable params: 1,325,160\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 01:31:58.913577: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 01:31:58.988748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 01:31:58.989100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 01:31:58.989157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 115s 567us/step - loss: 0.0457 - acc: 0.9456 - val_loss: 0.0219 - val_acc: 0.9710\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02191, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 116s 568us/step - loss: 0.0094 - acc: 0.9883 - val_loss: 0.0220 - val_acc: 0.9728\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 114s 560us/step - loss: 0.0057 - acc: 0.9931 - val_loss: 0.0224 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 114s 560us/step - loss: 0.0042 - acc: 0.9949 - val_loss: 0.0234 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 114s 561us/step - loss: 0.0033 - acc: 0.9959 - val_loss: 0.0228 - val_acc: 0.9729\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 115s 563us/step - loss: 0.0030 - acc: 0.9963 - val_loss: 0.0220 - val_acc: 0.9731\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 00006: early stopping\n",
      "'trainModelSP'  765590.40 ms\n",
      "'testFeatures'  3854648.80 ms\n",
      "\n",
      "\n",
      "Glove Embeddings\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 01:44:43.256055: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 01:44:43.338023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 01:44:43.338445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 01:44:43.338480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "embed (?, 9, 50)\n",
      "embed (?, 9, 50)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_nolearn_base\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 5, 256)            196864    \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             163840    \n",
      "_________________________________________________________________\n",
      "out_pred (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,286,760\n",
      "Trainable params: 399,360\n",
      "Non-trainable params: 887,400\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 114s 558us/step - loss: 0.0467 - acc: 0.9431 - val_loss: 0.0390 - val_acc: 0.9516\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03897, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 114s 562us/step - loss: 0.0237 - acc: 0.9696 - val_loss: 0.0342 - val_acc: 0.9558\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03897 to 0.03421, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 115s 563us/step - loss: 0.0185 - acc: 0.9765 - val_loss: 0.0352 - val_acc: 0.9557\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 114s 558us/step - loss: 0.0152 - acc: 0.9808 - val_loss: 0.0335 - val_acc: 0.9572\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03421 to 0.03348, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 113s 557us/step - loss: 0.0130 - acc: 0.9836 - val_loss: 0.0342 - val_acc: 0.9571\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 113s 555us/step - loss: 0.0114 - acc: 0.9855 - val_loss: 0.0328 - val_acc: 0.9595\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03348 to 0.03279, saving model to ./result/weights-06.h5\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 114s 561us/step - loss: 0.0099 - acc: 0.9872 - val_loss: 0.0338 - val_acc: 0.9581\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 114s 559us/step - loss: 0.0088 - acc: 0.9886 - val_loss: 0.0350 - val_acc: 0.9582\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 113s 557us/step - loss: 0.0079 - acc: 0.9898 - val_loss: 0.0338 - val_acc: 0.9593\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 114s 561us/step - loss: 0.0072 - acc: 0.9907 - val_loss: 0.0330 - val_acc: 0.9604\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 114s 562us/step - loss: 0.0065 - acc: 0.9916 - val_loss: 0.0334 - val_acc: 0.9608\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 00011: early stopping\n",
      "'trainModelSP'  1395999.14 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 02:07:59.129715: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 02:07:59.205000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 02:07:59.205336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 02:07:59.205383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "embed (?, 9, 95)\n",
      "embed (?, 9, 95)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_nolearn_pos\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 95)        0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       73216       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,321,320\n",
      "Trainable params: 433,920\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 107s 523us/step - loss: 0.0354 - acc: 0.9582 - val_loss: 0.0224 - val_acc: 0.9709\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02241, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0147 - acc: 0.9817 - val_loss: 0.0206 - val_acc: 0.9739\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02241 to 0.02061, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.0109 - acc: 0.9866 - val_loss: 0.0203 - val_acc: 0.9745\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02061 to 0.02028, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.0085 - acc: 0.9898 - val_loss: 0.0211 - val_acc: 0.9741\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.0070 - acc: 0.9914 - val_loss: 0.0205 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.0059 - acc: 0.9928 - val_loss: 0.0204 - val_acc: 0.9752\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0050 - acc: 0.9938 - val_loss: 0.0201 - val_acc: 0.9761\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02028 to 0.02009, saving model to ./result/weights-07.h5\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.0043 - acc: 0.9947 - val_loss: 0.0209 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0040 - acc: 0.9949 - val_loss: 0.0213 - val_acc: 0.9740\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.0033 - acc: 0.9958 - val_loss: 0.0214 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.0031 - acc: 0.9958 - val_loss: 0.0203 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0027 - acc: 0.9965 - val_loss: 0.0222 - val_acc: 0.9737\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 00012: early stopping\n",
      "'trainModelSP'  1407255.18 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 02:31:26.386825: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 02:31:26.463062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 02:31:26.463410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 02:31:26.463441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 55)\n",
      "embed (?, 9, 55)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_nolearn_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 55)        0           embedding_1[0][0]                \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       42496       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,290,600\n",
      "Trainable params: 403,200\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.0336 - acc: 0.9610 - val_loss: 0.0196 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01957, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.0128 - acc: 0.9845 - val_loss: 0.0167 - val_acc: 0.9786\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01957 to 0.01670, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0093 - acc: 0.9887 - val_loss: 0.0172 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.0073 - acc: 0.9915 - val_loss: 0.0159 - val_acc: 0.9805\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01670 to 0.01588, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.0060 - acc: 0.9927 - val_loss: 0.0163 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0050 - acc: 0.9940 - val_loss: 0.0158 - val_acc: 0.9806\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01588 to 0.01584, saving model to ./result/weights-06.h5\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0043 - acc: 0.9948 - val_loss: 0.0169 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0036 - acc: 0.9955 - val_loss: 0.0166 - val_acc: 0.9806\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0033 - acc: 0.9959 - val_loss: 0.0176 - val_acc: 0.9786\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.0030 - acc: 0.9962 - val_loss: 0.0165 - val_acc: 0.9803\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 105s 513us/step - loss: 0.0027 - acc: 0.9965 - val_loss: 0.0163 - val_acc: 0.9806\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 00011: early stopping\n",
      "'trainModelSP'  1303142.99 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 02:53:09.436715: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 02:53:09.509305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 02:53:09.509617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 02:53:09.509644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model: glove_nolearn_pos_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 100)       0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       77056       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,325,160\n",
      "Trainable params: 437,760\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 118s 577us/step - loss: 0.0311 - acc: 0.9634 - val_loss: 0.0194 - val_acc: 0.9769\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01939, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 117s 574us/step - loss: 0.0116 - acc: 0.9856 - val_loss: 0.0178 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01939 to 0.01779, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 116s 572us/step - loss: 0.0082 - acc: 0.9901 - val_loss: 0.0152 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01779 to 0.01523, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 117s 573us/step - loss: 0.0063 - acc: 0.9925 - val_loss: 0.0154 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 117s 576us/step - loss: 0.0051 - acc: 0.9938 - val_loss: 0.0167 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 116s 571us/step - loss: 0.0041 - acc: 0.9949 - val_loss: 0.0164 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 117s 576us/step - loss: 0.0035 - acc: 0.9958 - val_loss: 0.0161 - val_acc: 0.9806\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 117s 576us/step - loss: 0.0030 - acc: 0.9962 - val_loss: 0.0162 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 00008: early stopping\n",
      "'trainModelSP'  1034228.23 ms\n",
      "'testFeatures'  5140625.96 ms\n",
      "\n",
      "\n",
      "Glove Embeddings and Dropout\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 03:10:23.724395: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 03:10:23.799678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 03:10:23.800030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 03:10:23.800060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "embed (?, 9, 50)\n",
      "embed (?, 9, 50)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_nolearn_dropout_base\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 9, 50)             0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 5, 256)            196864    \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             163840    \n",
      "_________________________________________________________________\n",
      "out_pred (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,286,760\n",
      "Trainable params: 399,360\n",
      "Non-trainable params: 887,400\n",
      "_________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 112s 548us/step - loss: 0.0590 - acc: 0.9261 - val_loss: 0.0399 - val_acc: 0.9484\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03987, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 111s 544us/step - loss: 0.0359 - acc: 0.9530 - val_loss: 0.0385 - val_acc: 0.9506\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03987 to 0.03854, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621/203621 [==============================] - 111s 544us/step - loss: 0.0312 - acc: 0.9591 - val_loss: 0.0372 - val_acc: 0.9538\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03854 to 0.03720, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 110s 541us/step - loss: 0.0285 - acc: 0.9621 - val_loss: 0.0332 - val_acc: 0.9567\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03720 to 0.03318, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 110s 541us/step - loss: 0.0269 - acc: 0.9640 - val_loss: 0.0321 - val_acc: 0.9584\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03318 to 0.03211, saving model to ./result/weights-05.h5\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 111s 546us/step - loss: 0.0249 - acc: 0.9674 - val_loss: 0.0320 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03211 to 0.03204, saving model to ./result/weights-06.h5\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 110s 541us/step - loss: 0.0236 - acc: 0.9688 - val_loss: 0.0320 - val_acc: 0.9592\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03204 to 0.03197, saving model to ./result/weights-07.h5\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 111s 545us/step - loss: 0.0227 - acc: 0.9698 - val_loss: 0.0324 - val_acc: 0.9591\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 110s 538us/step - loss: 0.0215 - acc: 0.9711 - val_loss: 0.0332 - val_acc: 0.9594\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 111s 544us/step - loss: 0.0207 - acc: 0.9722 - val_loss: 0.0299 - val_acc: 0.9617\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03197 to 0.02995, saving model to ./result/weights-10.h5\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 112s 551us/step - loss: 0.0199 - acc: 0.9733 - val_loss: 0.0304 - val_acc: 0.9609\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "203621/203621 [==============================] - 112s 548us/step - loss: 0.0191 - acc: 0.9750 - val_loss: 0.0310 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/50\n",
      "203621/203621 [==============================] - 110s 540us/step - loss: 0.0187 - acc: 0.9753 - val_loss: 0.0310 - val_acc: 0.9612\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/50\n",
      "203621/203621 [==============================] - 111s 544us/step - loss: 0.0179 - acc: 0.9763 - val_loss: 0.0299 - val_acc: 0.9624\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02995 to 0.02989, saving model to ./result/weights-14.h5\n",
      "Epoch 15/50\n",
      "203621/203621 [==============================] - 113s 553us/step - loss: 0.0173 - acc: 0.9767 - val_loss: 0.0298 - val_acc: 0.9631\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02989 to 0.02976, saving model to ./result/weights-15.h5\n",
      "Epoch 16/50\n",
      "203621/203621 [==============================] - 112s 549us/step - loss: 0.0169 - acc: 0.9776 - val_loss: 0.0290 - val_acc: 0.9636\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02976 to 0.02903, saving model to ./result/weights-16.h5\n",
      "Epoch 17/50\n",
      "203621/203621 [==============================] - 110s 541us/step - loss: 0.0164 - acc: 0.9784 - val_loss: 0.0292 - val_acc: 0.9631\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/50\n",
      "203621/203621 [==============================] - 112s 551us/step - loss: 0.0158 - acc: 0.9790 - val_loss: 0.0292 - val_acc: 0.9635\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/50\n",
      "203621/203621 [==============================] - 112s 551us/step - loss: 0.0155 - acc: 0.9799 - val_loss: 0.0314 - val_acc: 0.9615\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/50\n",
      "203621/203621 [==============================] - 112s 550us/step - loss: 0.0152 - acc: 0.9801 - val_loss: 0.0302 - val_acc: 0.9624\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/50\n",
      "203621/203621 [==============================] - 112s 552us/step - loss: 0.0149 - acc: 0.9805 - val_loss: 0.0300 - val_acc: 0.9626\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 00021: early stopping\n",
      "'trainModelSP'  2592073.14 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 03:53:35.803079: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 03:53:35.877084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 03:53:35.877409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 03:53:35.877436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "embed (?, 9, 95)\n",
      "embed (?, 9, 95)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_nolearn_dropout_pos\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 95)        0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 95)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       73216       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,321,320\n",
      "Trainable params: 433,920\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 108s 529us/step - loss: 0.0474 - acc: 0.9416 - val_loss: 0.0248 - val_acc: 0.9674\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02483, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0259 - acc: 0.9663 - val_loss: 0.0213 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02483 to 0.02129, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0223 - acc: 0.9705 - val_loss: 0.0208 - val_acc: 0.9731\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02129 to 0.02084, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0201 - acc: 0.9735 - val_loss: 0.0186 - val_acc: 0.9755\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02084 to 0.01861, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 107s 523us/step - loss: 0.0185 - acc: 0.9756 - val_loss: 0.0191 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0174 - acc: 0.9770 - val_loss: 0.0183 - val_acc: 0.9769\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01861 to 0.01835, saving model to ./result/weights-06.h5\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 107s 526us/step - loss: 0.0163 - acc: 0.9784 - val_loss: 0.0187 - val_acc: 0.9764\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0153 - acc: 0.9798 - val_loss: 0.0177 - val_acc: 0.9769\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01835 to 0.01766, saving model to ./result/weights-08.h5\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0146 - acc: 0.9807 - val_loss: 0.0181 - val_acc: 0.9774\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0139 - acc: 0.9816 - val_loss: 0.0178 - val_acc: 0.9764\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0132 - acc: 0.9825 - val_loss: 0.0185 - val_acc: 0.9764\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0127 - acc: 0.9830 - val_loss: 0.0169 - val_acc: 0.9783\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01766 to 0.01694, saving model to ./result/weights-12.h5\n",
      "Epoch 13/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0123 - acc: 0.9836 - val_loss: 0.0189 - val_acc: 0.9756\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0119 - acc: 0.9842 - val_loss: 0.0171 - val_acc: 0.9773\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.0116 - acc: 0.9848 - val_loss: 0.0170 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0113 - acc: 0.9851 - val_loss: 0.0176 - val_acc: 0.9772\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/50\n",
      "203621/203621 [==============================] - 106s 523us/step - loss: 0.0108 - acc: 0.9854 - val_loss: 0.0185 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 00017: early stopping\n",
      "'trainModelSP'  2031063.21 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 04:27:26.918547: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 04:27:26.998573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 04:27:26.998945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 04:27:26.998985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 55)\n",
      "embed (?, 9, 55)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_nolearn_dropout_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 55)        0           embedding_1[0][0]                \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 55)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       42496       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,290,600\n",
      "Trainable params: 403,200\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0453 - acc: 0.9444 - val_loss: 0.0222 - val_acc: 0.9715\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02225, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0239 - acc: 0.9688 - val_loss: 0.0185 - val_acc: 0.9765\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02225 to 0.01854, saving model to ./result/weights-02.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.0206 - acc: 0.9727 - val_loss: 0.0180 - val_acc: 0.9778\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01854 to 0.01802, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0184 - acc: 0.9760 - val_loss: 0.0168 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01802 to 0.01684, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 107s 523us/step - loss: 0.0168 - acc: 0.9778 - val_loss: 0.0162 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01684 to 0.01616, saving model to ./result/weights-05.h5\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0157 - acc: 0.9793 - val_loss: 0.0156 - val_acc: 0.9798\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01616 to 0.01559, saving model to ./result/weights-06.h5\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 106s 523us/step - loss: 0.0147 - acc: 0.9805 - val_loss: 0.0162 - val_acc: 0.9793\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0142 - acc: 0.9813 - val_loss: 0.0154 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01559 to 0.01537, saving model to ./result/weights-08.h5\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 106s 523us/step - loss: 0.0132 - acc: 0.9826 - val_loss: 0.0147 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01537 to 0.01465, saving model to ./result/weights-09.h5\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0126 - acc: 0.9834 - val_loss: 0.0154 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 106s 523us/step - loss: 0.0123 - acc: 0.9838 - val_loss: 0.0151 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0117 - acc: 0.9844 - val_loss: 0.0149 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/50\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.0111 - acc: 0.9854 - val_loss: 0.0151 - val_acc: 0.9802\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/50\n",
      "203621/203621 [==============================] - 106s 523us/step - loss: 0.0109 - acc: 0.9857 - val_loss: 0.0139 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01465 to 0.01394, saving model to ./result/weights-14.h5\n",
      "Epoch 15/50\n",
      "203621/203621 [==============================] - 107s 523us/step - loss: 0.0104 - acc: 0.9864 - val_loss: 0.0146 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0102 - acc: 0.9866 - val_loss: 0.0146 - val_acc: 0.9807\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/50\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0100 - acc: 0.9869 - val_loss: 0.0148 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0095 - acc: 0.9874 - val_loss: 0.0145 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.0094 - acc: 0.9875 - val_loss: 0.0154 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 00019: early stopping\n",
      "'trainModelSP'  2256886.35 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 05:05:03.751829: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 05:05:03.825367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 05:05:03.825709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 05:05:03.825739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_nolearn_dropout_pos_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 100)       0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 100)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       77056       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,325,160\n",
      "Trainable params: 437,760\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0407 - acc: 0.9504 - val_loss: 0.0203 - val_acc: 0.9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02030, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 107s 528us/step - loss: 0.0208 - acc: 0.9729 - val_loss: 0.0166 - val_acc: 0.9786\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02030 to 0.01659, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0173 - acc: 0.9776 - val_loss: 0.0165 - val_acc: 0.9789\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01659 to 0.01645, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0155 - acc: 0.9801 - val_loss: 0.0153 - val_acc: 0.9802\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01645 to 0.01533, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 107s 523us/step - loss: 0.0140 - acc: 0.9816 - val_loss: 0.0159 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0126 - acc: 0.9836 - val_loss: 0.0157 - val_acc: 0.9801\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0118 - acc: 0.9848 - val_loss: 0.0139 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01533 to 0.01392, saving model to ./result/weights-07.h5\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 108s 529us/step - loss: 0.0112 - acc: 0.9854 - val_loss: 0.0144 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0105 - acc: 0.9863 - val_loss: 0.0139 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01392 to 0.01389, saving model to ./result/weights-09.h5\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0099 - acc: 0.9870 - val_loss: 0.0140 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.0093 - acc: 0.9880 - val_loss: 0.0141 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "203621/203621 [==============================] - 108s 529us/step - loss: 0.0091 - acc: 0.9881 - val_loss: 0.0137 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01389 to 0.01368, saving model to ./result/weights-12.h5\n",
      "Epoch 13/50\n",
      "203621/203621 [==============================] - 108s 529us/step - loss: 0.0085 - acc: 0.9888 - val_loss: 0.0135 - val_acc: 0.9822\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01368 to 0.01354, saving model to ./result/weights-13.h5\n",
      "Epoch 14/50\n",
      "203621/203621 [==============================] - 107s 528us/step - loss: 0.0082 - acc: 0.9895 - val_loss: 0.0143 - val_acc: 0.9822\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/50\n",
      "203621/203621 [==============================] - 107s 528us/step - loss: 0.0079 - acc: 0.9898 - val_loss: 0.0137 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0079 - acc: 0.9897 - val_loss: 0.0136 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/50\n",
      "203621/203621 [==============================] - 106s 523us/step - loss: 0.0074 - acc: 0.9904 - val_loss: 0.0134 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01354 to 0.01341, saving model to ./result/weights-17.h5\n",
      "Epoch 18/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0071 - acc: 0.9908 - val_loss: 0.0141 - val_acc: 0.9822\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/50\n",
      "203621/203621 [==============================] - 108s 531us/step - loss: 0.0070 - acc: 0.9910 - val_loss: 0.0137 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/50\n",
      "203621/203621 [==============================] - 108s 529us/step - loss: 0.0067 - acc: 0.9913 - val_loss: 0.0133 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01341 to 0.01333, saving model to ./result/weights-20.h5\n",
      "Epoch 21/50\n",
      "203621/203621 [==============================] - 108s 530us/step - loss: 0.0065 - acc: 0.9917 - val_loss: 0.0138 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.0064 - acc: 0.9915 - val_loss: 0.0135 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0063 - acc: 0.9919 - val_loss: 0.0134 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/50\n",
      "203621/203621 [==============================] - 108s 529us/step - loss: 0.0061 - acc: 0.9922 - val_loss: 0.0140 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/50\n",
      "203621/203621 [==============================] - 108s 528us/step - loss: 0.0060 - acc: 0.9923 - val_loss: 0.0138 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 00025: early stopping\n",
      "'trainModelSP'  2987228.41 ms\n",
      "'testFeatures'  9867251.58 ms\n",
      "\n",
      "\n",
      "Glove Embeddings with Learning\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 05:54:50.966670: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 05:54:51.039911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 05:54:51.040253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 05:54:51.040283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "embed (?, 9, 50)\n",
      "embed (?, 9, 50)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_learn_base\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 5, 256)            196864    \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             163840    \n",
      "_________________________________________________________________\n",
      "out_pred (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,286,760\n",
      "Trainable params: 1,286,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 119s 583us/step - loss: 0.0405 - acc: 0.9511 - val_loss: 0.0283 - val_acc: 0.9640\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02826, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621/203621 [==============================] - 118s 581us/step - loss: 0.0137 - acc: 0.9827 - val_loss: 0.0260 - val_acc: 0.9671\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02826 to 0.02599, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 118s 580us/step - loss: 0.0080 - acc: 0.9904 - val_loss: 0.0267 - val_acc: 0.9665\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 119s 583us/step - loss: 0.0056 - acc: 0.9932 - val_loss: 0.0269 - val_acc: 0.9669\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 117s 577us/step - loss: 0.0040 - acc: 0.9951 - val_loss: 0.0299 - val_acc: 0.9646\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 117s 577us/step - loss: 0.0031 - acc: 0.9962 - val_loss: 0.0296 - val_acc: 0.9656\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 119s 585us/step - loss: 0.0025 - acc: 0.9970 - val_loss: 0.0284 - val_acc: 0.9678\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 00007: early stopping\n",
      "'trainModelSP'  929319.00 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 06:10:20.382955: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 06:10:20.463288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 06:10:20.463646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 06:10:20.463678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "embed (?, 9, 95)\n",
      "embed (?, 9, 95)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_learn_pos\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 95)        0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       73216       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,321,320\n",
      "Trainable params: 1,321,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 108s 530us/step - loss: 0.0324 - acc: 0.9624 - val_loss: 0.0184 - val_acc: 0.9775\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01836, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 108s 528us/step - loss: 0.0095 - acc: 0.9887 - val_loss: 0.0183 - val_acc: 0.9770\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01836 to 0.01832, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 107s 523us/step - loss: 0.0054 - acc: 0.9937 - val_loss: 0.0184 - val_acc: 0.9769\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0037 - acc: 0.9957 - val_loss: 0.0182 - val_acc: 0.9785\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01832 to 0.01824, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0027 - acc: 0.9969 - val_loss: 0.0189 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0021 - acc: 0.9975 - val_loss: 0.0207 - val_acc: 0.9754\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0017 - acc: 0.9979 - val_loss: 0.0188 - val_acc: 0.9785\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0013 - acc: 0.9983 - val_loss: 0.0180 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01824 to 0.01801, saving model to ./result/weights-08.h5\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0012 - acc: 0.9985 - val_loss: 0.0195 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 107s 523us/step - loss: 0.0010 - acc: 0.9987 - val_loss: 0.0188 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 8.3044e-04 - acc: 0.9989 - val_loss: 0.0204 - val_acc: 0.9766\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 6.3403e-04 - acc: 0.9991 - val_loss: 0.0201 - val_acc: 0.9771\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/50\n",
      "203621/203621 [==============================] - 107s 526us/step - loss: 6.5639e-04 - acc: 0.9991 - val_loss: 0.0198 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 00013: early stopping\n",
      "'trainModelSP'  1556897.99 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-12 06:36:17.180701: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 06:36:17.260416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 06:36:17.260757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 06:36:17.260790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 55)\n",
      "embed (?, 9, 55)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_learn_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 55)        0           embedding_1[0][0]                \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       42496       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,290,600\n",
      "Trainable params: 1,290,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 109s 535us/step - loss: 0.0299 - acc: 0.9655 - val_loss: 0.0172 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01724, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 108s 528us/step - loss: 0.0081 - acc: 0.9903 - val_loss: 0.0156 - val_acc: 0.9811\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01724 to 0.01556, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 108s 532us/step - loss: 0.0044 - acc: 0.9949 - val_loss: 0.0144 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01556 to 0.01436, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 109s 533us/step - loss: 0.0029 - acc: 0.9967 - val_loss: 0.0147 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 108s 533us/step - loss: 0.0021 - acc: 0.9975 - val_loss: 0.0155 - val_acc: 0.9814\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 108s 528us/step - loss: 0.0015 - acc: 0.9982 - val_loss: 0.0151 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 108s 529us/step - loss: 0.0012 - acc: 0.9986 - val_loss: 0.0159 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 108s 533us/step - loss: 9.9352e-04 - acc: 0.9988 - val_loss: 0.0159 - val_acc: 0.9820\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 00008: early stopping\n",
      "'trainModelSP'  967952.91 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 06:52:25.141789: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 06:52:25.217470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 06:52:25.217928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 06:52:25.218013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_learn_pos_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 100)       0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       77056       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,325,160\n",
      "Trainable params: 1,325,160\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 108s 529us/step - loss: 0.0279 - acc: 0.9677 - val_loss: 0.0158 - val_acc: 0.9803\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01577, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 108s 528us/step - loss: 0.0076 - acc: 0.9908 - val_loss: 0.0135 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01577 to 0.01348, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 108s 531us/step - loss: 0.0042 - acc: 0.9952 - val_loss: 0.0142 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 108s 531us/step - loss: 0.0028 - acc: 0.9968 - val_loss: 0.0149 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 108s 531us/step - loss: 0.0020 - acc: 0.9977 - val_loss: 0.0163 - val_acc: 0.9818\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 108s 533us/step - loss: 0.0017 - acc: 0.9981 - val_loss: 0.0152 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 108s 531us/step - loss: 0.0014 - acc: 0.9984 - val_loss: 0.0144 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 00007: early stopping\n",
      "'trainModelSP'  859811.38 ms\n",
      "'testFeatures'  4313981.62 ms\n",
      "\n",
      "\n",
      "Glove Embeddings with Learning and Dropout\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 07:06:45.095276: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 07:06:45.177944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 07:06:45.178300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 07:06:45.178333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "embed (?, 9, 50)\n",
      "embed (?, 9, 50)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_learn_dropout_base\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 9, 50)             0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 5, 256)            196864    \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             163840    \n",
      "_________________________________________________________________\n",
      "out_pred (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,286,760\n",
      "Trainable params: 1,286,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 109s 533us/step - loss: 0.0514 - acc: 0.9365 - val_loss: 0.0303 - val_acc: 0.9616\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03030, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 108s 529us/step - loss: 0.0236 - acc: 0.9692 - val_loss: 0.0270 - val_acc: 0.9656\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03030 to 0.02700, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 107s 526us/step - loss: 0.0174 - acc: 0.9772 - val_loss: 0.0253 - val_acc: 0.9672\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02700 to 0.02530, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0142 - acc: 0.9816 - val_loss: 0.0248 - val_acc: 0.9678\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02530 to 0.02483, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0115 - acc: 0.9850 - val_loss: 0.0264 - val_acc: 0.9662\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0096 - acc: 0.9876 - val_loss: 0.0237 - val_acc: 0.9701\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02483 to 0.02367, saving model to ./result/weights-06.h5\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 108s 528us/step - loss: 0.0084 - acc: 0.9892 - val_loss: 0.0249 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 108s 528us/step - loss: 0.0071 - acc: 0.9909 - val_loss: 0.0257 - val_acc: 0.9678\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0062 - acc: 0.9922 - val_loss: 0.0244 - val_acc: 0.9703\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0053 - acc: 0.9931 - val_loss: 0.0257 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0050 - acc: 0.9935 - val_loss: 0.0251 - val_acc: 0.9696\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 00011: early stopping\n",
      "'trainModelSP'  1330780.05 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 07:28:55.777191: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 07:28:55.850433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 07:28:55.850767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 07:28:55.850795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "embed (?, 9, 95)\n",
      "embed (?, 9, 95)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model: glove_learn_dropout_pos\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 95)        0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 95)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       73216       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,321,320\n",
      "Trainable params: 1,321,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 118s 579us/step - loss: 0.0426 - acc: 0.9482 - val_loss: 0.0202 - val_acc: 0.9737\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02021, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 118s 581us/step - loss: 0.0185 - acc: 0.9764 - val_loss: 0.0177 - val_acc: 0.9781\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02021 to 0.01771, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 119s 586us/step - loss: 0.0139 - acc: 0.9824 - val_loss: 0.0168 - val_acc: 0.9785\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01771 to 0.01679, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 119s 587us/step - loss: 0.0112 - acc: 0.9856 - val_loss: 0.0164 - val_acc: 0.9799\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01679 to 0.01642, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 118s 579us/step - loss: 0.0090 - acc: 0.9885 - val_loss: 0.0165 - val_acc: 0.9793\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 119s 585us/step - loss: 0.0075 - acc: 0.9903 - val_loss: 0.0170 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 119s 582us/step - loss: 0.0066 - acc: 0.9914 - val_loss: 0.0180 - val_acc: 0.9773\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 118s 580us/step - loss: 0.0056 - acc: 0.9926 - val_loss: 0.0160 - val_acc: 0.9801\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01642 to 0.01605, saving model to ./result/weights-08.h5\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 119s 586us/step - loss: 0.0049 - acc: 0.9936 - val_loss: 0.0163 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 119s 586us/step - loss: 0.0045 - acc: 0.9943 - val_loss: 0.0161 - val_acc: 0.9799\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 119s 583us/step - loss: 0.0040 - acc: 0.9949 - val_loss: 0.0173 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "203621/203621 [==============================] - 117s 577us/step - loss: 0.0033 - acc: 0.9958 - val_loss: 0.0168 - val_acc: 0.9803\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/50\n",
      "203621/203621 [==============================] - 119s 587us/step - loss: 0.0032 - acc: 0.9958 - val_loss: 0.0187 - val_acc: 0.9771\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 00013: early stopping\n",
      "'trainModelSP'  1717832.76 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 07:57:33.653785: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 07:57:33.731704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 07:57:33.732066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 07:57:33.732096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 55)\n",
      "embed (?, 9, 55)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_learn_dropout_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 55)        0           embedding_1[0][0]                \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 55)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       42496       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,290,600\n",
      "Trainable params: 1,290,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 109s 536us/step - loss: 0.0413 - acc: 0.9502 - val_loss: 0.0193 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01935, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 109s 534us/step - loss: 0.0172 - acc: 0.9779 - val_loss: 0.0165 - val_acc: 0.9791\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01935 to 0.01654, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 108s 530us/step - loss: 0.0122 - acc: 0.9844 - val_loss: 0.0147 - val_acc: 0.9811\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01654 to 0.01468, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 108s 533us/step - loss: 0.0097 - acc: 0.9876 - val_loss: 0.0145 - val_acc: 0.9811\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01468 to 0.01452, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 108s 531us/step - loss: 0.0080 - acc: 0.9898 - val_loss: 0.0140 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01452 to 0.01395, saving model to ./result/weights-05.h5\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 108s 533us/step - loss: 0.0067 - acc: 0.9912 - val_loss: 0.0137 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01395 to 0.01374, saving model to ./result/weights-06.h5\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 109s 535us/step - loss: 0.0058 - acc: 0.9926 - val_loss: 0.0138 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 108s 530us/step - loss: 0.0051 - acc: 0.9933 - val_loss: 0.0132 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01374 to 0.01320, saving model to ./result/weights-08.h5\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 108s 532us/step - loss: 0.0043 - acc: 0.9945 - val_loss: 0.0140 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 109s 538us/step - loss: 0.0040 - acc: 0.9948 - val_loss: 0.0138 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 108s 528us/step - loss: 0.0034 - acc: 0.9956 - val_loss: 0.0142 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "203621/203621 [==============================] - 109s 537us/step - loss: 0.0033 - acc: 0.9956 - val_loss: 0.0146 - val_acc: 0.9823\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/50\n",
      "203621/203621 [==============================] - 109s 534us/step - loss: 0.0030 - acc: 0.9961 - val_loss: 0.0151 - val_acc: 0.9815\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 00013: early stopping\n",
      "'trainModelSP'  1587505.73 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 08:24:01.066977: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 08:24:01.140447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 08:24:01.140774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 08:24:01.140803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_learn_dropout_pos_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 100)       0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 100)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       77056       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,325,160\n",
      "Trainable params: 1,325,160\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 117s 577us/step - loss: 0.0370 - acc: 0.9558 - val_loss: 0.0166 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01655, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 118s 582us/step - loss: 0.0153 - acc: 0.9802 - val_loss: 0.0144 - val_acc: 0.9820\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01655 to 0.01444, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 118s 582us/step - loss: 0.0110 - acc: 0.9858 - val_loss: 0.0136 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01444 to 0.01355, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621/203621 [==============================] - 118s 581us/step - loss: 0.0086 - acc: 0.9890 - val_loss: 0.0138 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 119s 585us/step - loss: 0.0069 - acc: 0.9914 - val_loss: 0.0130 - val_acc: 0.9837\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01355 to 0.01298, saving model to ./result/weights-05.h5\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 118s 581us/step - loss: 0.0058 - acc: 0.9925 - val_loss: 0.0131 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 119s 582us/step - loss: 0.0051 - acc: 0.9935 - val_loss: 0.0123 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01298 to 0.01235, saving model to ./result/weights-07.h5\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 119s 583us/step - loss: 0.0044 - acc: 0.9943 - val_loss: 0.0128 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 118s 582us/step - loss: 0.0038 - acc: 0.9950 - val_loss: 0.0130 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 119s 586us/step - loss: 0.0033 - acc: 0.9957 - val_loss: 0.0127 - val_acc: 0.9847\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 117s 577us/step - loss: 0.0029 - acc: 0.9963 - val_loss: 0.0134 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/50\n",
      "203621/203621 [==============================] - 118s 577us/step - loss: 0.0026 - acc: 0.9967 - val_loss: 0.0143 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 00012: early stopping\n",
      "'trainModelSP'  1573101.17 ms\n",
      "'testFeatures'  6209220.06 ms\n"
     ]
    }
   ],
   "source": [
    "# CONSIDER WRAPPING IN A FUNCTION... PROS AND CONS...\n",
    "\n",
    "# capsnet training function\n",
    "testFunc = \"trainCapsModel.py\"\n",
    "\n",
    "hypers = hyper_param_caps.copy()\n",
    "hypers['epochs'] = 50\n",
    "hypers['stopping_patience'] = 5\n",
    "hypers['use_pos_tags'] = False\n",
    "hypers['use_capitalization_info'] = False\n",
    "\n",
    "# try different embeddings\n",
    "# learn embeddings\n",
    "print(\"\\n\\nLearn Embeddings\")\n",
    "hypers['use_glove'] = False\n",
    "hypers['embed_dropout'] = 0.0\n",
    "testFeatures( testFunc, \"learn\", hypers)\n",
    "\n",
    "# learn embeddings + Dropout\n",
    "print(\"\\n\\nLearn Embeddings and Dropout\")\n",
    "hypers['use_glove'] = False\n",
    "hypers['embed_dropout'] = 0.25\n",
    "testFeatures( testFunc, \"learn_dropout\", hypers)\n",
    "\n",
    "# use glove, no learn\n",
    "print(\"\\n\\nGlove Embeddings\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = False\n",
    "hypers['embed_dropout'] = 0.0\n",
    "testFeatures( testFunc, \"glove_nolearn\", hypers)\n",
    "\n",
    "# use glove, no learn + Dropout\n",
    "print(\"\\n\\nGlove Embeddings and Dropout\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = False\n",
    "hypers['embed_dropout'] = 0.25\n",
    "testFeatures( testFunc, \"glove_nolearn_dropout\", hypers)\n",
    "\n",
    "# use glove, learn\n",
    "print(\"\\n\\nGlove Embeddings with Learning\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = True\n",
    "hypers['embed_dropout'] = 0.0\n",
    "testFeatures( testFunc, \"glove_learn\", hypers)\n",
    "\n",
    "# use glove, learn + Dropout\n",
    "print(\"\\n\\nGlove Embeddings with Learning and Dropout\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = True\n",
    "hypers['embed_dropout'] = 0.25\n",
    "testFeatures( testFunc, \"glove_learn_dropout\", hypers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder if pos tags could be more useful with another representation (besides 1-hot encodings). I expected them to rock. Are they too sparse? do they make the input too large for the network? Check the paper for clues..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD - Nesterov Momentum Optimizer\n",
      "\n",
      "\n",
      "Learn Embeddings\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "embed (?, 9, 50)\n",
      "embed (?, 9, 50)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: SGD_primcaps_learn_base\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 5, 256)            196864    \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             163840    \n",
      "_________________________________________________________________\n",
      "out_pred (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,286,760\n",
      "Trainable params: 1,286,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 08:50:15.432326: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 08:50:15.508279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 08:50:15.508601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 08:50:15.508631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 111s 543us/step - loss: 0.8100 - acc: 0.0845 - val_loss: 0.8100 - val_acc: 0.0851\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.81000, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 111s 546us/step - loss: 0.8100 - acc: 0.0845 - val_loss: 0.8100 - val_acc: 0.0851\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 112s 548us/step - loss: 0.8100 - acc: 0.0845 - val_loss: 0.8100 - val_acc: 0.0851\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 112s 550us/step - loss: 0.8100 - acc: 0.0845 - val_loss: 0.8100 - val_acc: 0.0851\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 111s 547us/step - loss: 0.8100 - acc: 0.0845 - val_loss: 0.8100 - val_acc: 0.0851\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 111s 546us/step - loss: 0.8100 - acc: 0.0845 - val_loss: 0.8100 - val_acc: 0.0851\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 00006: early stopping\n",
      "'trainModelSP'  742268.01 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "embed (?, 9, 95)\n",
      "embed (?, 9, 95)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: SGD_primcaps_learn_pos\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 95)        0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       73216       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,321,320\n",
      "Trainable params: 1,321,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 09:02:37.797853: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 09:02:37.871782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 09:02:37.872114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 09:02:37.872160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.8100 - acc: 0.1185 - val_loss: 0.8100 - val_acc: 0.1145\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.81000, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.8100 - acc: 0.1199 - val_loss: 0.8100 - val_acc: 0.1158\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.8100 - acc: 0.1211 - val_loss: 0.8100 - val_acc: 0.1171\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.8100 - acc: 0.1224 - val_loss: 0.8100 - val_acc: 0.1184\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.8100 - acc: 0.1239 - val_loss: 0.8100 - val_acc: 0.1196\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-05.h5\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.1252 - val_loss: 0.8100 - val_acc: 0.1215\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-06.h5\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.8100 - acc: 0.1269 - val_loss: 0.8100 - val_acc: 0.1226\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-07.h5\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.8100 - acc: 0.1284 - val_loss: 0.8100 - val_acc: 0.1244\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-08.h5\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.8100 - acc: 0.1299 - val_loss: 0.8100 - val_acc: 0.1259\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-09.h5\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.1315 - val_loss: 0.8100 - val_acc: 0.1274\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-10.h5\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.1332 - val_loss: 0.8100 - val_acc: 0.1289\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-11.h5\n",
      "Epoch 12/50\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.8100 - acc: 0.1348 - val_loss: 0.8100 - val_acc: 0.1305\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-12.h5\n",
      "Epoch 13/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.8100 - acc: 0.1363 - val_loss: 0.8100 - val_acc: 0.1322\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-13.h5\n",
      "Epoch 14/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.1379 - val_loss: 0.8100 - val_acc: 0.1342\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-14.h5\n",
      "Epoch 15/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.8100 - acc: 0.1397 - val_loss: 0.8100 - val_acc: 0.1356\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-15.h5\n",
      "Epoch 16/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.8100 - acc: 0.1416 - val_loss: 0.8100 - val_acc: 0.1374\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-16.h5\n",
      "Epoch 17/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.8100 - acc: 0.1434 - val_loss: 0.8100 - val_acc: 0.1395\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-17.h5\n",
      "Epoch 18/50\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.8100 - acc: 0.1453 - val_loss: 0.8100 - val_acc: 0.1413\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-18.h5\n",
      "Epoch 19/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.1473 - val_loss: 0.8100 - val_acc: 0.1432\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-19.h5\n",
      "Epoch 20/50\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.8100 - acc: 0.1490 - val_loss: 0.8100 - val_acc: 0.1453\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-20.h5\n",
      "Epoch 21/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.1508 - val_loss: 0.8100 - val_acc: 0.1470\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-21.h5\n",
      "Epoch 22/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.8100 - acc: 0.1525 - val_loss: 0.8100 - val_acc: 0.1486\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-22.h5\n",
      "Epoch 23/50\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.8100 - acc: 0.1545 - val_loss: 0.8100 - val_acc: 0.1507\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-23.h5\n",
      "Epoch 24/50\n",
      "203621/203621 [==============================] - 106s 518us/step - loss: 0.8100 - acc: 0.1563 - val_loss: 0.8100 - val_acc: 0.1525\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-24.h5\n",
      "Epoch 25/50\n",
      "203621/203621 [==============================] - 105s 513us/step - loss: 0.8100 - acc: 0.1584 - val_loss: 0.8100 - val_acc: 0.1547\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-25.h5\n",
      "Epoch 26/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.8100 - acc: 0.1604 - val_loss: 0.8100 - val_acc: 0.1570\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-26.h5\n",
      "Epoch 27/50\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.8100 - acc: 0.1625 - val_loss: 0.8100 - val_acc: 0.1594\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-27.h5\n",
      "Epoch 28/50\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.8100 - acc: 0.1645 - val_loss: 0.8100 - val_acc: 0.1616\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-28.h5\n",
      "Epoch 29/50\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.8100 - acc: 0.1665 - val_loss: 0.8100 - val_acc: 0.1639\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-29.h5\n",
      "Epoch 30/50\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.8100 - acc: 0.1688 - val_loss: 0.8100 - val_acc: 0.1662\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-30.h5\n",
      "Epoch 31/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.8100 - acc: 0.1710 - val_loss: 0.8100 - val_acc: 0.1682\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-31.h5\n",
      "Epoch 32/50\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.8100 - acc: 0.1733 - val_loss: 0.8100 - val_acc: 0.1704\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-32.h5\n",
      "Epoch 33/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.8100 - acc: 0.1754 - val_loss: 0.8100 - val_acc: 0.1728\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-33.h5\n",
      "Epoch 34/50\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.8100 - acc: 0.1776 - val_loss: 0.8100 - val_acc: 0.1749\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-34.h5\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.1800 - val_loss: 0.8100 - val_acc: 0.1768\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-35.h5\n",
      "Epoch 36/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.8100 - acc: 0.1822 - val_loss: 0.8100 - val_acc: 0.1796\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-36.h5\n",
      "Epoch 37/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.8100 - acc: 0.1845 - val_loss: 0.8100 - val_acc: 0.1819\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-37.h5\n",
      "Epoch 38/50\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.8100 - acc: 0.1868 - val_loss: 0.8100 - val_acc: 0.1842\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-38.h5\n",
      "Epoch 39/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.8100 - acc: 0.1893 - val_loss: 0.8100 - val_acc: 0.1864\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-39.h5\n",
      "Epoch 40/50\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.8100 - acc: 0.1917 - val_loss: 0.8100 - val_acc: 0.1893\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-40.h5\n",
      "Epoch 41/50\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.8100 - acc: 0.1943 - val_loss: 0.8100 - val_acc: 0.1916\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-41.h5\n",
      "Epoch 42/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.8100 - acc: 0.1970 - val_loss: 0.8100 - val_acc: 0.1944\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-42.h5\n",
      "Epoch 43/50\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.8100 - acc: 0.1999 - val_loss: 0.8100 - val_acc: 0.1974\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-43.h5\n",
      "Epoch 44/50\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.8100 - acc: 0.2024 - val_loss: 0.8100 - val_acc: 0.1997\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-44.h5\n",
      "Epoch 45/50\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.8100 - acc: 0.2051 - val_loss: 0.8100 - val_acc: 0.2024\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-45.h5\n",
      "Epoch 46/50\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.8100 - acc: 0.2078 - val_loss: 0.8100 - val_acc: 0.2054\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-46.h5\n",
      "Epoch 47/50\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.8100 - acc: 0.2106 - val_loss: 0.8100 - val_acc: 0.2084\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-47.h5\n",
      "Epoch 48/50\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.8100 - acc: 0.2133 - val_loss: 0.8100 - val_acc: 0.2117\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-48.h5\n",
      "Epoch 49/50\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.8100 - acc: 0.2161 - val_loss: 0.8100 - val_acc: 0.2144\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-49.h5\n",
      "Epoch 50/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.8100 - acc: 0.2190 - val_loss: 0.8100 - val_acc: 0.2170\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-50.h5\n",
      "'trainModelSP'  5824369.27 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 55)\n",
      "embed (?, 9, 55)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: SGD_primcaps_learn_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 55)        0           embedding_1[0][0]                \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       42496       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,290,600\n",
      "Trainable params: 1,290,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 10:39:42.309275: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 10:39:42.389981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 10:39:42.390394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 10:39:42.390452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.8100 - acc: 0.1817 - val_loss: 0.8100 - val_acc: 0.1941\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.80999, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.8100 - acc: 0.2031 - val_loss: 0.8100 - val_acc: 0.2184\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-02.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.8100 - acc: 0.2291 - val_loss: 0.8100 - val_acc: 0.2491\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.8100 - acc: 0.2602 - val_loss: 0.8100 - val_acc: 0.2843\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.8100 - acc: 0.2984 - val_loss: 0.8100 - val_acc: 0.3262\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-05.h5\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 106s 518us/step - loss: 0.8100 - acc: 0.3429 - val_loss: 0.8100 - val_acc: 0.3747\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-06.h5\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.8100 - acc: 0.3928 - val_loss: 0.8100 - val_acc: 0.4278\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-07.h5\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.8100 - acc: 0.4454 - val_loss: 0.8100 - val_acc: 0.4822\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-08.h5\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.8100 - acc: 0.4973 - val_loss: 0.8100 - val_acc: 0.5300\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-09.h5\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 107s 526us/step - loss: 0.8100 - acc: 0.5441 - val_loss: 0.8100 - val_acc: 0.5712\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-10.h5\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.8100 - acc: 0.5838 - val_loss: 0.8100 - val_acc: 0.6077\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-11.h5\n",
      "Epoch 12/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.8100 - acc: 0.6162 - val_loss: 0.8100 - val_acc: 0.6366\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-12.h5\n",
      "Epoch 13/50\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.8100 - acc: 0.6445 - val_loss: 0.8100 - val_acc: 0.6629\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-13.h5\n",
      "Epoch 14/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.6701 - val_loss: 0.8100 - val_acc: 0.6848\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-14.h5\n",
      "Epoch 15/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.8100 - acc: 0.6913 - val_loss: 0.8100 - val_acc: 0.7071\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-15.h5\n",
      "Epoch 16/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.7114 - val_loss: 0.8100 - val_acc: 0.7233\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-16.h5\n",
      "Epoch 17/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.8100 - acc: 0.7282 - val_loss: 0.8100 - val_acc: 0.7397\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-17.h5\n",
      "Epoch 18/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.8100 - acc: 0.7440 - val_loss: 0.8100 - val_acc: 0.7528\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-18.h5\n",
      "Epoch 19/50\n",
      "203621/203621 [==============================] - 107s 526us/step - loss: 0.8100 - acc: 0.7568 - val_loss: 0.8100 - val_acc: 0.7642\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-19.h5\n",
      "Epoch 20/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.8100 - acc: 0.7689 - val_loss: 0.8100 - val_acc: 0.7757\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-20.h5\n",
      "Epoch 21/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.8100 - acc: 0.7811 - val_loss: 0.8100 - val_acc: 0.7864\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-21.h5\n",
      "Epoch 22/50\n",
      "203621/203621 [==============================] - 106s 518us/step - loss: 0.8100 - acc: 0.7900 - val_loss: 0.8100 - val_acc: 0.7938\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-22.h5\n",
      "Epoch 23/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.8100 - acc: 0.7978 - val_loss: 0.8100 - val_acc: 0.8011\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-23.h5\n",
      "Epoch 24/50\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.8100 - acc: 0.8045 - val_loss: 0.8100 - val_acc: 0.8063\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-24.h5\n",
      "Epoch 25/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.8100 - acc: 0.8092 - val_loss: 0.8100 - val_acc: 0.8103\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-25.h5\n",
      "Epoch 26/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.8100 - acc: 0.8126 - val_loss: 0.8100 - val_acc: 0.8138\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.80999 to 0.80998, saving model to ./result/weights-26.h5\n",
      "Epoch 27/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.8100 - acc: 0.8154 - val_loss: 0.8100 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-27.h5\n",
      "Epoch 28/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.8180 - val_loss: 0.8100 - val_acc: 0.8191\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-28.h5\n",
      "Epoch 29/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.8100 - acc: 0.8203 - val_loss: 0.8100 - val_acc: 0.8212\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-29.h5\n",
      "Epoch 30/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.8100 - acc: 0.8222 - val_loss: 0.8100 - val_acc: 0.8230\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-30.h5\n",
      "Epoch 31/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.8100 - acc: 0.8239 - val_loss: 0.8100 - val_acc: 0.8242\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-31.h5\n",
      "Epoch 32/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.8100 - acc: 0.8251 - val_loss: 0.8100 - val_acc: 0.8256\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-32.h5\n",
      "Epoch 33/50\n",
      "203621/203621 [==============================] - 106s 518us/step - loss: 0.8100 - acc: 0.8262 - val_loss: 0.8100 - val_acc: 0.8269\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-33.h5\n",
      "Epoch 34/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.8100 - acc: 0.8271 - val_loss: 0.8100 - val_acc: 0.8278\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-34.h5\n",
      "Epoch 35/50\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.8100 - acc: 0.8277 - val_loss: 0.8100 - val_acc: 0.8283\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-35.h5\n",
      "Epoch 36/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.8100 - acc: 0.8281 - val_loss: 0.8100 - val_acc: 0.8286\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-36.h5\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.8100 - acc: 0.8285 - val_loss: 0.8100 - val_acc: 0.8295\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.80998 to 0.80997, saving model to ./result/weights-37.h5\n",
      "Epoch 38/50\n",
      "203621/203621 [==============================] - 106s 518us/step - loss: 0.8100 - acc: 0.8291 - val_loss: 0.8100 - val_acc: 0.8296\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.80997 to 0.80997, saving model to ./result/weights-38.h5\n",
      "Epoch 39/50\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.8100 - acc: 0.8294 - val_loss: 0.8100 - val_acc: 0.8300\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.80997 to 0.80997, saving model to ./result/weights-39.h5\n",
      "Epoch 40/50\n",
      "203621/203621 [==============================] - 104s 508us/step - loss: 0.8100 - acc: 0.8298 - val_loss: 0.8100 - val_acc: 0.8301\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.80997 to 0.80997, saving model to ./result/weights-40.h5\n",
      "Epoch 41/50\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.8100 - acc: 0.8301 - val_loss: 0.8100 - val_acc: 0.8303\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.80997 to 0.80997, saving model to ./result/weights-41.h5\n",
      "Epoch 42/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.8100 - acc: 0.8303 - val_loss: 0.8100 - val_acc: 0.8306\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.80997 to 0.80997, saving model to ./result/weights-42.h5\n",
      "Epoch 43/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.8100 - acc: 0.8306 - val_loss: 0.8100 - val_acc: 0.8307\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.80997 to 0.80996, saving model to ./result/weights-43.h5\n",
      "Epoch 44/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.8100 - acc: 0.8308 - val_loss: 0.8100 - val_acc: 0.8310\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.80996 to 0.80996, saving model to ./result/weights-44.h5\n",
      "Epoch 45/50\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.8100 - acc: 0.8310 - val_loss: 0.8100 - val_acc: 0.8311\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.80996 to 0.80996, saving model to ./result/weights-45.h5\n",
      "Epoch 46/50\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.8100 - acc: 0.8314 - val_loss: 0.8100 - val_acc: 0.8314\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.80996 to 0.80996, saving model to ./result/weights-46.h5\n",
      "Epoch 47/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.8100 - acc: 0.8319 - val_loss: 0.8100 - val_acc: 0.8319\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.80996 to 0.80995, saving model to ./result/weights-47.h5\n",
      "Epoch 48/50\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.8100 - acc: 0.8322 - val_loss: 0.8099 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.80995 to 0.80995, saving model to ./result/weights-48.h5\n",
      "Epoch 49/50\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.8099 - acc: 0.8324 - val_loss: 0.8099 - val_acc: 0.8323\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.80995 to 0.80995, saving model to ./result/weights-49.h5\n",
      "Epoch 50/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.8099 - acc: 0.8325 - val_loss: 0.8099 - val_acc: 0.8324\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.80995 to 0.80994, saving model to ./result/weights-50.h5\n",
      "'trainModelSP'  5861192.02 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: SGD_primcaps_learn_pos_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 100)       0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       77056       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,325,160\n",
      "Trainable params: 1,325,160\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 12:17:23.369038: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 12:17:23.443383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 12:17:23.443719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 12:17:23.443751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.8100 - acc: 0.0883 - val_loss: 0.8100 - val_acc: 0.0985\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.80999, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.8100 - acc: 0.0991 - val_loss: 0.8100 - val_acc: 0.1099\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.8100 - acc: 0.1114 - val_loss: 0.8100 - val_acc: 0.1231\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.8100 - acc: 0.1259 - val_loss: 0.8100 - val_acc: 0.1391\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.1426 - val_loss: 0.8100 - val_acc: 0.1581\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-05.h5\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.1625 - val_loss: 0.8100 - val_acc: 0.1809\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-06.h5\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.8100 - acc: 0.1876 - val_loss: 0.8100 - val_acc: 0.2045\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-07.h5\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.8100 - acc: 0.2155 - val_loss: 0.8100 - val_acc: 0.2350\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-08.h5\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.8100 - acc: 0.2463 - val_loss: 0.8100 - val_acc: 0.2669\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-09.h5\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.2804 - val_loss: 0.8100 - val_acc: 0.3008\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-10.h5\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 104s 508us/step - loss: 0.8100 - acc: 0.3167 - val_loss: 0.8100 - val_acc: 0.3401\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-11.h5\n",
      "Epoch 12/50\n",
      "203621/203621 [==============================] - 106s 518us/step - loss: 0.8100 - acc: 0.3587 - val_loss: 0.8100 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.80999 to 0.80999, saving model to ./result/weights-12.h5\n",
      "Epoch 13/50\n",
      "203621/203621 [==============================] - 106s 518us/step - loss: 0.8100 - acc: 0.4018 - val_loss: 0.8100 - val_acc: 0.4263\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.80999 to 0.80998, saving model to ./result/weights-13.h5\n",
      "Epoch 14/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.4473 - val_loss: 0.8100 - val_acc: 0.4715\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-14.h5\n",
      "Epoch 15/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.4935 - val_loss: 0.8100 - val_acc: 0.5173\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-15.h5\n",
      "Epoch 16/50\n",
      "203621/203621 [==============================] - 103s 506us/step - loss: 0.8100 - acc: 0.5381 - val_loss: 0.8100 - val_acc: 0.5620\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-16.h5\n",
      "Epoch 17/50\n",
      "203621/203621 [==============================] - 106s 518us/step - loss: 0.8100 - acc: 0.5819 - val_loss: 0.8100 - val_acc: 0.6040\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-17.h5\n",
      "Epoch 18/50\n",
      "203621/203621 [==============================] - 102s 503us/step - loss: 0.8100 - acc: 0.6223 - val_loss: 0.8100 - val_acc: 0.6418\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-18.h5\n",
      "Epoch 19/50\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.8100 - acc: 0.6588 - val_loss: 0.8100 - val_acc: 0.6767\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-19.h5\n",
      "Epoch 20/50\n",
      "203621/203621 [==============================] - 104s 508us/step - loss: 0.8100 - acc: 0.6927 - val_loss: 0.8100 - val_acc: 0.7093\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-20.h5\n",
      "Epoch 21/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.7226 - val_loss: 0.8100 - val_acc: 0.7370\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-21.h5\n",
      "Epoch 22/50\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.8100 - acc: 0.7479 - val_loss: 0.8100 - val_acc: 0.7611\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.80998 to 0.80998, saving model to ./result/weights-22.h5\n",
      "Epoch 23/50\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.8100 - acc: 0.7692 - val_loss: 0.8100 - val_acc: 0.7799\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.80998 to 0.80997, saving model to ./result/weights-23.h5\n",
      "Epoch 24/50\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.8100 - acc: 0.7863 - val_loss: 0.8100 - val_acc: 0.7940\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.80997 to 0.80997, saving model to ./result/weights-24.h5\n",
      "Epoch 25/50\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.8100 - acc: 0.7984 - val_loss: 0.8100 - val_acc: 0.8043\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.80997 to 0.80997, saving model to ./result/weights-25.h5\n",
      "Epoch 26/50\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.8100 - acc: 0.8074 - val_loss: 0.8100 - val_acc: 0.8115\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.80997 to 0.80997, saving model to ./result/weights-26.h5\n",
      "Epoch 27/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.8100 - acc: 0.8142 - val_loss: 0.8100 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.80997 to 0.80997, saving model to ./result/weights-27.h5\n",
      "Epoch 28/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.8100 - acc: 0.8190 - val_loss: 0.8100 - val_acc: 0.8205\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.80997 to 0.80997, saving model to ./result/weights-28.h5\n",
      "Epoch 29/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.8226 - val_loss: 0.8100 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.80997 to 0.80996, saving model to ./result/weights-29.h5\n",
      "Epoch 30/50\n",
      "203621/203621 [==============================] - 103s 507us/step - loss: 0.8100 - acc: 0.8248 - val_loss: 0.8100 - val_acc: 0.8253\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.80996 to 0.80996, saving model to ./result/weights-30.h5\n",
      "Epoch 31/50\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.8100 - acc: 0.8270 - val_loss: 0.8100 - val_acc: 0.8268\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.80996 to 0.80996, saving model to ./result/weights-31.h5\n",
      "Epoch 32/50\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.8100 - acc: 0.8286 - val_loss: 0.8100 - val_acc: 0.8283\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.80996 to 0.80995, saving model to ./result/weights-32.h5\n",
      "Epoch 33/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.8304 - val_loss: 0.8100 - val_acc: 0.8310\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.80995 to 0.80995, saving model to ./result/weights-33.h5\n",
      "Epoch 34/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.8100 - acc: 0.8319 - val_loss: 0.8099 - val_acc: 0.8318\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.80995 to 0.80995, saving model to ./result/weights-34.h5\n",
      "Epoch 35/50\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.8099 - acc: 0.8324 - val_loss: 0.8099 - val_acc: 0.8322\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.80995 to 0.80994, saving model to ./result/weights-35.h5\n",
      "Epoch 36/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8099 - acc: 0.8326 - val_loss: 0.8099 - val_acc: 0.8324\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.80994 to 0.80994, saving model to ./result/weights-36.h5\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8099 - acc: 0.8328 - val_loss: 0.8099 - val_acc: 0.8324\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.80994 to 0.80993, saving model to ./result/weights-37.h5\n",
      "Epoch 38/50\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.8099 - acc: 0.8328 - val_loss: 0.8099 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.80993 to 0.80993, saving model to ./result/weights-38.h5\n",
      "Epoch 39/50\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.8099 - acc: 0.8328 - val_loss: 0.8099 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.80993 to 0.80992, saving model to ./result/weights-39.h5\n",
      "Epoch 40/50\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.8099 - acc: 0.8328 - val_loss: 0.8099 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.80992 to 0.80991, saving model to ./result/weights-40.h5\n",
      "Epoch 41/50\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.8099 - acc: 0.8328 - val_loss: 0.8099 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.80991 to 0.80991, saving model to ./result/weights-41.h5\n",
      "Epoch 42/50\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.8099 - acc: 0.8328 - val_loss: 0.8099 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.80991 to 0.80990, saving model to ./result/weights-42.h5\n",
      "Epoch 43/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8099 - acc: 0.8328 - val_loss: 0.8099 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.80990 to 0.80988, saving model to ./result/weights-43.h5\n",
      "Epoch 44/50\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.8099 - acc: 0.8328 - val_loss: 0.8099 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.80988 to 0.80987, saving model to ./result/weights-44.h5\n",
      "Epoch 45/50\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.8099 - acc: 0.8328 - val_loss: 0.8099 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.80987 to 0.80985, saving model to ./result/weights-45.h5\n",
      "Epoch 46/50\n",
      "203621/203621 [==============================] - 103s 508us/step - loss: 0.8098 - acc: 0.8328 - val_loss: 0.8098 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.80985 to 0.80984, saving model to ./result/weights-46.h5\n",
      "Epoch 47/50\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.8098 - acc: 0.8328 - val_loss: 0.8098 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.80984 to 0.80981, saving model to ./result/weights-47.h5\n",
      "Epoch 48/50\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.8098 - acc: 0.8328 - val_loss: 0.8098 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.80981 to 0.80978, saving model to ./result/weights-48.h5\n",
      "Epoch 49/50\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.8098 - acc: 0.8328 - val_loss: 0.8097 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.80978 to 0.80975, saving model to ./result/weights-49.h5\n",
      "Epoch 50/50\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.8097 - acc: 0.8328 - val_loss: 0.8097 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.80975 to 0.80970, saving model to ./result/weights-50.h5\n",
      "'trainModelSP'  5812353.66 ms\n",
      "'testFeatures'  18240183.31 ms\n",
      "\n",
      "\n",
      "Learn Embeddings and Dropout\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "embed (?, 9, 50)\n",
      "embed (?, 9, 50)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: SGD_primcaps_learn_dropout_base\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 9, 50)             0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 5, 256)            196864    \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             163840    \n",
      "_________________________________________________________________\n",
      "out_pred (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,286,760\n",
      "Trainable params: 1,286,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 13:54:15.670850: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 13:54:15.747786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 13:54:15.748157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 13:54:15.748191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 112s 549us/step - loss: 0.8100 - acc: 0.0564 - val_loss: 0.8100 - val_acc: 0.0558\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.81000, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 110s 538us/step - loss: 0.8100 - acc: 0.0560 - val_loss: 0.8100 - val_acc: 0.0558\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 112s 550us/step - loss: 0.8100 - acc: 0.0566 - val_loss: 0.8100 - val_acc: 0.0558\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 109s 537us/step - loss: 0.8100 - acc: 0.0559 - val_loss: 0.8100 - val_acc: 0.0558\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 111s 546us/step - loss: 0.8100 - acc: 0.0560 - val_loss: 0.8100 - val_acc: 0.0558\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 110s 540us/step - loss: 0.8100 - acc: 0.0568 - val_loss: 0.8100 - val_acc: 0.0558\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 00006: early stopping\n",
      "'trainModelSP'  736870.40 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "embed (?, 9, 95)\n",
      "embed (?, 9, 95)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: SGD_primcaps_learn_dropout_pos\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 95)        0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 95)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       73216       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,321,320\n",
      "Trainable params: 1,321,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 14:06:32.647274: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 14:06:32.725264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 14:06:32.725649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 14:06:32.725718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 110s 543us/step - loss: 0.8100 - acc: 0.1201 - val_loss: 0.8100 - val_acc: 0.1178\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.81000, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 111s 547us/step - loss: 0.8100 - acc: 0.1230 - val_loss: 0.8100 - val_acc: 0.1204\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 111s 545us/step - loss: 0.8100 - acc: 0.1242 - val_loss: 0.8100 - val_acc: 0.1233\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "203621/203621 [==============================] - 112s 549us/step - loss: 0.8100 - acc: 0.1262 - val_loss: 0.8100 - val_acc: 0.1261\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-04.h5\n",
      "Epoch 5/50\n",
      "203621/203621 [==============================] - 111s 545us/step - loss: 0.8100 - acc: 0.1302 - val_loss: 0.8100 - val_acc: 0.1287\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-05.h5\n",
      "Epoch 6/50\n",
      "203621/203621 [==============================] - 112s 548us/step - loss: 0.8100 - acc: 0.1319 - val_loss: 0.8100 - val_acc: 0.1317\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-06.h5\n",
      "Epoch 7/50\n",
      "203621/203621 [==============================] - 112s 549us/step - loss: 0.8100 - acc: 0.1358 - val_loss: 0.8100 - val_acc: 0.1348\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-07.h5\n",
      "Epoch 8/50\n",
      "203621/203621 [==============================] - 112s 548us/step - loss: 0.8100 - acc: 0.1381 - val_loss: 0.8100 - val_acc: 0.1379\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-08.h5\n",
      "Epoch 9/50\n",
      "203621/203621 [==============================] - 112s 551us/step - loss: 0.8100 - acc: 0.1406 - val_loss: 0.8100 - val_acc: 0.1412\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-09.h5\n",
      "Epoch 10/50\n",
      "203621/203621 [==============================] - 112s 551us/step - loss: 0.8100 - acc: 0.1443 - val_loss: 0.8100 - val_acc: 0.1443\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-10.h5\n",
      "Epoch 11/50\n",
      "203621/203621 [==============================] - 110s 541us/step - loss: 0.8100 - acc: 0.1474 - val_loss: 0.8100 - val_acc: 0.1475\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-11.h5\n",
      "Epoch 12/50\n",
      "203621/203621 [==============================] - 112s 551us/step - loss: 0.8100 - acc: 0.1499 - val_loss: 0.8100 - val_acc: 0.1507\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-12.h5\n",
      "Epoch 13/50\n",
      "203621/203621 [==============================] - 112s 551us/step - loss: 0.8100 - acc: 0.1528 - val_loss: 0.8100 - val_acc: 0.1545\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-13.h5\n",
      "Epoch 14/50\n",
      "203621/203621 [==============================] - 111s 547us/step - loss: 0.8100 - acc: 0.1557 - val_loss: 0.8100 - val_acc: 0.1583\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-14.h5\n",
      "Epoch 15/50\n",
      "203621/203621 [==============================] - 111s 547us/step - loss: 0.8100 - acc: 0.1616 - val_loss: 0.8100 - val_acc: 0.1618\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-15.h5\n",
      "Epoch 16/50\n",
      "203621/203621 [==============================] - 111s 547us/step - loss: 0.8100 - acc: 0.1641 - val_loss: 0.8100 - val_acc: 0.1662\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-16.h5\n",
      "Epoch 17/50\n",
      "203621/203621 [==============================] - 111s 546us/step - loss: 0.8100 - acc: 0.1674 - val_loss: 0.8100 - val_acc: 0.1711\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-17.h5\n",
      "Epoch 18/50\n",
      "203621/203621 [==============================] - 111s 545us/step - loss: 0.8100 - acc: 0.1715 - val_loss: 0.8100 - val_acc: 0.1757\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-18.h5\n",
      "Epoch 19/50\n",
      "203621/203621 [==============================] - 113s 553us/step - loss: 0.8100 - acc: 0.1738 - val_loss: 0.8100 - val_acc: 0.1801\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-19.h5\n",
      "Epoch 20/50\n",
      "203621/203621 [==============================] - 112s 548us/step - loss: 0.8100 - acc: 0.1776 - val_loss: 0.8100 - val_acc: 0.1847\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-20.h5\n",
      "Epoch 21/50\n",
      "203621/203621 [==============================] - 111s 544us/step - loss: 0.8100 - acc: 0.1823 - val_loss: 0.8100 - val_acc: 0.1895\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-21.h5\n",
      "Epoch 22/50\n",
      "203621/203621 [==============================] - 111s 545us/step - loss: 0.8100 - acc: 0.1864 - val_loss: 0.8100 - val_acc: 0.1939\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-22.h5\n",
      "Epoch 23/50\n",
      "203621/203621 [==============================] - 111s 547us/step - loss: 0.8100 - acc: 0.1916 - val_loss: 0.8100 - val_acc: 0.1987\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-23.h5\n",
      "Epoch 24/50\n",
      "203621/203621 [==============================] - 111s 544us/step - loss: 0.8100 - acc: 0.1949 - val_loss: 0.8100 - val_acc: 0.2042\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-24.h5\n",
      "Epoch 25/50\n",
      "203621/203621 [==============================] - 112s 548us/step - loss: 0.8100 - acc: 0.2000 - val_loss: 0.8100 - val_acc: 0.2098\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-25.h5\n",
      "Epoch 26/50\n",
      "203621/203621 [==============================] - 111s 546us/step - loss: 0.8100 - acc: 0.2031 - val_loss: 0.8100 - val_acc: 0.2148\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-26.h5\n",
      "Epoch 27/50\n",
      "203621/203621 [==============================] - 112s 549us/step - loss: 0.8100 - acc: 0.2084 - val_loss: 0.8100 - val_acc: 0.2198\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-27.h5\n",
      "Epoch 28/50\n",
      "203621/203621 [==============================] - 112s 550us/step - loss: 0.8100 - acc: 0.2140 - val_loss: 0.8100 - val_acc: 0.2252\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-28.h5\n",
      "Epoch 29/50\n",
      "203621/203621 [==============================] - 111s 545us/step - loss: 0.8100 - acc: 0.2194 - val_loss: 0.8100 - val_acc: 0.2306\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-29.h5\n",
      "Epoch 30/50\n",
      "203621/203621 [==============================] - 112s 549us/step - loss: 0.8100 - acc: 0.2236 - val_loss: 0.8100 - val_acc: 0.2360\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-30.h5\n",
      "Epoch 31/50\n",
      "203621/203621 [==============================] - 111s 546us/step - loss: 0.8100 - acc: 0.2284 - val_loss: 0.8100 - val_acc: 0.2427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-31.h5\n",
      "Epoch 32/50\n",
      "203621/203621 [==============================] - 113s 554us/step - loss: 0.8100 - acc: 0.2346 - val_loss: 0.8100 - val_acc: 0.2486\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-32.h5\n",
      "Epoch 33/50\n",
      "203621/203621 [==============================] - 111s 547us/step - loss: 0.8100 - acc: 0.2389 - val_loss: 0.8100 - val_acc: 0.2541\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-33.h5\n",
      "Epoch 34/50\n",
      " 13100/203621 [>.............................] - ETA: 1:37 - loss: 0.8100 - acc: 0.2380^C\n",
      "Traceback (most recent call last):\n",
      "  File \"trainCapsModel.py\", line 52, in <module>\n",
      "    main()\n",
      "  File \"trainCapsModel.py\", line 48, in main\n",
      "    caps.fit_model( hypers, model, modelName, trainX_dict, devX_list, trainY_cat, devY_cat)\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/buildCapsModel.py\", line 188, in fit_model\n",
      "    verbose=1)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1705, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1235, in _fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2478, in __call__\n",
      "    **self.session_kwargs)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n",
      "    status, run_metadata)\n",
      "KeyboardInterrupt\n",
      "'trainModelSP'  4084935.68 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 55)\n",
      "embed (?, 9, 55)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: SGD_primcaps_learn_dropout_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 55)        0           embedding_1[0][0]                \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 55)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       42496       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,290,600\n",
      "Trainable params: 1,290,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 15:14:37.561023: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 15:14:37.641377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 15:14:37.641699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 15:14:37.641730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.8100 - acc: 0.0553 - val_loss: 0.8100 - val_acc: 0.0517\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.81000, saving model to ./result/weights-01.h5\n",
      "Epoch 2/50\n",
      "203621/203621 [==============================] - 103s 503us/step - loss: 0.8100 - acc: 0.0624 - val_loss: 0.8100 - val_acc: 0.0606\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-02.h5\n",
      "Epoch 3/50\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.8100 - acc: 0.0730 - val_loss: 0.8100 - val_acc: 0.0739\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.81000 to 0.81000, saving model to ./result/weights-03.h5\n",
      "Epoch 4/50\n",
      "134800/203621 [==================>...........] - ETA: 31s - loss: 0.8100 - acc: 0.0863^C\n",
      "Traceback (most recent call last):\n",
      "  File \"trainCapsModel.py\", line 52, in <module>\n",
      "    main()\n",
      "  File \"trainCapsModel.py\", line 48, in main\n",
      "    caps.fit_model( hypers, model, modelName, trainX_dict, devX_list, trainY_cat, devY_cat)\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/buildCapsModel.py\", line 188, in fit_model\n",
      "    verbose=1)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1705, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1235, in _fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2478, in __call__\n",
      "    **self.session_kwargs)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n",
      "    status, run_metadata)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'trainModelSP'  427198.23 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: SGD_primcaps_learn_dropout_pos_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 100)       0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 100)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       77056       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,325,160\n",
      "Trainable params: 1,325,160\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 15:21:44.844179: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 15:21:44.932801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 15:21:44.933111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 15:21:44.933139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/50\n",
      "  1900/203621 [..............................] - ETA: 3:11 - loss: 0.8100 - acc: 0.0689^C\n",
      "Traceback (most recent call last):\n",
      "  File \"trainCapsModel.py\", line 52, in <module>\n",
      "    main()\n",
      "  File \"trainCapsModel.py\", line 48, in main\n",
      "    caps.fit_model( hypers, model, modelName, trainX_dict, devX_list, trainY_cat, devY_cat)\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/buildCapsModel.py\", line 188, in fit_model\n",
      "    verbose=1)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1705, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1235, in _fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2478, in __call__\n",
      "    **self.session_kwargs)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n",
      "    status, run_metadata)\n",
      "KeyboardInterrupt\n",
      "'trainModelSP'  6687.72 ms\n",
      "'testFeatures'  5255692.43 ms\n",
      "\n",
      "\n",
      "Glove Embeddings\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 15:21:50.079396: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 15:21:50.153522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 15:21:50.153824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 15:21:50.153851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "embed (?, 9, 50)\n",
      "embed (?, 9, 50)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: SGD_primcaps_glove_nolearn_base\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 5, 256)            196864    \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             163840    \n",
      "_________________________________________________________________\n",
      "out_pred (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,286,760\n",
      "Trainable params: 399,360\n",
      "Non-trainable params: 887,400\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      " 12100/203621 [>.............................] - ETA: 1:35 - loss: 0.8093 - acc: 0.2561^C\n",
      "Traceback (most recent call last):\n",
      "  File \"trainCapsModel.py\", line 52, in <module>\n",
      "    main()\n",
      "  File \"trainCapsModel.py\", line 48, in main\n",
      "    caps.fit_model( hypers, model, modelName, trainX_dict, devX_list, trainY_cat, devY_cat)\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/buildCapsModel.py\", line 188, in fit_model\n",
      "    verbose=1)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1705, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1235, in _fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2478, in __call__\n",
      "    **self.session_kwargs)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n",
      "    status, run_metadata)\n",
      "KeyboardInterrupt\n",
      "'trainModelSP'  10531.35 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"trainCapsModel.py\", line 4, in <module>\n",
      "    import buildCapsModel as caps\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/buildCapsModel.py\", line 4, in <module>\n",
      "    from keras import callbacks, optimizers\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/utils/__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/utils/conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/__init__.py\", line 84, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\n",
      "    from tensorflow.python import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 82, in <module>\n",
      "    from tensorflow.python import keras\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/__init__.py\", line 26, in <module>\n",
      "    from tensorflow.python.keras import activations\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/activations/__init__.py\", line 22, in <module>\n",
      "    from tensorflow.python.keras._impl.keras.activations import elu\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/__init__.py\", line 35, in <module>\n",
      "    from tensorflow.python.keras._impl.keras import preprocessing\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/preprocessing/__init__.py\", line 21, in <module>\n",
      "    from tensorflow.python.keras._impl.keras.preprocessing import image\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/preprocessing/image.py\", line 43, in <module>\n",
      "    from scipy import linalg\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/scipy/linalg/__init__.py\", line 202, in <module>\n",
      "    from ._decomp_update import *\n",
      "  File \"_decomp_update.pyx\", line 1, in init scipy.linalg._decomp_update\n",
      "KeyboardInterrupt\n",
      "'trainModelSP'  1229.81 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"trainCapsModel.py\", line 4, in <module>\n",
      "    import buildCapsModel as caps\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/buildCapsModel.py\", line 4, in <module>\n",
      "    from keras import callbacks, optimizers\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/utils/__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/utils/conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/__init__.py\", line 84, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\n",
      "    from tensorflow.python import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 82, in <module>\n",
      "    from tensorflow.python import keras\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/__init__.py\", line 26, in <module>\n",
      "    from tensorflow.python.keras import activations\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/activations/__init__.py\", line 22, in <module>\n",
      "    from tensorflow.python.keras._impl.keras.activations import elu\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/__init__.py\", line 21, in <module>\n",
      "    from tensorflow.python.keras._impl.keras import activations\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/activations.py\", line 24, in <module>\n",
      "    from tensorflow.python.keras._impl.keras.engine import Layer\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/__init__.py\", line 26, in <module>\n",
      "    from tensorflow.python.keras._impl.keras.engine.training import Model\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py\", line 27, in <module>\n",
      "    from tensorflow.python.keras._impl.keras import callbacks as cbks\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/callbacks.py\", line 41, in <module>\n",
      "    import requests\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/requests/__init__.py\", line 84, in <module>\n",
      "    from urllib3.contrib import pyopenssl\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py\", line 48, in <module>\n",
      "    from cryptography.hazmat.backends.openssl import backend as openssl_backend\n",
      "  File \"<frozen importlib._bootstrap>\", line 968, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 148, in __enter__\n",
      "  File \"<frozen importlib._bootstrap>\", line 187, in _get_module_lock\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'trainModelSP'  1041.88 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"trainCapsModel.py\", line 5, in <module>\n",
      "    from loadutils import retrieve_model, loadProcessedData\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/loadutils.py\", line 9, in <module>\n",
      "    from common import vocabulary, utils\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/common/utils.py\", line 11, in <module>\n",
      "    from IPython.display import display, HTML\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/IPython/__init__.py\", line 55, in <module>\n",
      "    from .terminal.embed import embed\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/IPython/terminal/embed.py\", line 14, in <module>\n",
      "    from IPython.core.magic import Magics, magics_class, line_magic\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/IPython/core/magic.py\", line 22, in <module>\n",
      "    from IPython.core.inputsplitter import ESC_MAGIC, ESC_MAGIC2\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/IPython/core/inputsplitter.py\", line 62, in <module>\n",
      "    r'^\\s+continue\\s*$', # continue (optionally followed by trailing spaces)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/re.py\", line 233, in compile\n",
      "    return _compile(pattern, flags)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/re.py\", line 301, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/sre_compile.py\", line 562, in compile\n",
      "    p = sre_parse.parse(p, flags)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/sre_parse.py\", line 855, in parse\n",
      "    p = _parse_sub(source, pattern, flags & SRE_FLAG_VERBOSE, 0)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/sre_parse.py\", line 416, in _parse_sub\n",
      "    not nested and not items))\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/sre_parse.py\", line 486, in _parse\n",
      "    if this in \"|)\":\n",
      "KeyboardInterrupt\n",
      "'trainModelSP'  1707.82 ms\n",
      "'testFeatures'  14511.32 ms\n",
      "\n",
      "\n",
      "Glove Embeddings and Dropout\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"trainCapsModel.py\", line 4, in <module>\n",
      "    import buildCapsModel as caps\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/buildCapsModel.py\", line 4, in <module>\n",
      "    from keras import callbacks, optimizers\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/utils/__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/utils/conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/__init__.py\", line 84, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\n",
      "    from tensorflow.python import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 114, in <module>\n",
      "    from tensorflow.python.platform import test\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/platform/test.py\", line 54, in <module>\n",
      "    from tensorflow.python.ops.gradient_checker import compute_gradient_error\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 674, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 779, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 487, in _compile_bytecode\n",
      "KeyboardInterrupt\n",
      "'trainModelSP'  1460.53 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 15:22:06.004244: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 15:22:06.079288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 15:22:06.079621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 15:22:06.079652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "embed (?, 9, 95)\n",
      "embed (?, 9, 95)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: SGD_primcaps_glove_nolearn_dropout_pos\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 95)        0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 95)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       73216       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,321,320\n",
      "Trainable params: 433,920\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      " 10100/203621 [>.............................] - ETA: 1:43 - loss: 0.8091 - acc: 0.1051^C\n",
      "Traceback (most recent call last):\n",
      "  File \"trainCapsModel.py\", line 52, in <module>\n",
      "    main()\n",
      "  File \"trainCapsModel.py\", line 48, in main\n",
      "    caps.fit_model( hypers, model, modelName, trainX_dict, devX_list, trainY_cat, devY_cat)\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/buildCapsModel.py\", line 188, in fit_model\n",
      "    verbose=1)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1705, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1235, in _fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2478, in __call__\n",
      "    **self.session_kwargs)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n",
      "    status, run_metadata)\n",
      "KeyboardInterrupt\n",
      "'trainModelSP'  9941.38 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 15:22:16.004585: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 15:22:16.078102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 15:22:16.078402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 15:22:16.078432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 55)\n",
      "embed (?, 9, 55)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: SGD_primcaps_glove_nolearn_dropout_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 55)        0           embedding_1[0][0]                \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 55)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       42496       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,290,600\n",
      "Trainable params: 403,200\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      "  5300/203621 [..............................] - ETA: 1:59 - loss: 0.8093 - acc: 0.0806^C\n",
      "Traceback (most recent call last):\n",
      "  File \"trainCapsModel.py\", line 52, in <module>\n",
      "    main()\n",
      "  File \"trainCapsModel.py\", line 48, in main\n",
      "    caps.fit_model( hypers, model, modelName, trainX_dict, devX_list, trainY_cat, devY_cat)\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/buildCapsModel.py\", line 188, in fit_model\n",
      "    verbose=1)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1705, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1235, in _fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2478, in __call__\n",
      "    **self.session_kwargs)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n",
      "    status, run_metadata)\n",
      "KeyboardInterrupt\n",
      "'trainModelSP'  7738.94 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 15:22:23.709814: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-12 15:22:23.787572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 15:22:23.787927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 15:22:23.787959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: SGD_primcaps_glove_nolearn_dropout_pos_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 100)       0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 100)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       77056       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,325,160\n",
      "Trainable params: 437,760\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      " 84500/203621 [===========>..................] - ETA: 54s - loss: 0.8080 - acc: 0.5931^C\n",
      "Traceback (most recent call last):\n",
      "  File \"trainCapsModel.py\", line 52, in <module>\n",
      "    main()\n",
      "  File \"trainCapsModel.py\", line 48, in main\n",
      "    caps.fit_model( hypers, model, modelName, trainX_dict, devX_list, trainY_cat, devY_cat)\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/buildCapsModel.py\", line 188, in fit_model\n",
      "    verbose=1)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1705, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1235, in _fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2478, in __call__\n",
      "    **self.session_kwargs)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n",
      "    status, run_metadata)\n",
      "KeyboardInterrupt\n",
      "'trainModelSP'  43521.00 ms\n",
      "'testFeatures'  62662.56 ms\n",
      "\n",
      "\n",
      "Glove Embeddings with Learning\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 15:23:07.343386: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 15:23:07.418247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 15:23:07.418641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 15:23:07.418670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "embed (?, 9, 50)\n",
      "embed (?, 9, 50)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: SGD_primcaps_glove_learn_base\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 5, 256)            196864    \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             163840    \n",
      "_________________________________________________________________\n",
      "out_pred (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,286,760\n",
      "Trainable params: 1,286,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      " 17900/203621 [=>............................] - ETA: 1:32 - loss: 0.8093 - acc: 0.0723^C\n",
      "Traceback (most recent call last):\n",
      "  File \"trainCapsModel.py\", line 52, in <module>\n",
      "    main()\n",
      "  File \"trainCapsModel.py\", line 48, in main\n",
      "    caps.fit_model( hypers, model, modelName, trainX_dict, devX_list, trainY_cat, devY_cat)\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/buildCapsModel.py\", line 188, in fit_model\n",
      "    verbose=1)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1705, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1235, in _fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2478, in __call__\n",
      "    **self.session_kwargs)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n",
      "    status, run_metadata)\n",
      "KeyboardInterrupt\n",
      "'trainModelSP'  13621.50 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"trainCapsModel.py\", line 4, in <module>\n",
      "    import buildCapsModel as caps\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/buildCapsModel.py\", line 4, in <module>\n",
      "    from keras import callbacks, optimizers\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/utils/__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/utils/conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/__init__.py\", line 84, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\n",
      "    from tensorflow.python import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 82, in <module>\n",
      "    from tensorflow.python import keras\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/__init__.py\", line 26, in <module>\n",
      "    from tensorflow.python.keras import activations\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/activations/__init__.py\", line 22, in <module>\n",
      "    from tensorflow.python.keras._impl.keras.activations import elu\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/__init__.py\", line 35, in <module>\n",
      "    from tensorflow.python.keras._impl.keras import preprocessing\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/preprocessing/__init__.py\", line 21, in <module>\n",
      "    from tensorflow.python.keras._impl.keras.preprocessing import image\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/preprocessing/image.py\", line 43, in <module>\n",
      "    from scipy import linalg\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/scipy/__init__.py\", line 110, in <module>\n",
      "    if _NumpyVersion(__numpy_version__) < '1.8.2':\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/scipy/_lib/_version.py\", line 57, in __init__\n",
      "    ver_main = re.match(r'\\d[.]\\d+[.]\\d+', vstring)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/re.py\", line 172, in match\n",
      "    return _compile(pattern, flags).match(string)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/re.py\", line 301, in _compile\n",
      "    p = sre_compile.compile(pattern, flags)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/sre_compile.py\", line 566, in compile\n",
      "    code = _code(p, flags)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/sre_compile.py\", line 551, in _code\n",
      "    _compile(code, p.data, flags)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/sre_compile.py\", line 118, in _compile\n",
      "    elif _simple(av) and op is not REPEAT:\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/sre_compile.py\", line 391, in _simple\n",
      "    return lo == hi == 1 and av[2][0][0] != SUBPATTERN\n",
      "KeyboardInterrupt\n",
      "'trainModelSP'  1168.37 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"trainCapsModel.py\", line 4, in <module>\n",
      "    import buildCapsModel as caps\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/buildCapsModel.py\", line 4, in <module>\n",
      "    from keras import callbacks, optimizers\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/utils/__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/utils/conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/__init__.py\", line 84, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\n",
      "    from tensorflow.python import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 82, in <module>\n",
      "    from tensorflow.python import keras\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/__init__.py\", line 26, in <module>\n",
      "    from tensorflow.python.keras import activations\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/activations/__init__.py\", line 22, in <module>\n",
      "    from tensorflow.python.keras._impl.keras.activations import elu\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/__init__.py\", line 21, in <module>\n",
      "    from tensorflow.python.keras._impl.keras import activations\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/activations.py\", line 23, in <module>\n",
      "    from tensorflow.python.keras._impl.keras import backend as K\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py\", line 54, in <module>\n",
      "    from tensorflow.python.training import moving_averages\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 674, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 771, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 475, in _validate_bytecode_header\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'trainModelSP'  922.77 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"trainCapsModel.py\", line 4, in <module>\n",
      "    import buildCapsModel as caps\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/buildCapsModel.py\", line 4, in <module>\n",
      "    from keras import callbacks, optimizers\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/utils/__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/utils/conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/__init__.py\", line 84, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\n",
      "    from tensorflow.python import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 83, in <module>\n",
      "    from tensorflow.python.estimator import estimator_lib as estimator\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/estimator/estimator_lib.py\", line 35, in <module>\n",
      "    from tensorflow.python.estimator.inputs import inputs\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/inputs.py\", line 22, in <module>\n",
      "    from tensorflow.python.estimator.inputs.numpy_io import numpy_input_fn\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/numpy_io.py\", line 22, in <module>\n",
      "    from tensorflow.python.estimator.inputs.queues import feeding_functions\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py\", line 40, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/pandas/__init__.py\", line 42, in <module>\n",
      "    from pandas.core.api import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/pandas/core/api.py\", line 7, in <module>\n",
      "    from pandas.core.algorithms import factorize, unique, value_counts\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/pandas/core/algorithms.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.cast import maybe_promote\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/pandas/core/dtypes/cast.py\", line 30, in <module>\n",
      "    from .missing import isna, notna\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 674, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 764, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 833, in get_data\n",
      "KeyboardInterrupt\n",
      "'trainModelSP'  1487.00 ms\n",
      "'testFeatures'  17200.24 ms\n",
      "\n",
      "\n",
      "Glove Embeddings with Learning and Dropout\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 15:23:24.536369: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 15:23:24.625584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 15:23:24.626028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 15:23:24.626075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "embed (?, 9, 50)\n",
      "embed (?, 9, 50)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: SGD_primcaps_glove_learn_dropout_base\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 9, 50)             0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 5, 256)            196864    \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             163840    \n",
      "_________________________________________________________________\n",
      "out_pred (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,286,760\n",
      "Trainable params: 1,286,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      " 18900/203621 [=>............................] - ETA: 1:34 - loss: 0.8089 - acc: 0.4064^C\n",
      "Traceback (most recent call last):\n",
      "  File \"trainCapsModel.py\", line 52, in <module>\n",
      "    main()\n",
      "  File \"trainCapsModel.py\", line 48, in main\n",
      "    caps.fit_model( hypers, model, modelName, trainX_dict, devX_list, trainY_cat, devY_cat)\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/buildCapsModel.py\", line 188, in fit_model\n",
      "    verbose=1)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1705, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1235, in _fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2478, in __call__\n",
      "    **self.session_kwargs)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n",
      "    status, run_metadata)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'trainModelSP'  14545.19 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 15:23:39.155158: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "^C\n",
      "2018-04-12 15:23:39.237249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 15:23:39.237594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 15:23:39.237625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "'trainModelSP'  2698.81 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"trainCapsModel.py\", line 4, in <module>\n",
      "    import buildCapsModel as caps\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/buildCapsModel.py\", line 4, in <module>\n",
      "    from keras import callbacks, optimizers\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/utils/__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/utils/conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/__init__.py\", line 84, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\n",
      "    from tensorflow.python import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 82, in <module>\n",
      "    from tensorflow.python import keras\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/__init__.py\", line 26, in <module>\n",
      "    from tensorflow.python.keras import activations\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/activations/__init__.py\", line 22, in <module>\n",
      "    from tensorflow.python.keras._impl.keras.activations import elu\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/__init__.py\", line 35, in <module>\n",
      "    from tensorflow.python.keras._impl.keras import preprocessing\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/preprocessing/__init__.py\", line 21, in <module>\n",
      "    from tensorflow.python.keras._impl.keras.preprocessing import image\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/preprocessing/image.py\", line 44, in <module>\n",
      "    import scipy.ndimage as ndi\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/scipy/ndimage/__init__.py\", line 161, in <module>\n",
      "    from .filters import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/scipy/ndimage/filters.py\", line 37, in <module>\n",
      "    from scipy.misc import doccer\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/scipy/misc/__init__.py\", line 67, in <module>\n",
      "    from scipy.interpolate._pade import pade as _pade\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/scipy/interpolate/__init__.py\", line 175, in <module>\n",
      "    from .interpolate import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/scipy/interpolate/interpolate.py\", line 21, in <module>\n",
      "    import scipy.special as spec\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/scipy/special/__init__.py\", line 648, in <module>\n",
      "    from ._ellip_harm import ellip_harm, ellip_harm_2, ellip_normal\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/scipy/special/_ellip_harm.py\", line 7, in <module>\n",
      "    from ._ellip_harm_2 import _ellipsoid, _ellipsoid_norm\n",
      "  File \"_ellip_harm_2.pyx\", line 7, in init scipy.special._ellip_harm_2\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/scipy/integrate/__init__.py\", line 93, in <module>\n",
      "    from ._bvp import solve_bvp\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/scipy/integrate/_bvp.py\", line 10, in <module>\n",
      "    from scipy.sparse.linalg import splu\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/scipy/sparse/linalg/__init__.py\", line 117, in <module>\n",
      "    from .eigen import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/scipy/sparse/linalg/eigen/__init__.py\", line 11, in <module>\n",
      "    from .arpack import *\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/scipy/sparse/linalg/eigen/arpack/__init__.py\", line 22, in <module>\n",
      "    from .arpack import *\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 674, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 779, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 487, in _compile_bytecode\n",
      "KeyboardInterrupt\n",
      "'trainModelSP'  1398.83 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 15:23:43.174237: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 15:23:43.248687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 15:23:43.249036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 15:23:43.249068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: SGD_primcaps_glove_learn_dropout_pos_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 100)       0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 100)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       77056       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,325,160\n",
      "Trainable params: 1,325,160\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/50\n",
      " 15200/203621 [=>............................] - ETA: 1:43 - loss: 0.8092 - acc: 0.0868^C\n",
      "Traceback (most recent call last):\n",
      "  File \"trainCapsModel.py\", line 52, in <module>\n",
      "    main()\n",
      "  File \"trainCapsModel.py\", line 48, in main\n",
      "    caps.fit_model( hypers, model, modelName, trainX_dict, devX_list, trainY_cat, devY_cat)\n",
      "  File \"/home/dhuber/CapsNet_for_NER/code/buildCapsModel.py\", line 188, in fit_model\n",
      "    verbose=1)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1705, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/engine/training.py\", line 1235, in _fit_loop\n",
      "    outs = f(ins_batch)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2478, in __call__\n",
      "    **self.session_kwargs)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1059, in _run\n",
      "    feed_dict = nest.flatten_dict_items(feed_dict)\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 261, in flatten_dict_items\n",
      "    if i in flat_dictionary:\n",
      "  File \"/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 473, in __hash__\n",
      "    def __hash__(self):\n",
      "KeyboardInterrupt\n",
      "'trainModelSP'  13217.38 ms\n",
      "'testFeatures'  31860.65 ms\n"
     ]
    }
   ],
   "source": [
    "# train a caps model with 2D Primary caps and repeat tests\n",
    "\n",
    "# capsnet training function\n",
    "testFunc = \"trainCapsModel.py\"\n",
    "\n",
    "hypers = hyper_param_caps.copy()\n",
    "hypers['optimizer'] = \"SGD\"\n",
    "print(\"Training with SGD - Nesterov Momentum Optimizer\")\n",
    "\n",
    "hypers['epochs'] = 50\n",
    "hypers['stopping_patience'] = 5\n",
    "hypers['use_pos_tags'] = False\n",
    "hypers['use_capitalization_info'] = False\n",
    "\n",
    "# try different embeddings\n",
    "# learn embeddings\n",
    "print(\"\\n\\nLearn Embeddings\")\n",
    "hypers['use_glove'] = False\n",
    "hypers['embed_dropout'] = 0.0\n",
    "testFeatures( testFunc, \"SGD_primcaps_learn\", hypers)\n",
    "\n",
    "# learn embeddings + Dropout\n",
    "print(\"\\n\\nLearn Embeddings and Dropout\")\n",
    "hypers['use_glove'] = False\n",
    "hypers['embed_dropout'] = 0.25\n",
    "testFeatures( testFunc, \"SGD_primcaps_learn_dropout\", hypers)\n",
    "\n",
    "# use glove, no learn\n",
    "print(\"\\n\\nGlove Embeddings\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = False\n",
    "hypers['embed_dropout'] = 0.0\n",
    "testFeatures( testFunc, \"SGD_primcaps_glove_nolearn\", hypers)\n",
    "\n",
    "# use glove, no learn + Dropout\n",
    "print(\"\\n\\nGlove Embeddings and Dropout\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = False\n",
    "hypers['embed_dropout'] = 0.25\n",
    "testFeatures( testFunc, \"SGD_primcaps_glove_nolearn_dropout\", hypers)\n",
    "\n",
    "# use glove, learn\n",
    "print(\"\\n\\nGlove Embeddings with Learning\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = True\n",
    "hypers['embed_dropout'] = 0.0\n",
    "testFeatures( testFunc, \"SGD_primcaps_glove_learn\", hypers)\n",
    "\n",
    "# use glove, learn + Dropout\n",
    "print(\"\\n\\nGlove Embeddings with Learning and Dropout\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = True\n",
    "hypers['embed_dropout'] = 0.25\n",
    "testFeatures( testFunc, \"SGD_primcaps_glove_learn_dropout\", hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Adam\n",
      "\n",
      "\n",
      "Learn Embeddings and Dropout at 50%\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "embed (?, 9, 50)\n",
      "embed (?, 9, 50)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_dropout_05_base\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 9, 50)             0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 5, 256)            196864    \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             163840    \n",
      "_________________________________________________________________\n",
      "out_pred (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,286,760\n",
      "Trainable params: 1,286,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 15:28:06.174496: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 15:28:06.261822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 15:28:06.262206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 15:28:06.262237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/60\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0994 - acc: 0.8966 - val_loss: 0.0358 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03584, saving model to ./result/weights-01.h5\n",
      "Epoch 2/60\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0220 - acc: 0.9715 - val_loss: 0.0300 - val_acc: 0.9602\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03584 to 0.03000, saving model to ./result/weights-02.h5\n",
      "Epoch 3/60\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0141 - acc: 0.9816 - val_loss: 0.0323 - val_acc: 0.9584\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/60\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0106 - acc: 0.9863 - val_loss: 0.0278 - val_acc: 0.9651\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03000 to 0.02779, saving model to ./result/weights-04.h5\n",
      "Epoch 5/60\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0086 - acc: 0.9889 - val_loss: 0.0282 - val_acc: 0.9650\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/60\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0073 - acc: 0.9907 - val_loss: 0.0272 - val_acc: 0.9659\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02779 to 0.02720, saving model to ./result/weights-06.h5\n",
      "Epoch 7/60\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0064 - acc: 0.9917 - val_loss: 0.0277 - val_acc: 0.9659\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/60\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0060 - acc: 0.9924 - val_loss: 0.0289 - val_acc: 0.9645\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/60\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0053 - acc: 0.9931 - val_loss: 0.0274 - val_acc: 0.9669\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/60\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0047 - acc: 0.9939 - val_loss: 0.0290 - val_acc: 0.9660\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/60\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0045 - acc: 0.9942 - val_loss: 0.0275 - val_acc: 0.9663\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/60\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0043 - acc: 0.9943 - val_loss: 0.0286 - val_acc: 0.9658\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/60\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0040 - acc: 0.9948 - val_loss: 0.0276 - val_acc: 0.9676\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/60\n",
      "203621/203621 [==============================] - 107s 523us/step - loss: 0.0039 - acc: 0.9951 - val_loss: 0.0285 - val_acc: 0.9660\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/60\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0036 - acc: 0.9955 - val_loss: 0.0291 - val_acc: 0.9662\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/60\n",
      "203621/203621 [==============================] - 106s 523us/step - loss: 0.0034 - acc: 0.9955 - val_loss: 0.0297 - val_acc: 0.9654\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 00016: early stopping\n",
      "'trainModelSP'  1895817.60 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "embed (?, 9, 95)\n",
      "embed (?, 9, 95)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_dropout_05_pos\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 95)        0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 95)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       73216       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,321,320\n",
      "Trainable params: 1,321,320\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 15:59:41.924338: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 15:59:42.000484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 15:59:42.000827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 15:59:42.000904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/60\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0665 - acc: 0.9200 - val_loss: 0.0255 - val_acc: 0.9670\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02550, saving model to ./result/weights-01.h5\n",
      "Epoch 2/60\n",
      "203621/203621 [==============================] - 106s 523us/step - loss: 0.0191 - acc: 0.9754 - val_loss: 0.0212 - val_acc: 0.9727\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02550 to 0.02119, saving model to ./result/weights-02.h5\n",
      "Epoch 3/60\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.0124 - acc: 0.9843 - val_loss: 0.0215 - val_acc: 0.9730\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/60\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0096 - acc: 0.9878 - val_loss: 0.0218 - val_acc: 0.9728\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/60\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.0081 - acc: 0.9897 - val_loss: 0.0210 - val_acc: 0.9747\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02119 to 0.02103, saving model to ./result/weights-05.h5\n",
      "Epoch 6/60\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0066 - acc: 0.9915 - val_loss: 0.0217 - val_acc: 0.9738\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/60\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0059 - acc: 0.9925 - val_loss: 0.0233 - val_acc: 0.9713\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/60\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0053 - acc: 0.9931 - val_loss: 0.0229 - val_acc: 0.9730\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/60\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0046 - acc: 0.9942 - val_loss: 0.0247 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0043 - acc: 0.9946 - val_loss: 0.0239 - val_acc: 0.9711\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/60\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0040 - acc: 0.9951 - val_loss: 0.0226 - val_acc: 0.9732\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/60\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.0038 - acc: 0.9950 - val_loss: 0.0239 - val_acc: 0.9716\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/60\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.0035 - acc: 0.9954 - val_loss: 0.0231 - val_acc: 0.9737\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/60\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.0032 - acc: 0.9960 - val_loss: 0.0227 - val_acc: 0.9726\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/60\n",
      "203621/203621 [==============================] - 106s 523us/step - loss: 0.0031 - acc: 0.9961 - val_loss: 0.0242 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 00015: early stopping\n",
      "'trainModelSP'  1782811.62 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 55)\n",
      "embed (?, 9, 55)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_dropout_05_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 55)        0           embedding_1[0][0]                \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 55)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       42496       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,290,600\n",
      "Trainable params: 1,290,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 16:29:25.004078: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 16:29:25.080430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 16:29:25.080804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 16:29:25.080837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/60\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0648 - acc: 0.9208 - val_loss: 0.0241 - val_acc: 0.9687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02413, saving model to ./result/weights-01.h5\n",
      "Epoch 2/60\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0179 - acc: 0.9768 - val_loss: 0.0208 - val_acc: 0.9739\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02413 to 0.02080, saving model to ./result/weights-02.h5\n",
      "Epoch 3/60\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0117 - acc: 0.9849 - val_loss: 0.0220 - val_acc: 0.9717\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/60\n",
      "203621/203621 [==============================] - 107s 524us/step - loss: 0.0089 - acc: 0.9887 - val_loss: 0.0198 - val_acc: 0.9758\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02080 to 0.01979, saving model to ./result/weights-04.h5\n",
      "Epoch 5/60\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0075 - acc: 0.9902 - val_loss: 0.0195 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01979 to 0.01951, saving model to ./result/weights-05.h5\n",
      "Epoch 6/60\n",
      "203621/203621 [==============================] - 107s 525us/step - loss: 0.0063 - acc: 0.9919 - val_loss: 0.0184 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01951 to 0.01843, saving model to ./result/weights-06.h5\n",
      "Epoch 7/60\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.0057 - acc: 0.9928 - val_loss: 0.0216 - val_acc: 0.9732\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/60\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.0049 - acc: 0.9938 - val_loss: 0.0231 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/60\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0046 - acc: 0.9940 - val_loss: 0.0218 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/60\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0042 - acc: 0.9946 - val_loss: 0.0217 - val_acc: 0.9732\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/60\n",
      "203621/203621 [==============================] - 106s 522us/step - loss: 0.0039 - acc: 0.9949 - val_loss: 0.0204 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/60\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0038 - acc: 0.9952 - val_loss: 0.0204 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/60\n",
      "203621/203621 [==============================] - 107s 523us/step - loss: 0.0035 - acc: 0.9955 - val_loss: 0.0215 - val_acc: 0.9740\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/60\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0032 - acc: 0.9958 - val_loss: 0.0220 - val_acc: 0.9736\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/60\n",
      "203621/203621 [==============================] - 106s 521us/step - loss: 0.0031 - acc: 0.9960 - val_loss: 0.0204 - val_acc: 0.9748\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/60\n",
      "203621/203621 [==============================] - 107s 527us/step - loss: 0.0028 - acc: 0.9965 - val_loss: 0.0205 - val_acc: 0.9755\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 00016: early stopping\n",
      "'trainModelSP'  1897720.54 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: learn_dropout_05_pos_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 100)       0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 100)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       77056       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,325,160\n",
      "Trainable params: 1,325,160\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "2018-04-12 17:01:02.634772: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 17:01:02.707599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 17:01:02.707981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 17:01:02.708009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Epoch 1/60\n",
      "203621/203621 [==============================] - 114s 558us/step - loss: 0.0554 - acc: 0.9312 - val_loss: 0.0211 - val_acc: 0.9729\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02111, saving model to ./result/weights-01.h5\n",
      "Epoch 2/60\n",
      "203621/203621 [==============================] - 114s 559us/step - loss: 0.0168 - acc: 0.9785 - val_loss: 0.0196 - val_acc: 0.9740\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02111 to 0.01958, saving model to ./result/weights-02.h5\n",
      "Epoch 3/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621/203621 [==============================] - 115s 563us/step - loss: 0.0107 - acc: 0.9863 - val_loss: 0.0186 - val_acc: 0.9765\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01958 to 0.01862, saving model to ./result/weights-03.h5\n",
      "Epoch 4/60\n",
      "203621/203621 [==============================] - 114s 560us/step - loss: 0.0081 - acc: 0.9897 - val_loss: 0.0173 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01862 to 0.01729, saving model to ./result/weights-04.h5\n",
      "Epoch 5/60\n",
      "203621/203621 [==============================] - 114s 561us/step - loss: 0.0069 - acc: 0.9913 - val_loss: 0.0180 - val_acc: 0.9782\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/60\n",
      "203621/203621 [==============================] - 112s 552us/step - loss: 0.0060 - acc: 0.9924 - val_loss: 0.0195 - val_acc: 0.9758\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/60\n",
      "203621/203621 [==============================] - 114s 561us/step - loss: 0.0051 - acc: 0.9935 - val_loss: 0.0186 - val_acc: 0.9783\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/60\n",
      "203621/203621 [==============================] - 114s 558us/step - loss: 0.0046 - acc: 0.9942 - val_loss: 0.0200 - val_acc: 0.9761\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/60\n",
      "203621/203621 [==============================] - 114s 558us/step - loss: 0.0043 - acc: 0.9945 - val_loss: 0.0185 - val_acc: 0.9772\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/60\n",
      "203621/203621 [==============================] - 112s 552us/step - loss: 0.0039 - acc: 0.9950 - val_loss: 0.0196 - val_acc: 0.9765\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/60\n",
      "203621/203621 [==============================] - 113s 557us/step - loss: 0.0034 - acc: 0.9958 - val_loss: 0.0195 - val_acc: 0.9768\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/60\n",
      "203621/203621 [==============================] - 114s 559us/step - loss: 0.0032 - acc: 0.9959 - val_loss: 0.0196 - val_acc: 0.9769\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/60\n",
      "203621/203621 [==============================] - 115s 564us/step - loss: 0.0031 - acc: 0.9960 - val_loss: 0.0191 - val_acc: 0.9774\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/60\n",
      "203621/203621 [==============================] - 114s 562us/step - loss: 0.0028 - acc: 0.9964 - val_loss: 0.0202 - val_acc: 0.9754\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 00014: early stopping\n",
      "'trainModelSP'  1768037.34 ms\n",
      "'testFeatures'  7344387.52 ms\n",
      "\n",
      "\n",
      "Glove Embeddings and Dropout at 50%\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 17:30:29.233043: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 17:30:29.308767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 17:30:29.309148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 17:30:29.309175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "embed (?, 9, 50)\n",
      "embed (?, 9, 50)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_nolearn_dropout_05_base\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 9, 50)             0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 5, 256)            196864    \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             163840    \n",
      "_________________________________________________________________\n",
      "out_pred (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,286,760\n",
      "Trainable params: 399,360\n",
      "Non-trainable params: 887,400\n",
      "_________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/60\n",
      "203621/203621 [==============================] - 102s 501us/step - loss: 0.0753 - acc: 0.9045 - val_loss: 0.0457 - val_acc: 0.9412\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04569, saving model to ./result/weights-01.h5\n",
      "Epoch 2/60\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0521 - acc: 0.9310 - val_loss: 0.0434 - val_acc: 0.9421\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04569 to 0.04341, saving model to ./result/weights-02.h5\n",
      "Epoch 3/60\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.0475 - acc: 0.9367 - val_loss: 0.0393 - val_acc: 0.9490\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04341 to 0.03934, saving model to ./result/weights-03.h5\n",
      "Epoch 4/60\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.0441 - acc: 0.9411 - val_loss: 0.0376 - val_acc: 0.9509\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03934 to 0.03765, saving model to ./result/weights-04.h5\n",
      "Epoch 5/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0423 - acc: 0.9433 - val_loss: 0.0383 - val_acc: 0.9514\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/60\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.0407 - acc: 0.9452 - val_loss: 0.0359 - val_acc: 0.9536\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03765 to 0.03591, saving model to ./result/weights-06.h5\n",
      "Epoch 7/60\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0395 - acc: 0.9471 - val_loss: 0.0365 - val_acc: 0.9531\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/60\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.0380 - acc: 0.9488 - val_loss: 0.0362 - val_acc: 0.9537\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/60\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0373 - acc: 0.9500 - val_loss: 0.0355 - val_acc: 0.9545\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03591 to 0.03554, saving model to ./result/weights-09.h5\n",
      "Epoch 10/60\n",
      "203621/203621 [==============================] - 102s 503us/step - loss: 0.0361 - acc: 0.9516 - val_loss: 0.0344 - val_acc: 0.9561\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.03554 to 0.03436, saving model to ./result/weights-10.h5\n",
      "Epoch 11/60\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.0355 - acc: 0.9524 - val_loss: 0.0354 - val_acc: 0.9547\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/60\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.0348 - acc: 0.9534 - val_loss: 0.0334 - val_acc: 0.9572\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03436 to 0.03335, saving model to ./result/weights-12.h5\n",
      "Epoch 13/60\n",
      "203621/203621 [==============================] - 103s 506us/step - loss: 0.0335 - acc: 0.9554 - val_loss: 0.0333 - val_acc: 0.9563\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03335 to 0.03331, saving model to ./result/weights-13.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/60\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.0335 - acc: 0.9552 - val_loss: 0.0344 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/60\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.0325 - acc: 0.9563 - val_loss: 0.0328 - val_acc: 0.9591\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.03331 to 0.03279, saving model to ./result/weights-15.h5\n",
      "Epoch 16/60\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.0320 - acc: 0.9574 - val_loss: 0.0341 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/60\n",
      "203621/203621 [==============================] - 105s 513us/step - loss: 0.0319 - acc: 0.9572 - val_loss: 0.0351 - val_acc: 0.9565\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/60\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.0311 - acc: 0.9587 - val_loss: 0.0348 - val_acc: 0.9563\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/60\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0306 - acc: 0.9592 - val_loss: 0.0337 - val_acc: 0.9578\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/60\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.0303 - acc: 0.9597 - val_loss: 0.0335 - val_acc: 0.9578\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/60\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.0299 - acc: 0.9599 - val_loss: 0.0319 - val_acc: 0.9595\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.03279 to 0.03192, saving model to ./result/weights-21.h5\n",
      "Epoch 22/60\n",
      "203621/203621 [==============================] - 105s 516us/step - loss: 0.0297 - acc: 0.9601 - val_loss: 0.0333 - val_acc: 0.9585\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/60\n",
      "203621/203621 [==============================] - 103s 506us/step - loss: 0.0291 - acc: 0.9612 - val_loss: 0.0329 - val_acc: 0.9581\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/60\n",
      "203621/203621 [==============================] - 101s 495us/step - loss: 0.0288 - acc: 0.9616 - val_loss: 0.0318 - val_acc: 0.9594\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.03192 to 0.03176, saving model to ./result/weights-24.h5\n",
      "Epoch 25/60\n",
      "203621/203621 [==============================] - 103s 507us/step - loss: 0.0285 - acc: 0.9621 - val_loss: 0.0328 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/60\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0283 - acc: 0.9624 - val_loss: 0.0330 - val_acc: 0.9584\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/60\n",
      "203621/203621 [==============================] - 103s 508us/step - loss: 0.0282 - acc: 0.9625 - val_loss: 0.0315 - val_acc: 0.9599\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.03176 to 0.03154, saving model to ./result/weights-27.h5\n",
      "Epoch 28/60\n",
      "203621/203621 [==============================] - 103s 507us/step - loss: 0.0277 - acc: 0.9630 - val_loss: 0.0326 - val_acc: 0.9587\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/60\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.0277 - acc: 0.9631 - val_loss: 0.0325 - val_acc: 0.9593\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/60\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.0275 - acc: 0.9633 - val_loss: 0.0317 - val_acc: 0.9603\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/60\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.0271 - acc: 0.9641 - val_loss: 0.0321 - val_acc: 0.9595\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/60\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.0268 - acc: 0.9641 - val_loss: 0.0315 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.03154 to 0.03147, saving model to ./result/weights-32.h5\n",
      "Epoch 33/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0265 - acc: 0.9645 - val_loss: 0.0330 - val_acc: 0.9586\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/60\n",
      "203621/203621 [==============================] - 105s 513us/step - loss: 0.0262 - acc: 0.9652 - val_loss: 0.0337 - val_acc: 0.9584\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/60\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.0261 - acc: 0.9651 - val_loss: 0.0311 - val_acc: 0.9609\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.03147 to 0.03106, saving model to ./result/weights-35.h5\n",
      "Epoch 36/60\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.0258 - acc: 0.9653 - val_loss: 0.0311 - val_acc: 0.9609\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/60\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.0255 - acc: 0.9662 - val_loss: 0.0321 - val_acc: 0.9593\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/60\n",
      "203621/203621 [==============================] - 103s 504us/step - loss: 0.0255 - acc: 0.9659 - val_loss: 0.0303 - val_acc: 0.9619\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.03106 to 0.03031, saving model to ./result/weights-38.h5\n",
      "Epoch 39/60\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0254 - acc: 0.9664 - val_loss: 0.0308 - val_acc: 0.9624\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/60\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.0252 - acc: 0.9663 - val_loss: 0.0321 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/60\n",
      "203621/203621 [==============================] - 103s 505us/step - loss: 0.0248 - acc: 0.9669 - val_loss: 0.0319 - val_acc: 0.9605\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/60\n",
      "203621/203621 [==============================] - 102s 501us/step - loss: 0.0248 - acc: 0.9671 - val_loss: 0.0312 - val_acc: 0.9612\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0246 - acc: 0.9673 - val_loss: 0.0310 - val_acc: 0.9608\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/60\n",
      "203621/203621 [==============================] - 103s 508us/step - loss: 0.0245 - acc: 0.9672 - val_loss: 0.0315 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/60\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.0243 - acc: 0.9677 - val_loss: 0.0320 - val_acc: 0.9601\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/60\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.0242 - acc: 0.9678 - val_loss: 0.0329 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/60\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.0239 - acc: 0.9682 - val_loss: 0.0318 - val_acc: 0.9605\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/60\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.0238 - acc: 0.9686 - val_loss: 0.0317 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 00048: early stopping\n",
      "'trainModelSP'  5545103.11 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 19:02:54.367458: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 19:02:54.443105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 19:02:54.443498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 19:02:54.443530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "embed (?, 9, 95)\n",
      "embed (?, 9, 95)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_nolearn_dropout_05_pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 95)        0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 95)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       73216       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,321,320\n",
      "Trainable params: 433,920\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/60\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.0637 - acc: 0.9195 - val_loss: 0.0302 - val_acc: 0.9614\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03018, saving model to ./result/weights-01.h5\n",
      "Epoch 2/60\n",
      "203621/203621 [==============================] - 107s 523us/step - loss: 0.0421 - acc: 0.9444 - val_loss: 0.0276 - val_acc: 0.9650\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03018 to 0.02762, saving model to ./result/weights-02.h5\n",
      "Epoch 3/60\n",
      "203621/203621 [==============================] - 106s 518us/step - loss: 0.0382 - acc: 0.9493 - val_loss: 0.0264 - val_acc: 0.9657\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02762 to 0.02639, saving model to ./result/weights-03.h5\n",
      "Epoch 4/60\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.0355 - acc: 0.9525 - val_loss: 0.0240 - val_acc: 0.9695\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02639 to 0.02403, saving model to ./result/weights-04.h5\n",
      "Epoch 5/60\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0336 - acc: 0.9550 - val_loss: 0.0241 - val_acc: 0.9685\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/60\n",
      "203621/203621 [==============================] - 106s 518us/step - loss: 0.0324 - acc: 0.9565 - val_loss: 0.0222 - val_acc: 0.9707\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02403 to 0.02220, saving model to ./result/weights-06.h5\n",
      "Epoch 7/60\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0307 - acc: 0.9587 - val_loss: 0.0231 - val_acc: 0.9711\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/60\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.0295 - acc: 0.9606 - val_loss: 0.0217 - val_acc: 0.9718\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02220 to 0.02169, saving model to ./result/weights-08.h5\n",
      "Epoch 9/60\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.0292 - acc: 0.9609 - val_loss: 0.0217 - val_acc: 0.9720\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02169 to 0.02169, saving model to ./result/weights-09.h5\n",
      "Epoch 10/60\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.0284 - acc: 0.9617 - val_loss: 0.0199 - val_acc: 0.9745\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02169 to 0.01986, saving model to ./result/weights-10.h5\n",
      "Epoch 11/60\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.0271 - acc: 0.9640 - val_loss: 0.0205 - val_acc: 0.9731\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/60\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0268 - acc: 0.9646 - val_loss: 0.0203 - val_acc: 0.9737\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/60\n",
      "203621/203621 [==============================] - 105s 514us/step - loss: 0.0262 - acc: 0.9651 - val_loss: 0.0200 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/60\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.0259 - acc: 0.9657 - val_loss: 0.0217 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/60\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0253 - acc: 0.9663 - val_loss: 0.0204 - val_acc: 0.9741\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/60\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.0250 - acc: 0.9666 - val_loss: 0.0207 - val_acc: 0.9732\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0246 - acc: 0.9675 - val_loss: 0.0207 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/60\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.0242 - acc: 0.9682 - val_loss: 0.0191 - val_acc: 0.9756\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01986 to 0.01907, saving model to ./result/weights-18.h5\n",
      "Epoch 19/60\n",
      "203621/203621 [==============================] - 105s 517us/step - loss: 0.0238 - acc: 0.9683 - val_loss: 0.0200 - val_acc: 0.9741\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/60\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0234 - acc: 0.9690 - val_loss: 0.0211 - val_acc: 0.9730\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/60\n",
      "203621/203621 [==============================] - 105s 518us/step - loss: 0.0233 - acc: 0.9690 - val_loss: 0.0212 - val_acc: 0.9729\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/60\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0229 - acc: 0.9697 - val_loss: 0.0203 - val_acc: 0.9740\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/60\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0223 - acc: 0.9702 - val_loss: 0.0195 - val_acc: 0.9751\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/60\n",
      "203621/203621 [==============================] - 106s 520us/step - loss: 0.0225 - acc: 0.9701 - val_loss: 0.0208 - val_acc: 0.9734\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/60\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0221 - acc: 0.9706 - val_loss: 0.0192 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/60\n",
      "203621/203621 [==============================] - 105s 515us/step - loss: 0.0218 - acc: 0.9712 - val_loss: 0.0193 - val_acc: 0.9752\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/60\n",
      "203621/203621 [==============================] - 106s 519us/step - loss: 0.0215 - acc: 0.9715 - val_loss: 0.0206 - val_acc: 0.9737\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.0215 - acc: 0.9718 - val_loss: 0.0204 - val_acc: 0.9741\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 00028: early stopping\n",
      "'trainModelSP'  3278370.43 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 19:57:32.653584: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 19:57:32.727860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 19:57:32.728203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 19:57:32.728242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 55)\n",
      "embed (?, 9, 55)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_nolearn_dropout_05_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 55)        0           embedding_1[0][0]                \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 55)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       42496       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,290,600\n",
      "Trainable params: 403,200\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/60\n",
      "203621/203621 [==============================] - 101s 495us/step - loss: 0.0630 - acc: 0.9207 - val_loss: 0.0264 - val_acc: 0.9666\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02643, saving model to ./result/weights-01.h5\n",
      "Epoch 2/60\n",
      "203621/203621 [==============================] - 103s 505us/step - loss: 0.0399 - acc: 0.9470 - val_loss: 0.0231 - val_acc: 0.9704\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02643 to 0.02306, saving model to ./result/weights-02.h5\n",
      "Epoch 3/60\n",
      "203621/203621 [==============================] - 103s 504us/step - loss: 0.0361 - acc: 0.9515 - val_loss: 0.0211 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02306 to 0.02110, saving model to ./result/weights-03.h5\n",
      "Epoch 4/60\n",
      "203621/203621 [==============================] - 103s 506us/step - loss: 0.0338 - acc: 0.9550 - val_loss: 0.0203 - val_acc: 0.9737\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02110 to 0.02027, saving model to ./result/weights-04.h5\n",
      "Epoch 5/60\n",
      "203621/203621 [==============================] - 103s 508us/step - loss: 0.0317 - acc: 0.9573 - val_loss: 0.0207 - val_acc: 0.9737\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0306 - acc: 0.9593 - val_loss: 0.0190 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02027 to 0.01898, saving model to ./result/weights-06.h5\n",
      "Epoch 7/60\n",
      "203621/203621 [==============================] - 103s 505us/step - loss: 0.0292 - acc: 0.9610 - val_loss: 0.0193 - val_acc: 0.9761\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0285 - acc: 0.9616 - val_loss: 0.0188 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01898 to 0.01880, saving model to ./result/weights-08.h5\n",
      "Epoch 9/60\n",
      "203621/203621 [==============================] - 103s 507us/step - loss: 0.0278 - acc: 0.9625 - val_loss: 0.0194 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0267 - acc: 0.9643 - val_loss: 0.0176 - val_acc: 0.9774\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01880 to 0.01763, saving model to ./result/weights-10.h5\n",
      "Epoch 11/60\n",
      "203621/203621 [==============================] - 103s 508us/step - loss: 0.0262 - acc: 0.9650 - val_loss: 0.0179 - val_acc: 0.9775\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/60\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0255 - acc: 0.9661 - val_loss: 0.0177 - val_acc: 0.9773\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/60\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0247 - acc: 0.9672 - val_loss: 0.0169 - val_acc: 0.9782\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01763 to 0.01690, saving model to ./result/weights-13.h5\n",
      "Epoch 14/60\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.0246 - acc: 0.9675 - val_loss: 0.0172 - val_acc: 0.9775\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/60\n",
      "203621/203621 [==============================] - 103s 508us/step - loss: 0.0242 - acc: 0.9677 - val_loss: 0.0168 - val_acc: 0.9784\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01690 to 0.01683, saving model to ./result/weights-15.h5\n",
      "Epoch 16/60\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.0235 - acc: 0.9690 - val_loss: 0.0176 - val_acc: 0.9775\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/60\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0233 - acc: 0.9689 - val_loss: 0.0176 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/60\n",
      "203621/203621 [==============================] - 103s 507us/step - loss: 0.0231 - acc: 0.9694 - val_loss: 0.0163 - val_acc: 0.9785\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01683 to 0.01635, saving model to ./result/weights-18.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0224 - acc: 0.9702 - val_loss: 0.0164 - val_acc: 0.9793\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/60\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.0223 - acc: 0.9706 - val_loss: 0.0168 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/60\n",
      "203621/203621 [==============================] - 103s 505us/step - loss: 0.0223 - acc: 0.9701 - val_loss: 0.0180 - val_acc: 0.9774\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/60\n",
      "203621/203621 [==============================] - 102s 501us/step - loss: 0.0219 - acc: 0.9709 - val_loss: 0.0171 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/60\n",
      "203621/203621 [==============================] - 103s 508us/step - loss: 0.0217 - acc: 0.9713 - val_loss: 0.0159 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01635 to 0.01590, saving model to ./result/weights-23.h5\n",
      "Epoch 24/60\n",
      "203621/203621 [==============================] - 103s 505us/step - loss: 0.0212 - acc: 0.9717 - val_loss: 0.0175 - val_acc: 0.9769\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0209 - acc: 0.9719 - val_loss: 0.0158 - val_acc: 0.9796\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01590 to 0.01583, saving model to ./result/weights-25.h5\n",
      "Epoch 26/60\n",
      "203621/203621 [==============================] - 103s 505us/step - loss: 0.0207 - acc: 0.9727 - val_loss: 0.0161 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0205 - acc: 0.9727 - val_loss: 0.0171 - val_acc: 0.9783\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/60\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0205 - acc: 0.9726 - val_loss: 0.0156 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01583 to 0.01559, saving model to ./result/weights-28.h5\n",
      "Epoch 29/60\n",
      "203621/203621 [==============================] - 103s 505us/step - loss: 0.0204 - acc: 0.9729 - val_loss: 0.0162 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/60\n",
      "203621/203621 [==============================] - 103s 506us/step - loss: 0.0202 - acc: 0.9730 - val_loss: 0.0156 - val_acc: 0.9806\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/60\n",
      "203621/203621 [==============================] - 102s 500us/step - loss: 0.0197 - acc: 0.9738 - val_loss: 0.0150 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01559 to 0.01498, saving model to ./result/weights-31.h5\n",
      "Epoch 32/60\n",
      "203621/203621 [==============================] - 103s 508us/step - loss: 0.0194 - acc: 0.9741 - val_loss: 0.0162 - val_acc: 0.9793\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0196 - acc: 0.9738 - val_loss: 0.0159 - val_acc: 0.9798\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/60\n",
      "203621/203621 [==============================] - 103s 508us/step - loss: 0.0193 - acc: 0.9744 - val_loss: 0.0155 - val_acc: 0.9806\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/60\n",
      "203621/203621 [==============================] - 103s 507us/step - loss: 0.0192 - acc: 0.9744 - val_loss: 0.0165 - val_acc: 0.9799\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/60\n",
      "203621/203621 [==============================] - 103s 506us/step - loss: 0.0189 - acc: 0.9749 - val_loss: 0.0163 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/60\n",
      "203621/203621 [==============================] - 103s 504us/step - loss: 0.0189 - acc: 0.9748 - val_loss: 0.0152 - val_acc: 0.9805\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/60\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0187 - acc: 0.9750 - val_loss: 0.0158 - val_acc: 0.9801\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/60\n",
      "203621/203621 [==============================] - 103s 504us/step - loss: 0.0186 - acc: 0.9752 - val_loss: 0.0153 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/60\n",
      "203621/203621 [==============================] - 103s 508us/step - loss: 0.0186 - acc: 0.9751 - val_loss: 0.0148 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.01498 to 0.01477, saving model to ./result/weights-40.h5\n",
      "Epoch 41/60\n",
      "203621/203621 [==============================] - 102s 503us/step - loss: 0.0183 - acc: 0.9757 - val_loss: 0.0155 - val_acc: 0.9805\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/60\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.0182 - acc: 0.9757 - val_loss: 0.0159 - val_acc: 0.9799\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0177 - acc: 0.9767 - val_loss: 0.0154 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/60\n",
      "203621/203621 [==============================] - 103s 507us/step - loss: 0.0180 - acc: 0.9761 - val_loss: 0.0158 - val_acc: 0.9801\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/60\n",
      "203621/203621 [==============================] - 103s 504us/step - loss: 0.0178 - acc: 0.9764 - val_loss: 0.0157 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0176 - acc: 0.9769 - val_loss: 0.0156 - val_acc: 0.9805\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/60\n",
      "203621/203621 [==============================] - 102s 503us/step - loss: 0.0177 - acc: 0.9765 - val_loss: 0.0155 - val_acc: 0.9805\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0174 - acc: 0.9770 - val_loss: 0.0159 - val_acc: 0.9798\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/60\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.0172 - acc: 0.9772 - val_loss: 0.0163 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/60\n",
      "203621/203621 [==============================] - 103s 508us/step - loss: 0.0174 - acc: 0.9770 - val_loss: 0.0153 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 00050: early stopping\n",
      "'trainModelSP'  5742577.88 ms\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "2018-04-12 21:33:15.293915: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 21:33:15.370358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 21:33:15.370744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 21:33:15.370791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_nolearn_dropout_05_pos_caps\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x (InputLayer)                  (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 50)        887400      x[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "x_pos (InputLayer)              (None, 9, 45)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_capital (InputLayer)          (None, 9, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 9, 100)       0           embedding_1[0][0]                \n",
      "                                                                 x_pos[0][0]                      \n",
      "                                                                 x_capital[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 9, 100)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 7, 256)       77056       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)      (None, 5, 256)       196864      conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 160, 8)       0           primarycap_conv1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 160, 8)       0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "nercaps (CapsuleLayer)          (None, 8, 16)        163840      primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "out_pred (Length)               (None, 8)            0           nercaps[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,325,160\n",
      "Trainable params: 437,760\n",
      "Non-trainable params: 887,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/60\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.0557 - acc: 0.9294 - val_loss: 0.0232 - val_acc: 0.9701\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02324, saving model to ./result/weights-01.h5\n",
      "Epoch 2/60\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.0345 - acc: 0.9541 - val_loss: 0.0200 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02324 to 0.02000, saving model to ./result/weights-02.h5\n",
      "Epoch 3/60\n",
      "203621/203621 [==============================] - 103s 504us/step - loss: 0.0308 - acc: 0.9591 - val_loss: 0.0193 - val_acc: 0.9749\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02000 to 0.01928, saving model to ./result/weights-03.h5\n",
      "Epoch 4/60\n",
      "203621/203621 [==============================] - 102s 500us/step - loss: 0.0286 - acc: 0.9618 - val_loss: 0.0178 - val_acc: 0.9772\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01928 to 0.01783, saving model to ./result/weights-04.h5\n",
      "Epoch 5/60\n",
      "203621/203621 [==============================] - 102s 499us/step - loss: 0.0269 - acc: 0.9638 - val_loss: 0.0167 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01783 to 0.01668, saving model to ./result/weights-05.h5\n",
      "Epoch 6/60\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.0256 - acc: 0.9658 - val_loss: 0.0161 - val_acc: 0.9791\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01668 to 0.01615, saving model to ./result/weights-06.h5\n",
      "Epoch 7/60\n",
      "203621/203621 [==============================] - 103s 505us/step - loss: 0.0244 - acc: 0.9673 - val_loss: 0.0168 - val_acc: 0.9781\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0236 - acc: 0.9682 - val_loss: 0.0159 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01615 to 0.01592, saving model to ./result/weights-08.h5\n",
      "Epoch 9/60\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.0228 - acc: 0.9692 - val_loss: 0.0153 - val_acc: 0.9801\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01592 to 0.01533, saving model to ./result/weights-09.h5\n",
      "Epoch 10/60\n",
      "203621/203621 [==============================] - 103s 508us/step - loss: 0.0224 - acc: 0.9700 - val_loss: 0.0158 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/60\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0217 - acc: 0.9709 - val_loss: 0.0153 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01533 to 0.01527, saving model to ./result/weights-11.h5\n",
      "Epoch 12/60\n",
      "203621/203621 [==============================] - 103s 506us/step - loss: 0.0211 - acc: 0.9715 - val_loss: 0.0152 - val_acc: 0.9805\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01527 to 0.01522, saving model to ./result/weights-12.h5\n",
      "Epoch 13/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0204 - acc: 0.9726 - val_loss: 0.0146 - val_acc: 0.9809\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01522 to 0.01462, saving model to ./result/weights-13.h5\n",
      "Epoch 14/60\n",
      "203621/203621 [==============================] - 103s 508us/step - loss: 0.0199 - acc: 0.9731 - val_loss: 0.0143 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01462 to 0.01427, saving model to ./result/weights-14.h5\n",
      "Epoch 15/60\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.0196 - acc: 0.9739 - val_loss: 0.0153 - val_acc: 0.9802\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0197 - acc: 0.9739 - val_loss: 0.0148 - val_acc: 0.9805\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/60\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0189 - acc: 0.9746 - val_loss: 0.0139 - val_acc: 0.9818\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01427 to 0.01389, saving model to ./result/weights-17.h5\n",
      "Epoch 18/60\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0187 - acc: 0.9753 - val_loss: 0.0139 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01389 to 0.01387, saving model to ./result/weights-18.h5\n",
      "Epoch 19/60\n",
      "203621/203621 [==============================] - 101s 497us/step - loss: 0.0184 - acc: 0.9754 - val_loss: 0.0138 - val_acc: 0.9825\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01387 to 0.01379, saving model to ./result/weights-19.h5\n",
      "Epoch 20/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0181 - acc: 0.9757 - val_loss: 0.0139 - val_acc: 0.9822\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/60\n",
      "203621/203621 [==============================] - 103s 508us/step - loss: 0.0179 - acc: 0.9760 - val_loss: 0.0142 - val_acc: 0.9811\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0177 - acc: 0.9761 - val_loss: 0.0138 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/60\n",
      "203621/203621 [==============================] - 103s 505us/step - loss: 0.0175 - acc: 0.9766 - val_loss: 0.0136 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01379 to 0.01361, saving model to ./result/weights-23.h5\n",
      "Epoch 24/60\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0170 - acc: 0.9773 - val_loss: 0.0138 - val_acc: 0.9825\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/60\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0165 - acc: 0.9780 - val_loss: 0.0145 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0165 - acc: 0.9781 - val_loss: 0.0141 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/60\n",
      "203621/203621 [==============================] - 103s 507us/step - loss: 0.0170 - acc: 0.9776 - val_loss: 0.0136 - val_acc: 0.9823\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01361 to 0.01357, saving model to ./result/weights-27.h5\n",
      "Epoch 28/60\n",
      "203621/203621 [==============================] - 104s 509us/step - loss: 0.0162 - acc: 0.9786 - val_loss: 0.0132 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01357 to 0.01319, saving model to ./result/weights-28.h5\n",
      "Epoch 29/60\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.0160 - acc: 0.9787 - val_loss: 0.0133 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/60\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.0159 - acc: 0.9790 - val_loss: 0.0134 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/60\n",
      "203621/203621 [==============================] - 103s 506us/step - loss: 0.0157 - acc: 0.9790 - val_loss: 0.0136 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/60\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0154 - acc: 0.9795 - val_loss: 0.0134 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/60\n",
      "203621/203621 [==============================] - 104s 512us/step - loss: 0.0155 - acc: 0.9794 - val_loss: 0.0137 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/60\n",
      "203621/203621 [==============================] - 104s 513us/step - loss: 0.0152 - acc: 0.9799 - val_loss: 0.0139 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/60\n",
      "203621/203621 [==============================] - 102s 503us/step - loss: 0.0151 - acc: 0.9799 - val_loss: 0.0133 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/60\n",
      "203621/203621 [==============================] - 105s 513us/step - loss: 0.0152 - acc: 0.9798 - val_loss: 0.0137 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/60\n",
      "203621/203621 [==============================] - 104s 510us/step - loss: 0.0145 - acc: 0.9811 - val_loss: 0.0132 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/60\n",
      "203621/203621 [==============================] - 104s 511us/step - loss: 0.0146 - acc: 0.9806 - val_loss: 0.0136 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 00038: early stopping\n",
      "'trainModelSP'  4380453.71 ms\n",
      "'testFeatures'  18946505.56 ms\n",
      "\n",
      "\n",
      "Glove Embeddings with Learning and Dropout at 50%\n",
      "/home/dhuber/miniconda3/envs/capsnet/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2018-04-12 22:46:15.821624: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-04-12 22:46:15.902513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-04-12 22:46:15.902871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-04-12 22:46:15.902903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "x (?, 9)\n",
      "embed (?, 9, 50)\n",
      "embed (?, 9, 50)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n",
      "\n",
      "Training Model: glove_learn_dropout_05_base\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 9, 50)             0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 5, 256)            196864    \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 160, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             163840    \n",
      "_________________________________________________________________\n",
      "out_pred (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,286,760\n",
      "Trainable params: 1,286,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/60\n",
      "203621/203621 [==============================] - 114s 558us/step - loss: 0.0654 - acc: 0.9169 - val_loss: 0.0343 - val_acc: 0.9557\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03429, saving model to ./result/weights-01.h5\n",
      "Epoch 2/60\n",
      "203621/203621 [==============================] - 113s 554us/step - loss: 0.0372 - acc: 0.9502 - val_loss: 0.0292 - val_acc: 0.9627\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03429 to 0.02922, saving model to ./result/weights-02.h5\n",
      "Epoch 3/60\n",
      "203621/203621 [==============================] - 113s 555us/step - loss: 0.0301 - acc: 0.9599 - val_loss: 0.0261 - val_acc: 0.9662\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02922 to 0.02614, saving model to ./result/weights-03.h5\n",
      "Epoch 4/60\n",
      "203621/203621 [==============================] - 113s 553us/step - loss: 0.0256 - acc: 0.9660 - val_loss: 0.0249 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02614 to 0.02488, saving model to ./result/weights-04.h5\n",
      "Epoch 5/60\n",
      "203621/203621 [==============================] - 114s 561us/step - loss: 0.0222 - acc: 0.9705 - val_loss: 0.0242 - val_acc: 0.9687\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02488 to 0.02418, saving model to ./result/weights-05.h5\n",
      "Epoch 6/60\n",
      "203621/203621 [==============================] - 114s 560us/step - loss: 0.0200 - acc: 0.9731 - val_loss: 0.0239 - val_acc: 0.9695\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02418 to 0.02386, saving model to ./result/weights-06.h5\n",
      "Epoch 7/60\n",
      "203500/203621 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9769"
     ]
    }
   ],
   "source": [
    "# capsnet training function\n",
    "testFunc = \"trainCapsModel.py\"\n",
    "\n",
    "hypers = hyper_param_caps.copy()\n",
    "# Back to Adam\n",
    "hypers['optimizer'] = \"Adam\"\n",
    "print(\"Training with Adam\")\n",
    "\n",
    "hypers['epochs'] = 60\n",
    "hypers['stopping_patience'] = 10 # more dropout... let it go longer\n",
    "hypers['use_pos_tags'] = False\n",
    "hypers['use_capitalization_info'] = False\n",
    "\n",
    "# learn embeddings + Dropout\n",
    "print(\"\\n\\nLearn Embeddings and Dropout at 50%\")\n",
    "hypers['use_glove'] = False\n",
    "hypers['embed_dropout'] = 0.5\n",
    "testFeatures( testFunc, \"learn_dropout_05\", hypers)\n",
    "\n",
    "# use glove, no learn + Dropout\n",
    "print(\"\\n\\nGlove Embeddings and Dropout at 50%\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = False\n",
    "hypers['embed_dropout'] = 0.5\n",
    "testFeatures( testFunc, \"glove_nolearn_dropout_05\", hypers)\n",
    "\n",
    "# use glove, learn + Dropout\n",
    "print(\"\\n\\nGlove Embeddings with Learning and Dropout at 50%\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = True\n",
    "hypers['embed_dropout'] = 0.5\n",
    "testFeatures( testFunc, \"glove_learn_dropout_05\", hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testModel( draw_capsnet_model, hyper_param_caps)\n",
    "#testModel( capsmodel, hyper_param_caps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Primary Caps Layer\n",
    ">**NOT YET ATTEMPTED!**  \n",
    "* try one first, see if it even trains...  \n",
    "* may need a new set of hypers to get it working/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 100)\n",
      "embed (?, 9, 100)\n",
      "conv1 (?, 7, 256, 1)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_pred (?, 8)\n"
     ]
    }
   ],
   "source": [
    "# train a caps model with 2D Primary caps and repeat tests\n",
    "\n",
    "# capsnet training function\n",
    "testFunc = \"trainCapsModel.py\"\n",
    "\n",
    "hypers = hyper_param_caps.copy()\n",
    "hypers['use_2D_primarycaps'] = True\n",
    "\n",
    "hypers['epochs'] = 1\n",
    "hypers['stopping_patience'] = 1\n",
    "hypers['use_pos_tags'] = False\n",
    "hypers['use_capitalization_info'] = False\n",
    "\n",
    "# try different embeddings\n",
    "# learn embeddings\n",
    "print(\"\\n\\nLearn Embeddings\")\n",
    "hypers['use_glove'] = False\n",
    "hypers['embed_dropout'] = 0.0\n",
    "testFeatures( testFunc, \"2D_primcaps_learn\", hypers)\n",
    "\n",
    "# learn embeddings + Dropout\n",
    "print(\"\\n\\nLearn Embeddings and Dropout\")\n",
    "hypers['use_glove'] = False\n",
    "hypers['embed_dropout'] = 0.25\n",
    "testFeatures( testFunc, \"2D_primcaps_learn_dropout\", hypers)\n",
    "\n",
    "# use glove, no learn\n",
    "print(\"\\n\\nGlove Embeddings\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = False\n",
    "hypers['embed_dropout'] = 0.0\n",
    "testFeatures( testFunc, \"2D_primcaps_glove_nolearn\", hypers)\n",
    "\n",
    "# use glove, no learn + Dropout\n",
    "print(\"\\n\\nGlove Embeddings and Dropout\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = False\n",
    "hypers['embed_dropout'] = 0.25\n",
    "testFeatures( testFunc, \"2D_primcaps_glove_nolearn_dropout\", hypers)\n",
    "\n",
    "# use glove, learn\n",
    "print(\"\\n\\nGlove Embeddings with Learning\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = True\n",
    "hypers['embed_dropout'] = 0.0\n",
    "testFeatures( testFunc, \"2D_primcaps_glove_learn\", hypers)\n",
    "\n",
    "# use glove, learn + Dropout\n",
    "print(\"\\n\\nGlove Embeddings with Learning and Dropout\")\n",
    "hypers['use_glove'] = True\n",
    "hypers['allow_glove_retrain'] = True\n",
    "hypers['embed_dropout'] = 0.25\n",
    "testFeatures( testFunc, \"2D_primcaps_glove_learn_dropout\", hypers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline thoughts... we want the cnn to be the BEST. we want to compare our results to state of the art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
