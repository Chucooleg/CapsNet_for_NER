{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# UPDATES!\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "from keras import callbacks, optimizers\n",
    "from keras import backend as KB\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Activation\n",
    "from keras.layers import LeakyReLU, Dense, Input, Embedding, Dropout, Reshape, Concatenate # !\n",
    "from keras.layers import Bidirectional, GRU, Flatten, SpatialDropout1D, Conv1D\n",
    "from keras.datasets import imdb # probably redundant\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from common import vocabulary, utils\n",
    "\n",
    "import time # !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.4.1\n",
      "Keras version: 2.1.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"Keras version:\", K.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"../data/conll2003/eng.train\"\n",
    "DEV_FILE = \"../data/conll2003/eng.testa\"\n",
    "TEST_FILE = \"../data/conll2003/eng.testb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Class !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UPDATES!\n",
    "\n",
    "\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print ('%r  %2.2f ms' % \\\n",
    "                  (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed\n",
    "\n",
    "\n",
    "def capitalizaion(word):\n",
    "    \"\"\"\n",
    "    check capitalization info for a word\n",
    "    return 'lowercase' for 'sfsd'\n",
    "    return 'allCaps' for 'SFSD'\n",
    "    return 'upperInitial' for 'Sfsd'\n",
    "    return 'mixedCaps' for 'SfSd'\n",
    "    return 'noinfo' for '$#%@#' or '12334'\n",
    "    \"\"\"\n",
    "    alphas = [c.isalpha() for c in word] \n",
    "    if sum(alphas) != len(word):\n",
    "        return 'noinfo'\n",
    "    caps = [char.lower()==char for char in word]\n",
    "    if sum(caps) == len(word):\n",
    "        return 'lowercase'\n",
    "    elif sum(caps) == 0:\n",
    "        return 'allCaps'\n",
    "    elif caps[0] == False and sum(caps) == len(word)-1:\n",
    "        return 'upperInitial'\n",
    "    elif 0 < sum(caps) < len(word):\n",
    "        return 'mixedCaps'\n",
    "    else:\n",
    "        return 'noinfo'\n",
    "        \n",
    "\n",
    "\n",
    "class conll2003Data(object):\n",
    "    \"\"\"\n",
    "    Keep track of data and processing operations for a single CoNLL2003 data file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filePath_train):\n",
    "        \"\"\"\n",
    "        filePath(string): path to a CoNLL2003 raw data file for training the vocabulary\n",
    "        \"\"\"\n",
    "        self.vocab = []\n",
    "        self.posTags = []\n",
    "        self.nerTags = []\n",
    "        self.train_sentences = self.readFile(filePath_train)\n",
    "\n",
    "\n",
    "    @timeit\n",
    "    def readFile(self, filePath, verbose=True):\n",
    "        \"\"\"\n",
    "        Read the conll2003 raw data file\n",
    "\n",
    "        filename(string) - path to conll2003 file (train, test, etc.)\n",
    "        \n",
    "        Returns: a list of lists of lists corresponding to the words, pos tags, and ner tags\n",
    "                 in each sentence\n",
    "\n",
    "        \"\"\"\n",
    "        f = open(filePath)\n",
    "        sentences = []\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            if len(line) == 0 or line.startswith(\"-DOCSTART\") or line[0] == '\\n':\n",
    "                if len(sentence) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                continue\n",
    "            \n",
    "            # input format is [ word, pos tag, chunck tag, ner tag]\n",
    "            # we are ignoring the chunck tag\n",
    "            splits = line.strip().split(' ')\n",
    "            word = [utils.canonicalize_word(splits[0]), splits[1], splits[3], capitalizaion(splits[0])]\n",
    "            sentence.append( word)\n",
    "        \n",
    "        # don't forget the last sentence\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "        \n",
    "        if verbose: \n",
    "            print (\"----------------------------------------------------\")\n",
    "            print (\"reading file from path\", str(filePath))\n",
    "            print (\"number of sentences on file =\",len(sentences))\n",
    "            print (\"first 5 sentences:\")\n",
    "            print (sentences[:5])\n",
    "\n",
    "        return sentences\n",
    "\n",
    "\n",
    "    @timeit\n",
    "    def buildVocab(self, vocabSize=None, verbose=True, return_vocab_objects=False):\n",
    "        \"\"\"\n",
    "        Builds the vocabulary based on the initial data file\n",
    "        \n",
    "        vocabSize(int, default: None-all words) - max number of words to use for vocabulary\n",
    "                                                  (only used for training)\n",
    "        verbose(boolean, default: False)        - print extra info\n",
    "        \"\"\"    \t\n",
    "        if verbose:\n",
    "            print (\"----------------------------------------------------\")\n",
    "            print (\"building vocabulary from TRAINING data...\")\n",
    "\n",
    "        flatData = [w for w in zip(*utils.flatten(self.train_sentences))]\n",
    "\n",
    "        # remember these vocabs will have the <s>, </s>, and <unk> tags in there\n",
    "        # sizes need to be interpreted \"-3\" - consider replacing...\n",
    "        self.vocab = vocabulary.Vocabulary( flatData[0], size=vocabSize)\n",
    "        self.posTags = vocabulary.Vocabulary( flatData[1])\n",
    "        self.nerTags = vocabulary.Vocabulary( flatData[2])\n",
    "        self.capitalTags = vocabulary.Vocabulary(flatData[3])\n",
    "\n",
    "        if verbose:\n",
    "            print (\"vocabulary for words, posTags, nerTags built and stored in object\")\n",
    "            print (\"vocab size =\", vocabSize)\n",
    "            print (\"10 sampled words from vocabulary\\n\", list(self.vocab.wordset)[:10], \"\\n\")\n",
    "            print (\"number of unique pos Tags in training =\", self.posTags.size)\n",
    "            print (\"all posTags used\\n\", list(self.posTags.wordset), \"\\n\")\n",
    "            print (\"number of unique NER tags in training =\", self.nerTags.size)\n",
    "            print (\"all nerTags for prediction\", list(self.nerTags.wordset), \"\\n\")\n",
    "            print (\"number of unique capitalization tags in training =\", self.capitalTags.size)\n",
    "            print ('all capitalTags for prediction', list(self.capitalTags.wordset), \"\\n\")\n",
    "\n",
    "        if return_vocab_objects:\n",
    "            return self.vocab, self.posTags, self.nerTags, self.capitalTags\n",
    "\n",
    "\n",
    "    @timeit\n",
    "    def formatWindowedData(self, sentences, windowLength=9, verbose=False):\n",
    "        \"\"\"\n",
    "        Format the raw data by blocking it into context windows of a fixed length corresponding \n",
    "        to the single target NER tag of the central word.\n",
    "        Make sure to call buildVocab first.\n",
    "        \n",
    "        sentences(list of lists of lists) - raw data from the CoNLL2003 dataset\n",
    "        windowLength(int, default: 9)     - The length of the context window\n",
    "                    NOTE - windowLength must be odd to have a central word. If itsn't, 1 will be added.\n",
    "        verbose(boolean, default: False)  - print extra info\n",
    "        \n",
    "        Returns: 3 numpy arrays: vocabulary training data windowed and converted to IDs, \n",
    "                                 POS tags windowed and converted to IDs,\n",
    "                                 NER label tags converted to IDs\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print (\"----------------------------------------------------\")\n",
    "            print (\"formatting sentences into input windows...\")\n",
    "\n",
    "        if windowLength % 2 == 0 or windowLength == 1:\n",
    "            raise ValueError(\"window Length must be an odd number and greater than one.\")\n",
    "    \n",
    "        pads = windowLength // 2\n",
    "\n",
    "        # we have a list of lists (sentences) of lists ([word, posTag, nerTag])\n",
    "        # parse through, pad each sentence with pads open and close tags, then convert to IDs\n",
    "        vocabIDs = [ self.vocab.words_to_ids( [\"<s>\"] * pads + [word[0] for word in sent] + [\"</s>\"] * pads) \\\n",
    "                     for sent in sentences]\n",
    "        posIDs = [ self.posTags.words_to_ids( [\"<s>\"] * pads + [word[1] for word in sent] + [\"</s>\"] * pads) \\\n",
    "                   for sent in sentences]\n",
    "        capitalIDs = [self.capitalTags.words_to_ids([\"<s>\"]*pads + [word[3] for word in sent] + [\"</s>\"]*pads) \\\n",
    "                     for sent in sentences]\n",
    "        nerIDs = [ self.nerTags.words_to_ids( [\"<s>\"] * pads + [word[2] for word in sent] + [\"</s>\"] * pads) \\\n",
    "                   for sent in sentences]\n",
    "        \n",
    "        if verbose: \n",
    "            print (\"STEP 1/2 -- PADDING\")\n",
    "            print (\"all sentences padded with {} pads to either end\".format(pads))\n",
    "            print (\"vocab idx for first 5 sentences:\\n\", vocabIDs[:5], \"\\n\")\n",
    "            print (\"pos idx for first 5 sentences:\\n\", posIDs[:5], \"\\n\")\n",
    "            print (\"ner idx for first 5 sentences:\\n\", nerIDs[:5], \"\\n\")\n",
    "            print (\"capitalization idx for first 5 sentences: \\n\", capitalIDs[:5], \"\\n\")\n",
    "            print (\"number of sentences = {}\".format(len(vocabIDs)), \"\\n\")\n",
    "\n",
    "        assert(len(vocabIDs) == len(posIDs) and len(posIDs) == len(nerIDs) == len(capitalIDs))\n",
    "\n",
    "        if verbose: \n",
    "            print (\"STEP 2/2 -- WINDOWING\")\n",
    "\n",
    "        # build the data to train on by sliding the window across each sentence\n",
    "        # at this point, all 3 lists are the same size, so we can run through them all at once\n",
    "        featsVocab, featsPOS, featsNER, featsCAPITAL = [], [], [], []\n",
    "        for sentID in range( len(vocabIDs)):\n",
    "            sent = vocabIDs[sentID]\n",
    "            sentPOS = posIDs[sentID]\n",
    "            sentNER = nerIDs[sentID]\n",
    "            sentCAPITAL = capitalIDs[sentID]\n",
    "            \n",
    "            for ID in range( len(sent) - windowLength + 1):\n",
    "                featsVocab.append( sent[ID:ID + windowLength])\n",
    "                featsPOS.append( sentPOS[ID:ID + windowLength])\n",
    "                featsCAPITAL.append(sentCAPITAL[ID:ID + windowLength])\n",
    "                featsNER.append( sentNER[ID + windowLength // 2])\n",
    "                \n",
    "        if verbose:\n",
    "            print (\"sample windows:\")\n",
    "            for i in range(3):\n",
    "                print (\"Vocab for window {}\".format(i))\n",
    "                print (featsVocab[i])\n",
    "                print (self.vocab.ids_to_words(featsVocab[i]))\n",
    "                print (\"PoS tags for window {}\".format(i))\n",
    "                print (featsPOS[i])\n",
    "                print (self.posTags.ids_to_words(featsPOS[i]))\n",
    "                print (\"Capitalization tags for window {}\".format(i))\n",
    "                print (featsCAPITAL[i])\n",
    "                print (self.capitalTags.ids_to_words(featsCAPITAL[i]))\n",
    "                print (\"NER tags for center word\")\n",
    "                print (featsNER[i])\n",
    "                print (self.nerTags.ids_to_words([featsNER[i]]),\"\\n\")\n",
    "                \n",
    "            print (\"rows of vocab features = {}\".format(len(featsVocab)))\n",
    "            print (\"rows of PoS features = {}\".format(len(featsVocab)))\n",
    "            print (\"rows of Capitalization features = {}\".format(len(featsCAPITAL)))\n",
    "            print (\"rows of NER features = {}\".format(len(featsNER)))\n",
    "            \n",
    "            print (\"numpy feature arrays are returned\")\n",
    "\n",
    "        assert(len(featsVocab) == len(featsVocab) == len(featsNER) == len(featsCAPITAL))\n",
    "        return np.array(featsVocab), np.array(featsPOS), np.array(featsCAPITAL), np.array(featsNER) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "reading file from path ../data/conll2003/eng.train\n",
      "number of sentences on file = 14041\n",
      "first 5 sentences:\n",
      "[[['eu', 'NNP', 'I-ORG', 'allCaps'], ['rejects', 'VBZ', 'O', 'lowercase'], ['german', 'JJ', 'I-MISC', 'upperInitial'], ['call', 'NN', 'O', 'lowercase'], ['to', 'TO', 'O', 'lowercase'], ['boycott', 'VB', 'O', 'lowercase'], ['british', 'JJ', 'I-MISC', 'upperInitial'], ['lamb', 'NN', 'O', 'lowercase'], ['.', '.', 'O', 'noinfo']], [['peter', 'NNP', 'I-PER', 'upperInitial'], ['blackburn', 'NNP', 'I-PER', 'upperInitial']], [['brussels', 'NNP', 'I-LOC', 'allCaps'], ['DGDGDGDG-DGDG-DGDG', 'CD', 'O', 'noinfo']], [['the', 'DT', 'O', 'upperInitial'], ['european', 'NNP', 'I-ORG', 'upperInitial'], ['commission', 'NNP', 'I-ORG', 'upperInitial'], ['said', 'VBD', 'O', 'lowercase'], ['on', 'IN', 'O', 'lowercase'], ['thursday', 'NNP', 'O', 'upperInitial'], ['it', 'PRP', 'O', 'lowercase'], ['disagreed', 'VBD', 'O', 'lowercase'], ['with', 'IN', 'O', 'lowercase'], ['german', 'JJ', 'I-MISC', 'upperInitial'], ['advice', 'NN', 'O', 'lowercase'], ['to', 'TO', 'O', 'lowercase'], ['consumers', 'NNS', 'O', 'lowercase'], ['to', 'TO', 'O', 'lowercase'], ['shun', 'VB', 'O', 'lowercase'], ['british', 'JJ', 'I-MISC', 'upperInitial'], ['lamb', 'NN', 'O', 'lowercase'], ['until', 'IN', 'O', 'lowercase'], ['scientists', 'NNS', 'O', 'lowercase'], ['determine', 'VBP', 'O', 'lowercase'], ['whether', 'IN', 'O', 'lowercase'], ['mad', 'JJ', 'O', 'lowercase'], ['cow', 'NN', 'O', 'lowercase'], ['disease', 'NN', 'O', 'lowercase'], ['can', 'MD', 'O', 'lowercase'], ['be', 'VB', 'O', 'lowercase'], ['transmitted', 'VBN', 'O', 'lowercase'], ['to', 'TO', 'O', 'lowercase'], ['sheep', 'NN', 'O', 'lowercase'], ['.', '.', 'O', 'noinfo']], [['germany', 'NNP', 'I-LOC', 'upperInitial'], [\"'s\", 'POS', 'O', 'noinfo'], ['representative', 'NN', 'O', 'lowercase'], ['to', 'TO', 'O', 'lowercase'], ['the', 'DT', 'O', 'lowercase'], ['european', 'NNP', 'I-ORG', 'upperInitial'], ['union', 'NNP', 'I-ORG', 'upperInitial'], [\"'s\", 'POS', 'O', 'noinfo'], ['veterinary', 'JJ', 'O', 'lowercase'], ['committee', 'NN', 'O', 'lowercase'], ['werner', 'NNP', 'I-PER', 'upperInitial'], ['zwingmann', 'NNP', 'I-PER', 'upperInitial'], ['said', 'VBD', 'O', 'lowercase'], ['on', 'IN', 'O', 'lowercase'], ['wednesday', 'NNP', 'O', 'upperInitial'], ['consumers', 'NNS', 'O', 'lowercase'], ['should', 'MD', 'O', 'lowercase'], ['buy', 'VB', 'O', 'lowercase'], ['sheepmeat', 'NN', 'O', 'lowercase'], ['from', 'IN', 'O', 'lowercase'], ['countries', 'NNS', 'O', 'lowercase'], ['other', 'JJ', 'O', 'lowercase'], ['than', 'IN', 'O', 'lowercase'], ['britain', 'NNP', 'I-LOC', 'upperInitial'], ['until', 'IN', 'O', 'lowercase'], ['the', 'DT', 'O', 'lowercase'], ['scientific', 'JJ', 'O', 'lowercase'], ['advice', 'NN', 'O', 'lowercase'], ['was', 'VBD', 'O', 'lowercase'], ['clearer', 'JJR', 'O', 'lowercase'], ['.', '.', 'O', 'noinfo']]]\n",
      "'readFile'  1139.50 ms\n",
      "----------------------------------------------------\n",
      "building vocabulary from TRAINING data...\n",
      "vocabulary for words, posTags, nerTags built and stored in object\n",
      "vocab size = 20000\n",
      "10 sampled words from vocabulary\n",
      " ['spa', 'leoni', 'quell', 'troubles', 'maoist', 'receiving', 'admira', 'mandatory', 'auto', 'inderjit'] \n",
      "\n",
      "number of unique pos Tags in training = 48\n",
      "all posTags used\n",
      " [':', 'JJ', '</s>', ',', 'VBN', 'UH', 'TO', '.', 'CD', ')', 'PRP', 'VBG', 'EX', 'WP$', 'VBZ', 'RB', 'NNP', 'RP', 'DT', 'FW', 'PDT', '<unk>', 'SYM', 'NN|SYM', 'VBD', 'RBS', 'NNS', \"''\", 'RBR', 'IN', 'PRP$', '$', 'VB', '(', 'WDT', 'POS', '\"', 'JJR', 'NNPS', '<s>', 'LS', 'JJS', 'VBP', 'WP', 'MD', 'WRB', 'NN', 'CC'] \n",
      "\n",
      "number of unique NER tags in training = 11\n",
      "all nerTags for prediction ['</s>', 'I-PER', 'B-MISC', 'O', 'I-MISC', '<unk>', 'B-ORG', 'I-LOC', 'I-ORG', 'B-LOC', '<s>'] \n",
      "\n",
      "number of unique capitalization tags in training = 8\n",
      "all capitalTags for prediction ['allCaps', 'mixedCaps', '</s>', 'noinfo', '<unk>', 'lowercase', 'upperInitial', '<s>'] \n",
      "\n",
      "'buildVocab'  1003.20 ms\n",
      "----------------------------------------------------\n",
      "formatting sentences into input windows...\n",
      "STEP 1/2 -- PADDING\n",
      "all sentences padded with 4 pads to either end\n",
      "vocab idx for first 5 sentences:\n",
      " [[0, 0, 0, 0, 931, 9970, 198, 583, 10, 3751, 207, 5455, 4, 1, 1, 1, 1], [0, 0, 0, 0, 723, 1774, 1, 1, 1, 1], [0, 0, 0, 0, 675, 26, 1, 1, 1, 1], [0, 0, 0, 0, 3, 208, 314, 17, 16, 73, 31, 7031, 27, 198, 3752, 10, 2287, 10, 9971, 207, 5455, 375, 3220, 1906, 481, 1652, 1775, 601, 290, 44, 7032, 10, 1548, 4, 1, 1, 1, 1], [0, 0, 0, 0, 126, 18, 2849, 10, 3, 208, 283, 18, 2531, 749, 7033, 7034, 17, 16, 78, 2287, 261, 860, 7035, 30, 500, 129, 127, 140, 375, 3, 2288, 3752, 22, 9972, 4, 1, 1, 1, 1]] \n",
      "\n",
      "pos idx for first 5 sentences:\n",
      " [[0, 0, 0, 0, 3, 22, 8, 4, 17, 13, 8, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 5, 1, 1, 1, 1], [0, 0, 0, 0, 7, 3, 3, 10, 6, 3, 18, 10, 6, 8, 4, 17, 9, 17, 13, 8, 4, 6, 9, 27, 6, 8, 4, 4, 28, 13, 14, 17, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 3, 25, 4, 17, 7, 3, 3, 25, 8, 4, 3, 3, 10, 6, 3, 9, 28, 13, 4, 6, 9, 8, 6, 3, 6, 7, 8, 4, 10, 36, 11, 1, 1, 1, 1]] \n",
      "\n",
      "ner idx for first 5 sentences:\n",
      " [[0, 0, 0, 0, 5, 3, 7, 3, 3, 3, 7, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 4, 4, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 5, 5, 3, 3, 3, 3, 3, 3, 7, 3, 3, 3, 3, 3, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 3, 3, 3, 5, 5, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1]] \n",
      "\n",
      "capitalization idx for first 5 sentences: \n",
      " [[0, 0, 0, 0, 6, 3, 5, 3, 3, 3, 5, 3, 4, 1, 1, 1, 1], [0, 0, 0, 0, 5, 5, 1, 1, 1, 1], [0, 0, 0, 0, 6, 4, 1, 1, 1, 1], [0, 0, 0, 0, 5, 5, 5, 3, 3, 5, 3, 3, 3, 5, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 1, 1, 1, 1], [0, 0, 0, 0, 5, 4, 3, 3, 3, 5, 5, 4, 3, 3, 5, 5, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 4, 1, 1, 1, 1]] \n",
      "\n",
      "number of sentences = 14041 \n",
      "\n",
      "STEP 2/2 -- WINDOWING\n",
      "sample windows:\n",
      "Vocab for window 0\n",
      "[0, 0, 0, 0, 931, 9970, 198, 583, 10]\n",
      "['<s>', '<s>', '<s>', '<s>', 'eu', 'rejects', 'german', 'call', 'to']\n",
      "PoS tags for window 0\n",
      "[0, 0, 0, 0, 3, 22, 8, 4, 17]\n",
      "['<s>', '<s>', '<s>', '<s>', 'NNP', 'VBZ', 'JJ', 'NN', 'TO']\n",
      "Capitalization tags for window 0\n",
      "[0, 0, 0, 0, 6, 3, 5, 3, 3]\n",
      "['<s>', '<s>', '<s>', '<s>', 'allCaps', 'lowercase', 'upperInitial', 'lowercase', 'lowercase']\n",
      "NER tags for center word\n",
      "5\n",
      "['I-ORG'] \n",
      "\n",
      "Vocab for window 1\n",
      "[0, 0, 0, 931, 9970, 198, 583, 10, 3751]\n",
      "['<s>', '<s>', '<s>', 'eu', 'rejects', 'german', 'call', 'to', 'boycott']\n",
      "PoS tags for window 1\n",
      "[0, 0, 0, 3, 22, 8, 4, 17, 13]\n",
      "['<s>', '<s>', '<s>', 'NNP', 'VBZ', 'JJ', 'NN', 'TO', 'VB']\n",
      "Capitalization tags for window 1\n",
      "[0, 0, 0, 6, 3, 5, 3, 3, 3]\n",
      "['<s>', '<s>', '<s>', 'allCaps', 'lowercase', 'upperInitial', 'lowercase', 'lowercase', 'lowercase']\n",
      "NER tags for center word\n",
      "3\n",
      "['O'] \n",
      "\n",
      "Vocab for window 2\n",
      "[0, 0, 931, 9970, 198, 583, 10, 3751, 207]\n",
      "['<s>', '<s>', 'eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british']\n",
      "PoS tags for window 2\n",
      "[0, 0, 3, 22, 8, 4, 17, 13, 8]\n",
      "['<s>', '<s>', 'NNP', 'VBZ', 'JJ', 'NN', 'TO', 'VB', 'JJ']\n",
      "Capitalization tags for window 2\n",
      "[0, 0, 6, 3, 5, 3, 3, 3, 5]\n",
      "['<s>', '<s>', 'allCaps', 'lowercase', 'upperInitial', 'lowercase', 'lowercase', 'lowercase', 'upperInitial']\n",
      "NER tags for center word\n",
      "7\n",
      "['I-MISC'] \n",
      "\n",
      "rows of vocab features = 203621\n",
      "rows of PoS features = 203621\n",
      "rows of Capitalization features = 203621\n",
      "rows of NER features = 203621\n",
      "numpy feature arrays are returned\n",
      "'formatWindowedData'  1989.43 ms\n",
      "----------------------------------------------------\n",
      "reading file from path ../data/conll2003/eng.testa\n",
      "number of sentences on file = 3250\n",
      "first 5 sentences:\n",
      "[[['cricket', 'NNP', 'O', 'allCaps'], ['-', ':', 'O', 'noinfo'], ['leicestershire', 'NNP', 'I-ORG', 'allCaps'], ['take', 'NNP', 'O', 'allCaps'], ['over', 'IN', 'O', 'allCaps'], ['at', 'NNP', 'O', 'allCaps'], ['top', 'NNP', 'O', 'allCaps'], ['after', 'NNP', 'O', 'allCaps'], ['innings', 'NNP', 'O', 'allCaps'], ['victory', 'NN', 'O', 'allCaps'], ['.', '.', 'O', 'noinfo']], [['london', 'NNP', 'I-LOC', 'allCaps'], ['DGDGDGDG-DGDG-DGDG', 'CD', 'O', 'noinfo']], [['west', 'NNP', 'I-MISC', 'upperInitial'], ['indian', 'NNP', 'I-MISC', 'upperInitial'], ['all-rounder', 'NN', 'O', 'noinfo'], ['phil', 'NNP', 'I-PER', 'upperInitial'], ['simmons', 'NNP', 'I-PER', 'upperInitial'], ['took', 'VBD', 'O', 'lowercase'], ['four', 'CD', 'O', 'lowercase'], ['for', 'IN', 'O', 'lowercase'], ['DGDG', 'CD', 'O', 'noinfo'], ['on', 'IN', 'O', 'lowercase'], ['friday', 'NNP', 'O', 'upperInitial'], ['as', 'IN', 'O', 'lowercase'], ['leicestershire', 'NNP', 'I-ORG', 'upperInitial'], ['beat', 'VBD', 'O', 'lowercase'], ['somerset', 'NNP', 'I-ORG', 'upperInitial'], ['by', 'IN', 'O', 'lowercase'], ['an', 'DT', 'O', 'lowercase'], ['innings', 'NN', 'O', 'lowercase'], ['and', 'CC', 'O', 'lowercase'], ['DGDG', 'CD', 'O', 'noinfo'], ['runs', 'NNS', 'O', 'lowercase'], ['in', 'IN', 'O', 'lowercase'], ['two', 'CD', 'O', 'lowercase'], ['days', 'NNS', 'O', 'lowercase'], ['to', 'TO', 'O', 'lowercase'], ['take', 'VB', 'O', 'lowercase'], ['over', 'IN', 'O', 'lowercase'], ['at', 'IN', 'O', 'lowercase'], ['the', 'DT', 'O', 'lowercase'], ['head', 'NN', 'O', 'lowercase'], ['of', 'IN', 'O', 'lowercase'], ['the', 'DT', 'O', 'lowercase'], ['county', 'NN', 'O', 'lowercase'], ['championship', 'NN', 'O', 'lowercase'], ['.', '.', 'O', 'noinfo']], [['their', 'PRP$', 'O', 'upperInitial'], ['stay', 'NN', 'O', 'lowercase'], ['on', 'IN', 'O', 'lowercase'], ['top', 'NN', 'O', 'lowercase'], [',', ',', 'O', 'noinfo'], ['though', 'RB', 'O', 'lowercase'], [',', ',', 'O', 'noinfo'], ['may', 'MD', 'O', 'lowercase'], ['be', 'VB', 'O', 'lowercase'], ['short-lived', 'JJ', 'O', 'noinfo'], ['as', 'IN', 'O', 'lowercase'], ['title', 'NN', 'O', 'lowercase'], ['rivals', 'NNS', 'O', 'lowercase'], ['essex', 'NNP', 'I-ORG', 'upperInitial'], [',', ',', 'O', 'noinfo'], ['derbyshire', 'NNP', 'I-ORG', 'upperInitial'], ['and', 'CC', 'O', 'lowercase'], ['surrey', 'NNP', 'I-ORG', 'upperInitial'], ['all', 'DT', 'O', 'lowercase'], ['closed', 'VBD', 'O', 'lowercase'], ['in', 'RP', 'O', 'lowercase'], ['on', 'IN', 'O', 'lowercase'], ['victory', 'NN', 'O', 'lowercase'], ['while', 'IN', 'O', 'lowercase'], ['kent', 'NNP', 'I-ORG', 'upperInitial'], ['made', 'VBD', 'O', 'lowercase'], ['up', 'RP', 'O', 'lowercase'], ['for', 'IN', 'O', 'lowercase'], ['lost', 'VBN', 'O', 'lowercase'], ['time', 'NN', 'O', 'lowercase'], ['in', 'IN', 'O', 'lowercase'], ['their', 'PRP$', 'O', 'lowercase'], ['rain-affected', 'JJ', 'O', 'noinfo'], ['match', 'NN', 'O', 'lowercase'], ['against', 'IN', 'O', 'lowercase'], ['nottinghamshire', 'NNP', 'I-ORG', 'upperInitial'], ['.', '.', 'O', 'noinfo']], [['after', 'IN', 'O', 'upperInitial'], ['bowling', 'VBG', 'O', 'lowercase'], ['somerset', 'NNP', 'I-ORG', 'upperInitial'], ['out', 'RP', 'O', 'lowercase'], ['for', 'IN', 'O', 'lowercase'], ['DGDG', 'CD', 'O', 'noinfo'], ['on', 'IN', 'O', 'lowercase'], ['the', 'DT', 'O', 'lowercase'], ['opening', 'NN', 'O', 'lowercase'], ['morning', 'NN', 'O', 'lowercase'], ['at', 'IN', 'O', 'lowercase'], ['grace', 'NNP', 'I-LOC', 'upperInitial'], ['road', 'NNP', 'I-LOC', 'upperInitial'], [',', ',', 'O', 'noinfo'], ['leicestershire', 'NNP', 'I-ORG', 'upperInitial'], ['extended', 'VBD', 'O', 'lowercase'], ['their', 'PRP$', 'O', 'lowercase'], ['first', 'JJ', 'O', 'lowercase'], ['innings', 'NN', 'O', 'lowercase'], ['by', 'IN', 'O', 'lowercase'], ['DGDG', 'CD', 'O', 'noinfo'], ['runs', 'VBZ', 'O', 'lowercase'], ['before', 'IN', 'O', 'lowercase'], ['being', 'VBG', 'O', 'lowercase'], ['bowled', 'VBD', 'O', 'lowercase'], ['out', 'RP', 'O', 'lowercase'], ['for', 'IN', 'O', 'lowercase'], ['DGDGDG', 'CD', 'O', 'noinfo'], ['with', 'IN', 'O', 'lowercase'], ['england', 'NNP', 'I-LOC', 'upperInitial'], ['discard', 'VBP', 'O', 'lowercase'], ['andy', 'NNP', 'I-PER', 'upperInitial'], ['caddick', 'NNP', 'I-PER', 'upperInitial'], ['taking', 'VBG', 'O', 'lowercase'], ['three', 'CD', 'O', 'lowercase'], ['for', 'IN', 'O', 'lowercase'], ['DGDG', 'CD', 'O', 'noinfo'], ['.', '.', 'O', 'noinfo']]]\n",
      "'readFile'  249.38 ms\n",
      "----------------------------------------------------\n",
      "formatting sentences into input windows...\n",
      "STEP 1/2 -- PADDING\n",
      "all sentences padded with 4 pads to either end\n",
      "vocab idx for first 5 sentences:\n",
      " [[0, 0, 0, 0, 279, 20, 2331, 246, 76, 21, 358, 45, 274, 292, 4, 1, 1, 1, 1], [0, 0, 0, 0, 104, 26, 1, 1, 1, 1], [0, 0, 0, 0, 232, 892, 5610, 3281, 5588, 270, 155, 19, 9, 16, 101, 35, 2331, 60, 2900, 32, 41, 274, 12, 9, 356, 8, 52, 272, 10, 246, 76, 21, 3, 505, 7, 3, 642, 293, 4, 1, 1, 1, 1], [0, 0, 0, 0, 59, 1143, 16, 358, 5, 1625, 5, 187, 44, 14010, 35, 680, 2540, 1248, 5, 6256, 12, 1676, 123, 491, 8, 16, 292, 191, 2585, 189, 65, 19, 197, 125, 8, 59, 15311, 136, 82, 2332, 4, 1, 1, 1, 1], [0, 0, 0, 0, 45, 1126, 2900, 71, 19, 9, 16, 3, 728, 517, 21, 9131, 740, 5, 2331, 3259, 59, 47, 274, 32, 9, 356, 116, 230, 4015, 71, 19, 24, 27, 122, 2, 2604, 9110, 435, 91, 19, 9, 4, 1, 1, 1, 1]] \n",
      "\n",
      "pos idx for first 5 sentences:\n",
      " [[0, 0, 0, 0, 3, 23, 3, 3, 6, 3, 3, 3, 3, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 3, 5, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 4, 3, 3, 10, 5, 6, 5, 6, 3, 6, 3, 10, 3, 6, 7, 4, 16, 5, 9, 6, 5, 9, 17, 13, 6, 6, 7, 4, 6, 7, 4, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 26, 4, 6, 4, 12, 15, 12, 28, 13, 8, 6, 4, 9, 3, 12, 3, 16, 3, 7, 10, 31, 6, 4, 6, 3, 10, 31, 6, 14, 4, 6, 26, 8, 4, 6, 3, 11, 1, 1, 1, 1], [0, 0, 0, 0, 6, 21, 3, 31, 6, 5, 6, 7, 4, 4, 6, 3, 3, 12, 3, 10, 26, 8, 4, 6, 5, 22, 6, 21, 10, 31, 6, 5, 6, 3, 27, 3, 3, 21, 5, 6, 5, 11, 1, 1, 1, 1]] \n",
      "\n",
      "ner idx for first 5 sentences:\n",
      " [[0, 0, 0, 0, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 1, 1, 1, 1], [0, 0, 0, 0, 7, 7, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 5, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3, 5, 3, 5, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 6, 6, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 4, 4, 3, 3, 3, 3, 3, 1, 1, 1, 1]] \n",
      "\n",
      "capitalization idx for first 5 sentences: \n",
      " [[0, 0, 0, 0, 6, 4, 6, 6, 6, 6, 6, 6, 6, 6, 4, 1, 1, 1, 1], [0, 0, 0, 0, 6, 4, 1, 1, 1, 1], [0, 0, 0, 0, 5, 5, 4, 5, 5, 3, 3, 3, 4, 3, 5, 3, 5, 3, 5, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 1, 1, 1, 1], [0, 0, 0, 0, 5, 3, 3, 3, 4, 3, 4, 3, 3, 4, 3, 3, 3, 5, 4, 5, 3, 5, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 5, 4, 1, 1, 1, 1], [0, 0, 0, 0, 5, 3, 5, 3, 3, 4, 3, 3, 3, 3, 3, 5, 5, 4, 5, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 4, 3, 5, 3, 5, 5, 3, 3, 3, 4, 4, 1, 1, 1, 1]] \n",
      "\n",
      "number of sentences = 3250 \n",
      "\n",
      "STEP 2/2 -- WINDOWING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample windows:\n",
      "Vocab for window 0\n",
      "[0, 0, 0, 0, 279, 20, 2331, 246, 76]\n",
      "['<s>', '<s>', '<s>', '<s>', 'cricket', '-', 'leicestershire', 'take', 'over']\n",
      "PoS tags for window 0\n",
      "[0, 0, 0, 0, 3, 23, 3, 3, 6]\n",
      "['<s>', '<s>', '<s>', '<s>', 'NNP', ':', 'NNP', 'NNP', 'IN']\n",
      "Capitalization tags for window 0\n",
      "[0, 0, 0, 0, 6, 4, 6, 6, 6]\n",
      "['<s>', '<s>', '<s>', '<s>', 'allCaps', 'noinfo', 'allCaps', 'allCaps', 'allCaps']\n",
      "NER tags for center word\n",
      "3\n",
      "['O'] \n",
      "\n",
      "Vocab for window 1\n",
      "[0, 0, 0, 279, 20, 2331, 246, 76, 21]\n",
      "['<s>', '<s>', '<s>', 'cricket', '-', 'leicestershire', 'take', 'over', 'at']\n",
      "PoS tags for window 1\n",
      "[0, 0, 0, 3, 23, 3, 3, 6, 3]\n",
      "['<s>', '<s>', '<s>', 'NNP', ':', 'NNP', 'NNP', 'IN', 'NNP']\n",
      "Capitalization tags for window 1\n",
      "[0, 0, 0, 6, 4, 6, 6, 6, 6]\n",
      "['<s>', '<s>', '<s>', 'allCaps', 'noinfo', 'allCaps', 'allCaps', 'allCaps', 'allCaps']\n",
      "NER tags for center word\n",
      "3\n",
      "['O'] \n",
      "\n",
      "Vocab for window 2\n",
      "[0, 0, 279, 20, 2331, 246, 76, 21, 358]\n",
      "['<s>', '<s>', 'cricket', '-', 'leicestershire', 'take', 'over', 'at', 'top']\n",
      "PoS tags for window 2\n",
      "[0, 0, 3, 23, 3, 3, 6, 3, 3]\n",
      "['<s>', '<s>', 'NNP', ':', 'NNP', 'NNP', 'IN', 'NNP', 'NNP']\n",
      "Capitalization tags for window 2\n",
      "[0, 0, 6, 4, 6, 6, 6, 6, 6]\n",
      "['<s>', '<s>', 'allCaps', 'noinfo', 'allCaps', 'allCaps', 'allCaps', 'allCaps', 'allCaps']\n",
      "NER tags for center word\n",
      "5\n",
      "['I-ORG'] \n",
      "\n",
      "rows of vocab features = 51362\n",
      "rows of PoS features = 51362\n",
      "rows of Capitalization features = 51362\n",
      "rows of NER features = 51362\n",
      "numpy feature arrays are returned\n",
      "'formatWindowedData'  481.53 ms\n",
      "----------------------------------------------------\n",
      "reading file from path ../data/conll2003/eng.testb\n",
      "number of sentences on file = 3453\n",
      "first 5 sentences:\n",
      "[[['soccer', 'NN', 'O', 'allCaps'], ['-', ':', 'O', 'noinfo'], ['japan', 'NNP', 'I-LOC', 'allCaps'], ['get', 'VB', 'O', 'allCaps'], ['lucky', 'NNP', 'O', 'allCaps'], ['win', 'NNP', 'O', 'allCaps'], [',', ',', 'O', 'noinfo'], ['china', 'NNP', 'I-PER', 'allCaps'], ['in', 'IN', 'O', 'allCaps'], ['surprise', 'DT', 'O', 'allCaps'], ['defeat', 'NN', 'O', 'allCaps'], ['.', '.', 'O', 'noinfo']], [['nadim', 'NNP', 'I-PER', 'upperInitial'], ['ladki', 'NNP', 'I-PER', 'upperInitial']], [['al-ain', 'NNP', 'I-LOC', 'noinfo'], [',', ',', 'O', 'noinfo'], ['united', 'NNP', 'I-LOC', 'upperInitial'], ['arab', 'NNP', 'I-LOC', 'upperInitial'], ['emirates', 'NNPS', 'I-LOC', 'upperInitial'], ['DGDGDGDG-DGDG-DGDG', 'CD', 'O', 'noinfo']], [['japan', 'NNP', 'I-LOC', 'upperInitial'], ['began', 'VBD', 'O', 'lowercase'], ['the', 'DT', 'O', 'lowercase'], ['defence', 'NN', 'O', 'lowercase'], ['of', 'IN', 'O', 'lowercase'], ['their', 'PRP$', 'O', 'lowercase'], ['asian', 'JJ', 'I-MISC', 'upperInitial'], ['cup', 'NNP', 'I-MISC', 'upperInitial'], ['title', 'NN', 'O', 'lowercase'], ['with', 'IN', 'O', 'lowercase'], ['a', 'DT', 'O', 'lowercase'], ['lucky', 'JJ', 'O', 'lowercase'], ['DG-DG', 'CD', 'O', 'noinfo'], ['win', 'VBP', 'O', 'lowercase'], ['against', 'IN', 'O', 'lowercase'], ['syria', 'NNP', 'I-LOC', 'upperInitial'], ['in', 'IN', 'O', 'lowercase'], ['a', 'DT', 'O', 'lowercase'], ['group', 'NNP', 'O', 'upperInitial'], ['c', 'NNP', 'O', 'allCaps'], ['championship', 'NN', 'O', 'lowercase'], ['match', 'NN', 'O', 'lowercase'], ['on', 'IN', 'O', 'lowercase'], ['friday', 'NNP', 'O', 'upperInitial'], ['.', '.', 'O', 'noinfo']], [['but', 'CC', 'O', 'upperInitial'], ['china', 'NNP', 'I-LOC', 'upperInitial'], ['saw', 'VBD', 'O', 'lowercase'], ['their', 'PRP$', 'O', 'lowercase'], ['luck', 'NN', 'O', 'lowercase'], ['desert', 'VB', 'O', 'lowercase'], ['them', 'PRP', 'O', 'lowercase'], ['in', 'IN', 'O', 'lowercase'], ['the', 'DT', 'O', 'lowercase'], ['second', 'NN', 'O', 'lowercase'], ['match', 'NN', 'O', 'lowercase'], ['of', 'IN', 'O', 'lowercase'], ['the', 'DT', 'O', 'lowercase'], ['group', 'NN', 'O', 'lowercase'], [',', ',', 'O', 'noinfo'], ['crashing', 'VBG', 'O', 'lowercase'], ['to', 'TO', 'O', 'lowercase'], ['a', 'DT', 'O', 'lowercase'], ['surprise', 'NN', 'O', 'lowercase'], ['DG-DG', 'CD', 'O', 'noinfo'], ['defeat', 'NN', 'O', 'lowercase'], ['to', 'TO', 'O', 'lowercase'], ['newcomers', 'NNS', 'O', 'lowercase'], ['uzbekistan', 'NNP', 'I-LOC', 'upperInitial'], ['.', '.', 'O', 'noinfo']]]\n",
      "'readFile'  227.72 ms\n",
      "----------------------------------------------------\n",
      "formatting sentences into input windows...\n",
      "STEP 1/2 -- PADDING\n",
      "all sentences padded with 4 pads to either end\n",
      "vocab idx for first 5 sentences:\n",
      " [[0, 0, 0, 0, 92, 20, 215, 410, 7671, 162, 5, 205, 8, 2628, 994, 4, 1, 1, 1, 1], [0, 0, 0, 0, 2, 2, 1, 1, 1, 1], [0, 0, 0, 0, 2, 5, 168, 1069, 10061, 26, 1, 1, 1, 1], [0, 0, 0, 0, 215, 511, 3, 824, 7, 59, 4709, 160, 680, 27, 11, 7671, 23, 162, 82, 1556, 8, 11, 119, 404, 293, 136, 16, 101, 4, 1, 1, 1, 1], [0, 0, 0, 0, 40, 205, 1201, 59, 5769, 10669, 196, 8, 3, 83, 136, 7, 3, 119, 5, 6084, 10, 11, 2628, 23, 994, 10, 15580, 2, 4, 1, 1, 1, 1]] \n",
      "\n",
      "pos idx for first 5 sentences:\n",
      " [[0, 0, 0, 0, 4, 23, 3, 13, 3, 3, 12, 3, 6, 7, 4, 11, 1, 1, 1, 1], [0, 0, 0, 0, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 12, 3, 3, 29, 5, 1, 1, 1, 1], [0, 0, 0, 0, 3, 10, 7, 4, 6, 26, 8, 3, 4, 6, 7, 8, 5, 27, 6, 3, 6, 7, 3, 3, 4, 4, 6, 3, 11, 1, 1, 1, 1], [0, 0, 0, 0, 16, 3, 10, 26, 4, 13, 18, 6, 7, 4, 4, 6, 7, 4, 12, 21, 17, 7, 4, 5, 4, 17, 9, 3, 11, 1, 1, 1, 1]] \n",
      "\n",
      "ner idx for first 5 sentences:\n",
      " [[0, 0, 0, 0, 3, 3, 6, 3, 3, 3, 3, 4, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 4, 4, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 6, 6, 6, 3, 1, 1, 1, 1], [0, 0, 0, 0, 6, 3, 3, 3, 3, 3, 7, 7, 3, 3, 3, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1], [0, 0, 0, 0, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 1, 1, 1, 1]] \n",
      "\n",
      "capitalization idx for first 5 sentences: \n",
      " [[0, 0, 0, 0, 6, 4, 6, 6, 6, 6, 4, 6, 6, 6, 6, 4, 1, 1, 1, 1], [0, 0, 0, 0, 5, 5, 1, 1, 1, 1], [0, 0, 0, 0, 4, 4, 5, 5, 5, 4, 1, 1, 1, 1], [0, 0, 0, 0, 5, 3, 3, 3, 3, 3, 5, 5, 3, 3, 3, 3, 4, 3, 3, 5, 3, 3, 5, 6, 3, 3, 3, 5, 4, 1, 1, 1, 1], [0, 0, 0, 0, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 4, 3, 3, 3, 5, 4, 1, 1, 1, 1]] \n",
      "\n",
      "number of sentences = 3453 \n",
      "\n",
      "STEP 2/2 -- WINDOWING\n",
      "sample windows:\n",
      "Vocab for window 0\n",
      "[0, 0, 0, 0, 92, 20, 215, 410, 7671]\n",
      "['<s>', '<s>', '<s>', '<s>', 'soccer', '-', 'japan', 'get', 'lucky']\n",
      "PoS tags for window 0\n",
      "[0, 0, 0, 0, 4, 23, 3, 13, 3]\n",
      "['<s>', '<s>', '<s>', '<s>', 'NN', ':', 'NNP', 'VB', 'NNP']\n",
      "Capitalization tags for window 0\n",
      "[0, 0, 0, 0, 6, 4, 6, 6, 6]\n",
      "['<s>', '<s>', '<s>', '<s>', 'allCaps', 'noinfo', 'allCaps', 'allCaps', 'allCaps']\n",
      "NER tags for center word\n",
      "3\n",
      "['O'] \n",
      "\n",
      "Vocab for window 1\n",
      "[0, 0, 0, 92, 20, 215, 410, 7671, 162]\n",
      "['<s>', '<s>', '<s>', 'soccer', '-', 'japan', 'get', 'lucky', 'win']\n",
      "PoS tags for window 1\n",
      "[0, 0, 0, 4, 23, 3, 13, 3, 3]\n",
      "['<s>', '<s>', '<s>', 'NN', ':', 'NNP', 'VB', 'NNP', 'NNP']\n",
      "Capitalization tags for window 1\n",
      "[0, 0, 0, 6, 4, 6, 6, 6, 6]\n",
      "['<s>', '<s>', '<s>', 'allCaps', 'noinfo', 'allCaps', 'allCaps', 'allCaps', 'allCaps']\n",
      "NER tags for center word\n",
      "3\n",
      "['O'] \n",
      "\n",
      "Vocab for window 2\n",
      "[0, 0, 92, 20, 215, 410, 7671, 162, 5]\n",
      "['<s>', '<s>', 'soccer', '-', 'japan', 'get', 'lucky', 'win', ',']\n",
      "PoS tags for window 2\n",
      "[0, 0, 4, 23, 3, 13, 3, 3, 12]\n",
      "['<s>', '<s>', 'NN', ':', 'NNP', 'VB', 'NNP', 'NNP', ',']\n",
      "Capitalization tags for window 2\n",
      "[0, 0, 6, 4, 6, 6, 6, 6, 4]\n",
      "['<s>', '<s>', 'allCaps', 'noinfo', 'allCaps', 'allCaps', 'allCaps', 'allCaps', 'noinfo']\n",
      "NER tags for center word\n",
      "6\n",
      "['I-LOC'] \n",
      "\n",
      "rows of vocab features = 46435\n",
      "rows of PoS features = 46435\n",
      "rows of Capitalization features = 46435\n",
      "rows of NER features = 46435\n",
      "numpy feature arrays are returned\n",
      "'formatWindowedData'  454.10 ms\n"
     ]
    }
   ],
   "source": [
    "# UPDATES!\n",
    "\n",
    "windowLength = 9\n",
    "testNumSents = 5000\n",
    "\n",
    "# Use training set to build vocab here\n",
    "vocabData = conll2003Data(TRAIN_FILE)\n",
    "vocabData.buildVocab( vocabSize=20000)\n",
    "\n",
    "# Format training data\n",
    "trainX, trainX_pos, trainX_capitals, trainY  = vocabData.formatWindowedData( vocabData.train_sentences, \n",
    "                                                  windowLength=windowLength,\n",
    "                                                  verbose=True)\n",
    "\n",
    "# read in dev data\n",
    "devSents = vocabData.readFile( DEV_FILE)\n",
    "# Format dev data\n",
    "devX, devX_pos, devX_capitals, devY = vocabData.formatWindowedData( devSents, \n",
    "                                              windowLength=windowLength,\n",
    "                                              verbose=True)\n",
    "\n",
    "# read in the test data\n",
    "testSents = vocabData.readFile( TEST_FILE)\n",
    "# Format test data\n",
    "testX, testX_pos, testX_capitals, testY = vocabData.formatWindowedData( testSents, \n",
    "                                                windowLength=windowLength,\n",
    "                                                verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# capsule layers from Xifeng Guo \n",
    "# https://github.com/XifengGuo/CapsNet-Keras\n",
    "from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding 1-hot for ner targets\n",
    "trainY_cat = to_categorical(trainY.astype('float32'))\n",
    "devY_cat = to_categorical(devY.astype('float32'), num_classes=trainY_cat.shape[1])\n",
    "testY_cat = to_categorical(testY.astype('float32'), num_classes=trainY_cat.shape[1])\n",
    "\n",
    "trainY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), trainY_cat)), dtype=np.float)\n",
    "devY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), devY_cat)), dtype=np.float)\n",
    "testY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), testY_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEW!\n",
    "\n",
    "# encoding 1-hot for pos tags\n",
    "trainX_pos_cat = to_categorical(trainX_pos.astype('float32'))\n",
    "devX_pos_cat = to_categorical(devX_pos.astype('float32'), num_classes=trainX_pos_cat.shape[2]) \n",
    "testX_pos_cat = to_categorical(testX_pos.astype('float32'), num_classes=trainX_pos_cat.shape[2])\n",
    "\n",
    "trainX_pos_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), trainX_pos_cat)), dtype=np.float)\n",
    "devX_pos_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), devX_pos_cat)), dtype=np.float)\n",
    "testX_pos_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), testX_pos_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEW!\n",
    "\n",
    "# encoding 1-hot for capitalization info  (\"allCaps\", \"upperInitial\", \"lowercase\", \"mixedCaps\", \"noinfo\")\n",
    "trainX_capitals_cat = to_categorical(trainX_capitals.astype('float32'))\n",
    "devX_capitals_cat = to_categorical(devX_capitals.astype('float32'), num_classes=trainX_capitals_cat.shape[2]) \n",
    "testX_capitals_cat = to_categorical(testX_capitals.astype('float32'), num_classes=trainX_capitals_cat.shape[2])\n",
    "\n",
    "trainX_capitals_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), trainX_capitals_cat)), dtype=np.float)\n",
    "devX_capitals_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), devX_capitals_cat)), dtype=np.float)\n",
    "testX_capitals_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), testX_capitals_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UPDATES!\n",
    "\n",
    "# Define hyperparameters\n",
    "max_features = vocabData.vocab.size # 20000\n",
    "maxlen = trainX.shape[1] # 9  --> window size\n",
    "poslen = trainX_pos_cat.shape[2] # 45 pos classes   #! NEW\n",
    "capitallen = trainX_capitals_cat.shape[2] # 5 capitalization classes #! NEW\n",
    "ner_classes = trainY_cat.shape[1] # 8 \n",
    "embed_dim = 50 # word embedding size\n",
    "num_routing = 3 \n",
    "\n",
    "save_dir = './result'\n",
    "batch_size = 100\n",
    "debug = 2\n",
    "epochs = 5\n",
    "dropout_p = 0.25\n",
    "embed_dropout = 0.25\n",
    "lam_recon = 0.0005\n",
    "\n",
    "use_pos_tags = True #! NEW\n",
    "use_capitalization_info = True #! NEW\n",
    "\n",
    "#Load train and test data\n",
    "#(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_features)\n",
    "#x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "#x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.50d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 50))\n"
     ]
    }
   ],
   "source": [
    "# NEW!\n",
    "# construct embedding matrix\n",
    "\n",
    "import glove_helper; \n",
    "reload(glove_helper)\n",
    "\n",
    "hands = glove_helper.Hands(ndim=embed_dim)\n",
    "embedding_matrix = np.zeros((vocabData.vocab.size, embed_dim))\n",
    "\n",
    "for i in range(vocabData.vocab.size):\n",
    "    word = vocabData.vocab.ids_to_words([i])[0]\n",
    "    try:\n",
    "        embedding_vector = hands.get_vector(word)\n",
    "    except:\n",
    "        embedding_vector = hands.get_vector(\"<unk>\")\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print (embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (?, 9)\n",
      "x_pos (?, 9, 45)\n",
      "x_capital (?, 9, 5)\n",
      "embed (?, 9, 50)\n",
      "embed_concat (?, 9, 100)\n",
      "conv1 (?, 7, 256)\n",
      "primarycaps (?, ?, 8)\n",
      "ner_caps (?, 8, 16)\n",
      "out_caps (?, 8)\n"
     ]
    }
   ],
   "source": [
    "# UPDATES!\n",
    "\n",
    "# input\n",
    "x = Input(shape=(maxlen,), name='x')\n",
    "x_pos = Input(shape=(maxlen,poslen), name='x_pos')\n",
    "x_capital = Input(shape=(maxlen, capitallen), name='x_capital')\n",
    "print (\"x\", x.get_shape())\n",
    "print (\"x_pos\", x_pos.get_shape())\n",
    "print (\"x_capital\", x_capital.get_shape())\n",
    "\n",
    "# feed input into embedding layer\n",
    "embed = Embedding(max_features, embed_dim, input_length=maxlen, embeddings_initializer=\"random_uniform\" )(x)\n",
    "#embed = Embedding(max_features, embed_dim, weights=[embedding_matrix],input_length=maxlen, trainable=False)(x)\n",
    "print (\"embed\", embed.get_shape())\n",
    "\n",
    "# concat embeddings with pos Tags here\n",
    "if use_pos_tags and use_capitalization_info : \n",
    "    embed_concat = Concatenate(axis=-1)([embed, x_pos, x_capital])\n",
    "elif use_pos_tags and (not use_capitalization_info) :\n",
    "    embed_concat = Concatenate(axis=-1)([embed, x_pos])\n",
    "elif (not use_pos_tags) and use_capitalization_info :\n",
    "    embed_concat = Concatenate(axis=-1)([embed, x_capital])    \n",
    "else :\n",
    "    embed_concat = embed\n",
    "    \n",
    "print (\"embed_concat\", embed_concat.get_shape())\n",
    "\n",
    "# feed embeddings into conv1\n",
    "conv1 = Conv1D( filters=256, kernel_size=3, strides=1, padding='valid', activation='relu', name='conv1')(embed_concat)\n",
    "print (\"conv1\", conv1.get_shape())\n",
    "\n",
    "# make primary capsules\n",
    "primarycaps = PrimaryCap(embed, dim_capsule=8, n_channels=32, kernel_size=3, strides=1, padding='valid')\n",
    "print (\"primarycaps\", primarycaps.get_shape())\n",
    "\n",
    "# make ner capsules\n",
    "ner_caps = CapsuleLayer(num_capsule=ner_classes, dim_capsule=16, routings=num_routing, name='nercaps')(primarycaps)\n",
    "print (\"ner_caps\", ner_caps.get_shape())\n",
    "\n",
    "# replace each ner capsuel with its length\n",
    "out_caps = Length(name='out_caps')(ner_caps)\n",
    "print (\"out_caps\", out_caps.get_shape())\n",
    "\n",
    "capsmodel = Model(inputs=[x, x_pos, x_capital], outputs=[out_caps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 9, 50)             887400    \n",
      "_________________________________________________________________\n",
      "primarycap_conv2d (Conv1D)   (None, 7, 256)            38656     \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 224, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 224, 8)            0         \n",
      "_________________________________________________________________\n",
      "nercaps (CapsuleLayer)       (None, 8, 16)             229376    \n",
      "_________________________________________________________________\n",
      "out_caps (Length)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,155,432\n",
      "Trainable params: 1,155,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Saving weights and logging\n",
    "log = callbacks.CSVLogger(save_dir + '/log.csv')\n",
    "tb = callbacks.TensorBoard(log_dir=save_dir + '/tensorboard-logs', \n",
    "                           batch_size=batch_size, histogram_freq=debug)\n",
    "checkpoint = callbacks.ModelCheckpoint(save_dir + '/weights-{epoch:02d}.h5', \n",
    "                                       save_best_only=True, \n",
    "                                       save_weights_only=True, \n",
    "                                       verbose=1)\n",
    "#lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
    "\n",
    "#margin_loss\n",
    "def margin_loss(y_true, y_pred):\n",
    "    L = y_true * KB.square(KB.maximum(0., 0.9 - y_pred)) + 0.5 * (1 - y_true) * KB.square(KB.maximum(0., y_pred - 0.1))\n",
    "    return KB.mean(KB.sum(L, 1))\n",
    "\n",
    "capsmodel.summary()\n",
    "\n",
    "#Save a png of the model shapes and flow\n",
    "#plot_model(capsmodel, to_file=save_dir + '/reuters-model.png', show_shapes=True)\n",
    "\n",
    "opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#opt = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.5, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "capsmodel.compile(optimizer=opt, #'adam',\n",
    "              loss=margin_loss,\n",
    "              metrics={'out_caps': 'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203621, 9, 45)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX_pos_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203621 samples, validate on 51362 samples\n",
      "Epoch 1/5\n",
      " 10800/203621 [>.............................] - ETA: 11:27 - loss: 0.8100 - acc: 0.1524"
     ]
    }
   ],
   "source": [
    "# UPDATES!\n",
    "\n",
    "# train the model - no recon\n",
    "# compile the model\n",
    "capsmodel.compile(optimizer=opt, #'adam',\n",
    "              loss=margin_loss,\n",
    "              metrics={'out_caps': 'accuracy'})\n",
    "\n",
    "data = capsmodel.fit( x={'x':trainX, 'x_pos':trainX_pos_cat, 'x_capital':trainX_capitals_cat}, \n",
    "                      y=trainY_cat, \n",
    "                      batch_size=batch_size, \n",
    "                      epochs=epochs, \n",
    "                      validation_data=[[devX, devX_pos_cat, devX_capitals_cat], devY_cat], \n",
    "                      callbacks=[log, tb, checkpoint], \n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 2331,  246,   76],\n",
       "       [   0,    0,    0, ...,  246,   76,   21],\n",
       "       [   0,    0,  279, ...,   76,   21,  358],\n",
       "       ..., \n",
       "       [   0,    0,    0, ...,    2,    1,    1],\n",
       "       [   0,    0,   58, ...,    1,    1,    1],\n",
       "       [   0,   58, 1359, ...,    1,    1,    1]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
