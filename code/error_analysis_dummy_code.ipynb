{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`updates from the last main_model_code.ipynb in master branch.`\n",
    "\n",
    "`0.1 possibly some new libraries`\n",
    "\n",
    "`2.1 I moved this code chunk from the bottom to here.`\n",
    "\n",
    "`2.3 New code chunk -- this gets the Y label for the decoder`\n",
    "\n",
    "`2.6 Major code change here to allow encoder option`\n",
    "\n",
    "`3.2 New code chunk -- this calculates cosine similarity loss for decoder.`\n",
    "\n",
    "`3.4, 3.5 compile_model() function is now customized as two compile_caps_model() and compile_cnn_model() for code claritys sake. corresponding loss functions and loss weights are updated`\n",
    "\n",
    "`(you can delete the two existing code chunks which contains `def decoder_loss()` and compile_model())`\n",
    "\n",
    "`3.6 fit_model() arguments have changed to allow for multiple model outputs, replace this whole chunk`\n",
    "\n",
    "`4.0 'embed_dim' : embed_dim;  swtich 'use_decoder' to True to enable decoder`\n",
    "\n",
    "`4.1 'embed_dim' : embed_dim`\n",
    "\n",
    "`4.2 compile_model() now becomes compile_caps_model`\n",
    "\n",
    "`4.3 compile_model() now becomes compile_cnn_model`\n",
    "\n",
    "`4.4 New code chunk -- draw and compile CapsNet with decoder`\n",
    "\n",
    "`4.5, 4.6 minor argument format change according to change described in 3.6`\n",
    "\n",
    "`4.7 New code chunk to fit CapsNet with decoder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yeunghoman/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "from keras import callbacks, optimizers\n",
    "from keras import backend as KB\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Activation\n",
    "from keras.layers import LeakyReLU, Dense, Input, Embedding, Dropout, Reshape, Concatenate, MaxPooling1D, Flatten\n",
    "from keras.layers import Bidirectional, GRU, Flatten, SpatialDropout1D, Conv1D\n",
    "from keras.layers import Add\n",
    "# from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from common import vocabulary, utils\n",
    "\n",
    "# capsule layers from Xifeng Guo \n",
    "# https://github.com/XifengGuo/CapsNet-Keras\n",
    "from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask\n",
    "import glove_helper\n",
    "\n",
    "import time # !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from evaluation_helper import EvalDev_Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.4.1\n",
      "Keras version: 2.1.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"Keras version:\", K.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"../data/conll2003/eng.train\"\n",
    "DEV_FILE = \"../data/conll2003/eng.testa\"\n",
    "TEST_FILE = \"../data/conll2003/eng.testb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Class !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# local untils\n",
    "\n",
    "# timeit decorator\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print ('%r  %2.2f ms' % \\\n",
    "                  (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_embedding_matrix(embed_dim):\n",
    "    \"\"\"\n",
    "    construct embedding matrix from GloVe 6Bn word data\n",
    "    \n",
    "    reuse glove_helper code from w266 \n",
    "    \n",
    "    Returns: an embedding matrix directly plugged into keras.layers.Embedding(weights=[embedding_matrix])\n",
    "    \"\"\"\n",
    "    reload(glove_helper)\n",
    "    hands = glove_helper.Hands(ndim=embed_dim)\n",
    "    embedding_matrix = np.zeros((vocabData.vocab.size, embed_dim))\n",
    "\n",
    "    for i in range(vocabData.vocab.size):\n",
    "        word = vocabData.vocab.ids_to_words([i])[0]\n",
    "        try:\n",
    "            embedding_vector = hands.get_vector(word)\n",
    "        except:\n",
    "            embedding_vector = hands.get_vector(\"<unk>\")\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class conll2003Data(object):\n",
    "    \"\"\"\n",
    "    Keep track of data and processing operations for a single CoNLL2003 data file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filePath_train):\n",
    "        \"\"\"\n",
    "        filePath(string): path to a CoNLL2003 raw data file for training the vocabulary\n",
    "        \"\"\"\n",
    "        self.vocab = []\n",
    "        self.posTags = []\n",
    "        self.nerTags = []\n",
    "        self.train_sentences = self.readFile(filePath_train)\n",
    "\n",
    "\n",
    "    @timeit\n",
    "    def readFile(self, filePath, canonicalize=True, verbose=False):\n",
    "        \"\"\"\n",
    "        Read the conll2003 raw data file\n",
    "\n",
    "        filename(string) - path to conll2003 file (train, test, etc.)\n",
    "        \n",
    "        Returns: a list of lists of lists corresponding to the words, pos tags, ner tags, capitalization\n",
    "                 in each sentence\n",
    "\n",
    "        \"\"\"\n",
    "        print (\"----------------------------------------------------\")\n",
    "        print (\"reading file from path\", str(filePath))\n",
    "        f = open(filePath)\n",
    "        sentences = []\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            if len(line) == 0 or line.startswith(\"-DOCSTART\") or line[0] == '\\n':\n",
    "                if len(sentence) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                continue\n",
    "            \n",
    "            # input format is [ word, pos tag, chunck tag, ner tag]\n",
    "            # we are ignoring the chunck tag\n",
    "            splits = line.strip().split(' ')\n",
    "            if canonicalize:\n",
    "                word = [utils.canonicalize_word(splits[0]), splits[1], splits[3], self.capitalizaion(splits[0])]\n",
    "            else:\n",
    "                word = [splits[0], splits[1], splits[3], capitalizaion(splits[0])]\n",
    "            sentence.append( word)\n",
    "        \n",
    "        # don't forget the last sentence\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "        \n",
    "        if verbose: \n",
    "            print (\"number of sentences on file =\",len(sentences))\n",
    "            print (\"first 5 sentences:\")\n",
    "            print (sentences[:5])\n",
    "\n",
    "        return sentences\n",
    "    \n",
    "    \n",
    "    def capitalizaion(self, word):\n",
    "        \"\"\"\n",
    "        check capitalization info for a word\n",
    "        return 'lowercase' for 'sfsd'\n",
    "        return 'allCaps' for 'SFSD'\n",
    "        return 'upperInitial' for 'Sfsd'\n",
    "        return 'mixedCaps' for 'SfSd'\n",
    "        return 'noinfo' for '$#%@#' or '12334'\n",
    "        \"\"\"\n",
    "        alphas = [c.isalpha() for c in word] \n",
    "        if sum(alphas) != len(word):\n",
    "            return 'noinfo'\n",
    "        caps = [char.lower()==char for char in word]\n",
    "        if sum(caps) == len(word):\n",
    "            return 'lowercase'\n",
    "        elif sum(caps) == 0:\n",
    "            return 'allCaps'\n",
    "        elif caps[0] == False and sum(caps) == len(word)-1:\n",
    "            return 'upperInitial'\n",
    "        elif 0 < sum(caps) < len(word):\n",
    "            return 'mixedCaps'\n",
    "        else:\n",
    "            return 'noinfo'    \n",
    "    \n",
    "    @timeit\n",
    "    def buildVocab(self, vocabSize=None, verbose=False, return_vocab_objects=False):\n",
    "        \"\"\"\n",
    "        Builds the vocabulary based on the initial data file\n",
    "        \n",
    "        vocabSize(int, default: None-all words) - max number of words to use for vocabulary\n",
    "                                                  (only used for training)\n",
    "        verbose(boolean, default: False)        - print extra info\n",
    "        \"\"\"    \t\n",
    "        print (\"----------------------------------------------------\")\n",
    "        print (\"building vocabulary from TRAINING data...\")\n",
    "\n",
    "        flatData = [w for w in zip(*utils.flatten(self.train_sentences))]\n",
    "\n",
    "        # remember these vocabs will have the <s>, </s>, and <unk> tags in there\n",
    "        # sizes need to be interpreted \"-3\" - consider replacing...\n",
    "        self.vocab = vocabulary.Vocabulary( flatData[0], size=vocabSize)\n",
    "        self.posTags = vocabulary.Vocabulary( flatData[1])\n",
    "        self.nerTags = vocabulary.Vocabulary( flatData[2])\n",
    "        self.capitalTags = vocabulary.Vocabulary(flatData[3])\n",
    "\n",
    "        if verbose:\n",
    "            print (\"vocabulary for words, posTags, nerTags built and stored in object\")\n",
    "            print (\"vocab size =\", vocabSize)\n",
    "            print (\"10 sampled words from vocabulary\\n\", list(self.vocab.wordset)[:10], \"\\n\")\n",
    "            print (\"number of unique pos Tags in training =\", self.posTags.size)\n",
    "            print (\"all posTags used\\n\", list(self.posTags.wordset), \"\\n\")\n",
    "            print (\"number of unique NER tags in training =\", self.nerTags.size)\n",
    "            print (\"all nerTags for prediction\", list(self.nerTags.wordset), \"\\n\")\n",
    "            print (\"number of unique capitalization tags in training =\", self.capitalTags.size)\n",
    "            print ('all capitalTags for prediction', list(self.capitalTags.wordset), \"\\n\")\n",
    "\n",
    "        if return_vocab_objects:\n",
    "            return self.vocab, self.posTags, self.nerTags, self.capitalTags\n",
    "\n",
    "\n",
    "    @timeit\n",
    "    def formatWindowedData(self, sentences, windowLength=9, verbose=False):\n",
    "        \"\"\"\n",
    "        Format the raw data by blocking it into context windows of a fixed length corresponding \n",
    "        to the single target NER tag of the central word.\n",
    "        Make sure to call buildVocab first.\n",
    "        \n",
    "        sentences(list of lists of lists) - raw data from the CoNLL2003 dataset\n",
    "        windowLength(int, default: 9)     - The length of the context window\n",
    "                    NOTE - windowLength must be odd to have a central word. If itsn't, 1 will be added.\n",
    "        verbose(boolean, default: False)  - print extra info\n",
    "        \n",
    "        Returns: 4 numpy arrays: vocabulary training data windowed and converted to IDs, \n",
    "                                 POS tags windowed and converted to IDs,\n",
    "                                 Capitalization info windowed and converted into IDs,\n",
    "                                 NER label tags converted to IDs\n",
    "        \"\"\"\n",
    "\n",
    "        print (\"----------------------------------------------------\")\n",
    "        print (\"formatting sentences into input windows...\")\n",
    "\n",
    "        if windowLength % 2 == 0 or windowLength == 1:\n",
    "            raise ValueError(\"window Length must be an odd number and greater than one.\")\n",
    "    \n",
    "        pads = windowLength // 2\n",
    "\n",
    "        # we have a list of lists (sentences) of lists ([word, posTag, nerTag])\n",
    "        # parse through, pad each sentence with pads open and close tags, then convert to IDs\n",
    "        vocabIDs = [ self.vocab.words_to_ids( [\"<s>\"] * pads + [word[0] for word in sent] + [\"</s>\"] * pads) \\\n",
    "                     for sent in sentences]\n",
    "        posIDs = [ self.posTags.words_to_ids( [\"<s>\"] * pads + [word[1] for word in sent] + [\"</s>\"] * pads) \\\n",
    "                   for sent in sentences]\n",
    "        capitalIDs = [self.capitalTags.words_to_ids([\"<s>\"]*pads + [word[3] for word in sent] + [\"</s>\"]*pads) \\\n",
    "                     for sent in sentences]\n",
    "        nerIDs = [ self.nerTags.words_to_ids( [\"<s>\"] * pads + [word[2] for word in sent] + [\"</s>\"] * pads) \\\n",
    "                   for sent in sentences]\n",
    "        \n",
    "        if verbose: \n",
    "            print (\"STEP 1/2 -- PADDING\")\n",
    "            print (\"all sentences padded with {} pads to either end\".format(pads))\n",
    "            print (\"vocab idx for first 5 sentences:\\n\", vocabIDs[:5], \"\\n\")\n",
    "            print (\"pos idx for first 5 sentences:\\n\", posIDs[:5], \"\\n\")\n",
    "            print (\"ner idx for first 5 sentences:\\n\", nerIDs[:5], \"\\n\")\n",
    "            print (\"capitalization idx for first 5 sentences: \\n\", capitalIDs[:5], \"\\n\")\n",
    "            print (\"number of sentences = {}\".format(len(vocabIDs)), \"\\n\")\n",
    "\n",
    "        assert(len(vocabIDs) == len(posIDs) and len(posIDs) == len(nerIDs) == len(capitalIDs))\n",
    "\n",
    "        if verbose: \n",
    "            print (\"STEP 2/2 -- WINDOWING\")\n",
    "\n",
    "        # build the data to train on by sliding the window across each sentence\n",
    "        # at this point, all 3 lists are the same size, so we can run through them all at once\n",
    "        featsVocab, featsPOS, featsNER, featsCAPITAL = [], [], [], []\n",
    "        for sentID in range( len(vocabIDs)):\n",
    "            sent = vocabIDs[sentID]\n",
    "            sentPOS = posIDs[sentID]\n",
    "            sentNER = nerIDs[sentID]\n",
    "            sentCAPITAL = capitalIDs[sentID]\n",
    "            \n",
    "            for ID in range( len(sent) - windowLength + 1):\n",
    "                featsVocab.append( sent[ID:ID + windowLength])\n",
    "                featsPOS.append( sentPOS[ID:ID + windowLength])\n",
    "                featsCAPITAL.append(sentCAPITAL[ID:ID + windowLength])\n",
    "                featsNER.append( sentNER[ID + windowLength // 2])\n",
    "                \n",
    "        if verbose:\n",
    "            print (\"sample windows:\")\n",
    "            for i in range(3):\n",
    "                print (\"Vocab for window {}\".format(i))\n",
    "                print (featsVocab[i])\n",
    "                print (self.vocab.ids_to_words(featsVocab[i]))\n",
    "                print (\"PoS tags for window {}\".format(i))\n",
    "                print (featsPOS[i])\n",
    "                print (self.posTags.ids_to_words(featsPOS[i]))\n",
    "                print (\"Capitalization tags for window {}\".format(i))\n",
    "                print (featsCAPITAL[i])\n",
    "                print (self.capitalTags.ids_to_words(featsCAPITAL[i]))\n",
    "                print (\"NER tags for center word\")\n",
    "                print (featsNER[i])\n",
    "                print (self.nerTags.ids_to_words([featsNER[i]]),\"\\n\")\n",
    "                \n",
    "            print (\"rows of vocab features = {}\".format(len(featsVocab)))\n",
    "            print (\"rows of PoS features = {}\".format(len(featsVocab)))\n",
    "            print (\"rows of Capitalization features = {}\".format(len(featsCAPITAL)))\n",
    "            print (\"rows of NER features = {}\".format(len(featsNER)))\n",
    "            \n",
    "            print (\"numpy feature arrays are returned\")\n",
    "\n",
    "        assert(len(featsVocab) == len(featsVocab) == len(featsNER) == len(featsCAPITAL))\n",
    "        return np.array(featsVocab), np.array(featsPOS), np.array(featsCAPITAL), np.array(featsNER)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "reading file from path ../data/conll2003/eng.train\n",
      "'readFile'  1254.42 ms\n",
      "----------------------------------------------------\n",
      "building vocabulary from TRAINING data...\n",
      "'buildVocab'  1048.01 ms\n",
      "----------------------------------------------------\n",
      "formatting sentences into input windows...\n",
      "'formatWindowedData'  1834.51 ms\n",
      "----------------------------------------------------\n",
      "reading file from path ../data/conll2003/eng.testa\n",
      "'readFile'  301.05 ms\n",
      "----------------------------------------------------\n",
      "formatting sentences into input windows...\n",
      "'formatWindowedData'  423.75 ms\n",
      "----------------------------------------------------\n",
      "reading file from path ../data/conll2003/eng.testb\n",
      "'readFile'  235.79 ms\n",
      "----------------------------------------------------\n",
      "formatting sentences into input windows...\n",
      "'formatWindowedData'  400.10 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "windowLength = 9\n",
    "testNumSents = 5000\n",
    "\n",
    "# Use training set to build vocab here\n",
    "vocabData = conll2003Data(TRAIN_FILE)\n",
    "vocabData.buildVocab( vocabSize=20000)\n",
    "\n",
    "# Format training data\n",
    "trainX, trainX_pos, trainX_capitals, trainY  = vocabData.formatWindowedData( vocabData.train_sentences, \n",
    "                                                  windowLength=windowLength,\n",
    "                                                  verbose=False)\n",
    "\n",
    "# read in dev data\n",
    "devSents = vocabData.readFile( DEV_FILE)\n",
    "devX, devX_pos, devX_capitals, devY = vocabData.formatWindowedData( devSents, \n",
    "                                              windowLength=windowLength,\n",
    "                                              verbose=False)\n",
    "\n",
    "# read in the test data\n",
    "testSents = vocabData.readFile( TEST_FILE)\n",
    "testX, testX_pos, testX_capitals, testY = vocabData.formatWindowedData( testSents, \n",
    "                                                windowLength=windowLength,\n",
    "                                                verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.50d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 50))\n"
     ]
    }
   ],
   "source": [
    "# !\n",
    "embed_dim = 50\n",
    "# Load GloVe embedding matrix\n",
    "embedding_matrix = construct_embedding_matrix(embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Y\n",
    "\n",
    "# encoding 1-hot for ner targets\n",
    "trainY_cat = to_categorical(trainY.astype('float32'))\n",
    "devY_cat = to_categorical(devY.astype('float32'), num_classes=trainY_cat.shape[1])\n",
    "testY_cat = to_categorical(testY.astype('float32'), num_classes=trainY_cat.shape[1])\n",
    "\n",
    "trainY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), trainY_cat)), dtype=np.float)\n",
    "devY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), devY_cat)), dtype=np.float)\n",
    "testY_cat = np.array(list(map( lambda i: np.array(i[3:], dtype=np.float), testY_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIP here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dummy prediction labels\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(trainY_cat.shape[0]))\n",
    "trainY_cat_pred = trainY_cat[shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 7, ..., 3, 5, 3])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY_cat_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "report1 = EvalDev_Report(y_true=trainY, raw_y_pred=trainY_cat_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043738800928237814"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report1.f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 169578,\n",
       "         4: 11128,\n",
       "         5: 10001,\n",
       "         6: 8286,\n",
       "         7: 4556,\n",
       "         8: 37,\n",
       "         9: 24,\n",
       "         10: 11})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report1.gold_cts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !\n",
    "# Get decoder Y -- 50 dim embedding of center word\n",
    "\n",
    "train_decoderY = embedding_matrix[trainX[:,4]]\n",
    "dev_decoderY = embedding_matrix[devX[:,4]]\n",
    "test_decoderY = embedding_matrix[testX[:,4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get X pos tags\n",
    "\n",
    "# encoding 1-hot for pos tags\n",
    "trainX_pos_cat = to_categorical(trainX_pos.astype('float32'))\n",
    "devX_pos_cat = to_categorical(devX_pos.astype('float32'), num_classes=trainX_pos_cat.shape[2]) \n",
    "testX_pos_cat = to_categorical(testX_pos.astype('float32'), num_classes=trainX_pos_cat.shape[2])\n",
    "\n",
    "trainX_pos_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), trainX_pos_cat)), dtype=np.float)\n",
    "devX_pos_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), devX_pos_cat)), dtype=np.float)\n",
    "testX_pos_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), testX_pos_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get X capitlization \n",
    "\n",
    "# encoding 1-hot for capitalization info  (\"allCaps\", \"upperInitial\", \"lowercase\", \"mixedCaps\", \"noinfo\")\n",
    "trainX_capitals_cat = to_categorical(trainX_capitals.astype('float32'))\n",
    "devX_capitals_cat = to_categorical(devX_capitals.astype('float32'), num_classes=trainX_capitals_cat.shape[2]) \n",
    "testX_capitals_cat = to_categorical(testX_capitals.astype('float32'), num_classes=trainX_capitals_cat.shape[2])\n",
    "\n",
    "trainX_capitals_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), trainX_capitals_cat)), dtype=np.float)\n",
    "devX_capitals_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), devX_capitals_cat)), dtype=np.float)\n",
    "testX_capitals_cat = np.array(list(map( lambda i: np.array(i[:,3:], dtype=np.float), testX_capitals_cat)), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
